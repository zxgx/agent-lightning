{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agent Lightning","text":"<p>Agent Lightning is the absolute trainer to light up AI agents.</p> <p>Join our Discord community to connect with other users and contributors.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Turn your agent into an optimizable beast with ZERO CODE CHANGE (almost)! \ud83d\udca4</li> <li>Build with ANY agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! \ud83e\udd16</li> <li>Selectively optimize one or more agents in a multi-agent system. \ud83c\udfaf</li> <li>Embraces Algorithms like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. \ud83e\udd17</li> </ul>"},{"location":"#how-to-read-this-documentation","title":"How to Read this Documentation","text":"<p>This documentation is organized into the following parts:</p> <ul> <li>Installation - Get started with Agent Lightning</li> <li>How-to Recipes (e.g., Train SQL Agent with RL) - Practical examples of training agents and customizing algorithms.</li> <li>Learning More (e.g., Debugging) - Guides on specific topics like debugging or parallelization.</li> <li>Algorithm Zoo (e.g., APO) - References for built-in algorithms.</li> <li>Deep Dive (e.g., Bird's Eye View) - For a deeper understanding of what Agent-lightning is doing under the hood.</li> <li>API References (e.g., Agent) - References for the Agent-lightning Python API.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<ul> <li>10/22/2025 No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL vLLM blog. See also Zhihu writeup.</li> <li>8/11/2025 Training AI Agents to Write and Self-correct SQL with Reinforcement Learning Medium.</li> <li>8/5/2025 Agent Lightning: Train ANY AI Agents with Reinforcement Learning arXiv paper.</li> <li>7/26/2025 We discovered an approach to train any AI agent with RL, with (almost) zero code changes. Reddit.</li> <li>6/6/2025 Agent Lightning - Microsoft Research Project page.</li> </ul>"},{"location":"#community-projects","title":"Community Projects","text":"<ul> <li>DeepWerewolf \u2014 A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.</li> <li>AgentFlow \u2014 A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you find Agent Lightning useful in your research or projects, please cite our paper:</p> <pre><code>@misc{luo2025agentlightningtrainai,\n      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},\n      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},\n      year={2025},\n      eprint={2508.03680},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2508.03680},\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>See the LICENSE file for details.</p>"},{"location":"algorithm-zoo/","title":"Algorithm Zoo","text":"<p>AgentLightning includes several popular and frequently requested algorithms in its built-in library, allowing agent developers to use them directly. These algorithms are designed to be compatible with most agent scenarios.</p> <p>For customizing algorithms, see Algorithm-side References.</p> Algorithm Optimizing Resources Description APO <code>{&lt;initial_prompt_key&gt;: [PromptTemplate][agentlightning.PromptTemplate]}</code> Automatic Prompt Optimization (APO) algorithm using textual gradients and beam search. VERL <code>{\"main_llm\": [LLM][agentlightning.LLM]}</code> Reinforcement Learning with VERL framework."},{"location":"algorithm-zoo/apo/","title":"APO","text":"<p>Shortcut</p> <p>You can use the shortcut <code>agl.APO(...)</code> to create an APO instance.</p> <pre><code>import agentlightning as agl\n\nagl.APO(...)\n</code></pre>"},{"location":"algorithm-zoo/apo/#installation","title":"Installation","text":"<pre><code>pip install agentlightning[apo]\n</code></pre>"},{"location":"algorithm-zoo/apo/#scope-of-current-implementation","title":"Scope of Current Implementation","text":"<p>APO is currently scoped to optimize a single prompt template. Optimizing multiple prompt templates is not supported yet.</p> <p>There is however no restriction on the number of variable placeholders in the prompt template (can range from zero to many). It's possible that invalid prompts are created during the optimization process. It is up to the agent developer to ensure that the prompt template is valid for the agent's task.</p>"},{"location":"algorithm-zoo/apo/#initial-prompt","title":"Initial Prompt","text":"<p>APO expects the initial prompt to be provided in the <code>initial_resources</code> dictionary. This can be done in two approaches:</p> <ol> <li>Pass to the Trainer constructor:</li> </ol> <pre><code>trainer = agl.Trainer(\n    algorithm=agl.APO(...),\n    initial_resources={\"main_prompt\": agl.PromptTemplate(template=\"You are a helpful assistant.\", engine=\"f-string\")},\n)\n</code></pre> <ol> <li>Pass to the <code>[APO][agentlightning.algorithm.apo.APO].set_initial_resources()</code> method:</li> </ol> <pre><code>algo = agl.APO(...)\nalgo.set_initial_resources(\n    {\"this_is_also_valid_key\": agl.PromptTemplate(template=\"You are a helpful assistant.\", engine=\"f-string\")}\n)\n</code></pre> <p>The resource key can be arbitrary, which is used to identify the prompt template in class-based implementations when you have multiple resources. When the key changes, the agent developer needs to update the key in the <code>rollout</code> method.</p>"},{"location":"algorithm-zoo/apo/#tutorials-using-apo","title":"Tutorials Using APO","text":"<ul> <li>Train the First Agent with APO - A step-by-step guide to training your first agent using APO.</li> </ul>"},{"location":"algorithm-zoo/apo/#references","title":"References","text":""},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo","title":"<code>agentlightning.algorithm.apo</code>","text":""},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO","title":"<code>APO</code>","text":"<p>               Bases: <code>Algorithm</code>, <code>Generic[T_task]</code></p> <p>Automatic Prompt Optimization (APO) algorithm using textual gradients and beam search.</p> <p>APO is an iterative prompt optimization algorithm that uses LLM-generated textual gradients to improve prompts through a beam search process. It evaluates prompts on rollouts, computes critiques based on the results, and applies edits to generate improved prompts.</p> <p>The algorithm operates in rounds, where each round:</p> <ol> <li>Samples parent prompts from the current beam</li> <li>Generates new prompts by computing textual gradients and applying edits</li> <li>Evaluates all candidates on a validation set</li> <li>Selects the top-k prompts for the next round</li> </ol> <p>Based on the ideas from:</p> <ul> <li>ProTeGi</li> <li>TextGrad</li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.__init__","title":"<code>__init__(async_openai_client, *, gradient_model='gpt-5-mini', apply_edit_model='gpt-4.1-mini', diversity_temperature=1.0, gradient_batch_size=4, val_batch_size=16, beam_width=4, branch_factor=4, beam_rounds=3, rollout_batch_timeout=3600.0, run_initial_validation=True, _poml_trace=False)</code>","text":"<p>Initialize the APO algorithm with configuration parameters.</p> <p>Parameters:</p> <ul> <li> <code>async_openai_client</code>               (<code>AsyncOpenAI</code>)           \u2013            <p>AsyncOpenAI client for making LLM API calls.</p> </li> <li> <code>gradient_model</code>               (<code>str</code>, default:                   <code>'gpt-5-mini'</code> )           \u2013            <p>Model name for computing textual gradients (critiques).</p> </li> <li> <code>apply_edit_model</code>               (<code>str</code>, default:                   <code>'gpt-4.1-mini'</code> )           \u2013            <p>Model name for applying edits based on critiques.</p> </li> <li> <code>diversity_temperature</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>Temperature parameter for LLM calls to control diversity.</p> </li> <li> <code>gradient_batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of rollout results to sample for gradient computation.</p> </li> <li> <code>val_batch_size</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>Number of validation examples to use for evaluation.</p> </li> <li> <code>beam_width</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of top-scoring prompts to keep in the beam at each round.</p> </li> <li> <code>branch_factor</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of new prompt candidates to generate from each parent prompt by applying textual gradient edits. This controls the expansion of the search tree.</p> </li> <li> <code>beam_rounds</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of beam search rounds to perform.</p> </li> <li> <code>rollout_batch_timeout</code>               (<code>float</code>, default:                   <code>3600.0</code> )           \u2013            <p>Maximum time in seconds to wait for rollout batch completion.</p> </li> <li> <code>run_initial_validation</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, runs validation on the seed prompt before starting optimization to establish a baseline score. Defaults to True.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.compute_textual_gradient","title":"<code>compute_textual_gradient(current_prompt, rollout_results, *, prefix=None)</code>  <code>async</code>","text":"<p>Compute a textual gradient (critique) for the current prompt based on rollout results.</p> <p>This method samples rollout results, sends them to an LLM along with the current prompt, and generates a critique describing how the prompt could be improved.</p> <p>Parameters:</p> <ul> <li> <code>current_prompt</code>               (<code>VersionedPromptTemplate</code>)           \u2013            <p>The prompt template to critique.</p> </li> <li> <code>rollout_results</code>               (<code>List[RolloutResultForAPO]</code>)           \u2013            <p>List of rollout results containing spans, messages, and rewards.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>A textual critique generated by the LLM, or None if generation fails.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.evaluate_prompt_on_batch","title":"<code>evaluate_prompt_on_batch(prompt, resource_name, dataset, mode, *, prefix=None)</code>  <code>async</code>","text":"<p>Evaluate a prompt on a batch of tasks by running rollouts and computing average reward.</p> <p>This method:</p> <ol> <li>Adds the prompt as a named resource to the store</li> <li>Enqueues rollouts for each task in the dataset</li> <li>Waits for rollouts to complete (with timeout)</li> <li>Computes and returns the average reward</li> </ol> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>VersionedPromptTemplate</code>)           \u2013            <p>The prompt template string to evaluate.</p> </li> <li> <code>resource_name</code>               (<code>str</code>)           \u2013            <p>The name to register the prompt under in the store.</p> </li> <li> <code>dataset</code>               (<code>Sequence[T_task]</code>)           \u2013            <p>Sequence of tasks to evaluate the prompt on.</p> </li> <li> <code>mode</code>               (<code>RolloutMode</code>)           \u2013            <p>Rollout mode (\"train\" or \"val\") for logging/tracking.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[RolloutResultForAPO]</code>           \u2013            <p>A tuple of (rollout_results, average_reward) where rollout_results contains</p> </li> <li> <code>float</code>           \u2013            <p>detailed information for each rollout and average_reward is the mean final reward.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.get_adapter","title":"<code>get_adapter()</code>","text":"<p>Get the adapter for converting spans to messages.</p> <p>Returns:</p> <ul> <li> <code>TraceToMessages</code>           \u2013            <p>The TraceToMessages instance for this algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the adapter is not a TraceToMessages.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.get_best_prompt","title":"<code>get_best_prompt()</code>","text":"<p>Retrieve the best prompt discovered during optimization.</p> <p>Returns:</p> <ul> <li> <code>PromptTemplate</code>           \u2013            <p>The prompt template with the highest validation score found so far.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no best prompt has been found yet (run() not called).</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.get_rollout_results","title":"<code>get_rollout_results(rollout, *, prefix=None)</code>  <code>async</code>","text":"<p>Convert completed rollouts to APO-compatible result format.</p> <p>Fetches spans for each rollout, adapts them to messages, and packages them with rewards and status information for gradient computation.</p> <p>Parameters:</p> <ul> <li> <code>rollout</code>               (<code>List[Rollout]</code>)           \u2013            <p>List of completed rollout metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[RolloutResultForAPO]</code>           \u2013            <p>List of rollout results formatted for APO processing.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.get_seed_prompt_template","title":"<code>get_seed_prompt_template()</code>","text":"<p>Extract the initial prompt template from the algorithm's resources.</p> <p>Returns:</p> <ul> <li> <code>Tuple[str, PromptTemplate]</code>           \u2013            <p>A tuple of (resource_name, prompt_template) representing the seed prompt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If initial_resources is not set or no PromptTemplate is found.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.run","title":"<code>run(train_dataset=None, val_dataset=None)</code>  <code>async</code>","text":"<p>Execute the APO algorithm to optimize prompts through beam search with textual gradients.</p> <p>The algorithm performs iterative prompt optimization over multiple rounds:</p> <ul> <li>Each round: samples parent prompts, generates new candidates via textual gradients,   evaluates all candidates on validation data, and keeps the top performers</li> <li>Tracks the historically best prompt across all rounds</li> <li>Uses different training data samples for each gradient computation to ensure diversity</li> </ul> <p>Parameters:</p> <ul> <li> <code>train_dataset</code>               (<code>Optional[Dataset[T_task]]</code>, default:                   <code>None</code> )           \u2013            <p>Dataset of tasks for computing textual gradients. Required.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[T_task]]</code>, default:                   <code>None</code> )           \u2013            <p>Dataset of tasks for evaluating and selecting prompts. Required.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If train_dataset or val_dataset is None, or if resources are not set.</p> </li> </ul>"},{"location":"algorithm-zoo/apo/#agentlightning.algorithm.apo.APO.textual_gradient_and_apply_edit","title":"<code>textual_gradient_and_apply_edit(current_prompt, rollout, *, prefix=None)</code>  <code>async</code>","text":"<p>Generate an improved prompt by computing a textual gradient and applying an edit.</p> <p>This is the main optimization step that:</p> <ol> <li>Computes a critique (textual gradient) based on rollout performance</li> <li>Uses another LLM to apply the critique and generate an improved prompt</li> </ol> <p>Parameters:</p> <ul> <li> <code>current_prompt</code>               (<code>VersionedPromptTemplate</code>)           \u2013            <p>The current prompt template to improve.</p> </li> <li> <code>rollout</code>               (<code>List[RolloutResultForAPO]</code>)           \u2013            <p>List of rollout results to base the critique on.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>The improved prompt text, or the original prompt if gradient computation fails.</p> </li> </ul>"},{"location":"algorithm-zoo/verl/","title":"VERL","text":"<p>Shortcut</p> <p>You can use the shortcut <code>agl.VERL(...)</code> to create a VERL instance.</p> <pre><code>import agentlightning as agl\n\nagl.VERL(...)\n</code></pre>"},{"location":"algorithm-zoo/verl/#installation","title":"Installation","text":"<pre><code>pip install agentlightning[verl]\n</code></pre> <p>Warning</p> <p>To avoid various compatibility issues, follow the steps in the installation guide to set up VERL and its dependencies. Installing VERL directly with <code>pip install agentlightning[verl]</code> can cause issues unless you already have a compatible version of PyTorch installed.</p> <p>Notes for Readers</p> <p>VERL in this article refers to a wrapper, provided by Agent-lightning, of the VERL framework. It's a subclass of agentlightning.Algorithm. To differentiate it from the VERL framework, all references to the VERL framework shall use the term \"VERL framework\", and all references to the Agent-lightning wrapper shall be highlighted with a link.</p>"},{"location":"algorithm-zoo/verl/#resources","title":"Resources","text":"<p>VERL expects no initial resources. The first LLM endpoint is directly deployed from the VERL configuration (<code>.actor_rollout_ref.model.path</code>). The resource key is always <code>main_llm</code>.</p> <p>VERL currently does not support optimizing multiple LLMs together.</p> <p>Note</p> <p>The resource type created by VERL is actually a ProxyLLM, a subclass of the LLM type. This object contains a URL template provided by VERL, with placeholders for rollout and attempt IDs. When a rollout begins on the agent side, the framework uses the current <code>rollout_id</code> and <code>attempt_id</code> to format this template, generating a final, unique endpoint URL. This URL points to VERL's internal proxy, allowing it to intercept and log all traffic for that specific attempt, for tracing and load balancing purposes. For agents created with the <code>@rollout</code> decorator, this resolution of the template is handled automatically (\"auto-stripped\"). Class-based agents will need to manually resolve the <code>ProxyLLM</code> using the rollout context.</p> <pre><code>proxy_llm = resources[\"main_llm\"]\nproxy_llm.get_base_url(rollout.rollout_id, rollout.attempt.attempt_id)\n</code></pre>"},{"location":"algorithm-zoo/verl/#customization","title":"Customization","text":"<p>Internally, VERL decomposes each agent execution into prompt\u2013response pairs via the Adapter and associates them with their corresponding reward signals as Triplet objects. The final scalar reward, derived from the last triplet in the trajectory, is propagated to all preceding triplets following the identical assignment strategy. This ensures that each triplet receives an identical reward signal and can be independently optimized as a valid RLHF trajectory within the VERL framework.</p> <p>At present, VERL does not expose fine-grained control over its reward propagation or credit assignment mechanisms. Users requiring customized reward shaping or trajectory decomposition are advised to clone and modify the VERL source implementation directly.</p>"},{"location":"algorithm-zoo/verl/#tutorials-using-verl","title":"Tutorials Using VERL","text":"<ul> <li>Train SQL Agent with RL - A practical example of training a SQL agent using VERL.</li> </ul>"},{"location":"algorithm-zoo/verl/#references-entrypoint","title":"References - Entrypoint","text":""},{"location":"algorithm-zoo/verl/#agentlightning.algorithm.verl","title":"<code>agentlightning.algorithm.verl</code>","text":""},{"location":"algorithm-zoo/verl/#agentlightning.algorithm.verl.VERL","title":"<code>VERL</code>","text":"<p>               Bases: <code>Algorithm</code></p> <p>VERL-powered algorithm that delegates training to the VERL PPO runner.</p> <p>Warning</p> <p>Advanced customisation currently requires copying the VERL source and modifying it directly. Native hooks for overriding training behaviour will land in a future release.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Dictionary mirroring the overrides passed to the VERL CLI. The overrides are merged with VERL's packaged defaults via Hydra before launching training.</p> </li> </ul> <p>Examples:</p> <pre><code>from agentlightning.algorithm.verl import VERL\n\nalgorithm = VERL(\n    config={\n        \"algorithm\": {\n            \"adv_estimator\": \"grpo\",\n            \"use_kl_in_reward\": False,\n        },\n        \"data\": {\n            \"train_batch_size\": 32,\n            \"max_prompt_length\": 4096,\n            \"max_response_length\": 2048,\n        },\n        \"actor_rollout_ref\": {\n            \"rollout\": {\n                \"tensor_model_parallel_size\": 1,\n                \"n\": 4,\n                \"log_prob_micro_batch_size_per_gpu\": 4,\n                \"multi_turn\": {\"format\": \"hermes\"},\n                \"name\": \"vllm\",\n                \"gpu_memory_utilization\": 0.6,\n            },\n            \"actor\": {\n                \"ppo_mini_batch_size\": 32,\n                \"ppo_micro_batch_size_per_gpu\": 4,\n                \"optim\": {\"lr\": 1e-6},\n                \"use_kl_loss\": False,\n                \"kl_loss_coef\": 0.0,\n                \"entropy_coeff\": 0,\n                \"clip_ratio_low\": 0.2,\n                \"clip_ratio_high\": 0.3,\n                \"fsdp_config\": {\n                    \"param_offload\": True,\n                    \"optimizer_offload\": True,\n                },\n            },\n            \"ref\": {\n                \"log_prob_micro_batch_size_per_gpu\": 8,\n                \"fsdp_config\": {\"param_offload\": True},\n            },\n            \"model\": {\n                \"path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n                \"use_remove_padding\": True,\n                \"enable_gradient_checkpointing\": True,\n            },\n        },\n        \"trainer\": {\n            \"n_gpus_per_node\": 1,\n            \"val_before_train\": True,\n            \"critic_warmup\": 0,\n            \"logger\": [\"console\", \"wandb\"],\n            \"project_name\": \"AgentLightning\",\n            \"experiment_name\": \"calc_x\",\n            \"nnodes\": 1,\n            \"save_freq\": 64,\n            \"test_freq\": 32,\n            \"total_epochs\": 2,\n        },\n    }\n)\ntrainer.fit(algorithm, train_dataset=my_train_dataset)\n</code></pre>"},{"location":"algorithm-zoo/verl/#agentlightning.algorithm.verl.VERL.get_client","title":"<code>get_client()</code>","text":"<p>Create a client bound to the VERL-managed Agent Lightning server.</p> Deprecated <p>Since v0.2.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.algorithm.verl.VERL.run","title":"<code>run(train_dataset=None, val_dataset=None)</code>","text":"<p>Launch the VERL PPO entrypoint with the configured runtime context.</p> <p>Parameters:</p> <ul> <li> <code>train_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataset forwarded to VERL for training.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional dataset forwarded to VERL for evaluation.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If required dependencies such as the store, LLM proxy, or adapter have been garbage-collected when using the V1 execution mode.</p> </li> </ul>"},{"location":"algorithm-zoo/verl/#references-implementation","title":"References - Implementation","text":""},{"location":"algorithm-zoo/verl/#agentlightning.verl","title":"<code>agentlightning.verl</code>","text":"<p>This package contains a hacky integration of VERL with Agent Lightning.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentLightningTrainer","title":"<code>AgentLightningTrainer</code>","text":"<p>               Bases: <code>RayPPOTrainer</code></p> <p>Specialized PPO trainer for agent-based reinforcement learning.</p> <p>This trainer is designed specifically for scenarios where the model interacts with external environments, tools, or APIs through an AgentLightningServer. It simplifies the training loop by removing the complex conditional logic present in the original RayPPOTrainer and focusing on the agent mode workflow.</p> <p>Key differences from RayPPOTrainer:</p> <ol> <li>Uses AgentModeDaemon for server communication</li> <li>Simplified data flow without pop/union operations</li> <li>Direct batch processing through agent daemon</li> <li>Streamlined validation using agent_mode validation</li> </ol>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon","title":"<code>AgentModeDaemon</code>","text":"<p>AgentModeDaemon using the AgentLightningServer SDK.</p> <p>This class manages the server lifecycle, task queueing, and results retrieval, while also running a proxy server for LLM requests. It maintains the original interface for compatibility with the RayPPOTrainer.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.clear_data_and_server","title":"<code>clear_data_and_server()</code>","text":"<p>Resets the internal state of the daemon for the next run.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.get_test_metrics","title":"<code>get_test_metrics()</code>","text":"<p>Calculates and returns metrics for a validation run.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.get_train_data_batch","title":"<code>get_train_data_batch(max_prompt_length, max_response_length, device)</code>","text":"<p>Processes completed rollouts to generate a training data batch.</p> <p>This function reconstructs the logic from the original AgentModeDaemon, using data retrieved from the new server architecture. It handles padding, truncation, and tensor creation for the PPO training loop.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.run_until_all_finished","title":"<code>run_until_all_finished(verbose=True)</code>","text":"<p>Synchronously waits for all queued tasks to be completed and reported.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.set_up_data_and_server","title":"<code>set_up_data_and_server(data, server_addresses, is_train=True)</code>","text":"<p>Synchronous wrapper for setting up data and server resources.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.AgentModeDaemon.start","title":"<code>start()</code>","text":"<p>Starts the main AgentLightningServer and the proxy server.</p>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.get_left_padded_ids_and_attention_mask","title":"<code>get_left_padded_ids_and_attention_mask(ids, max_length, pad_token_id)</code>","text":"<p>Left-pad (or truncate) a sequence of token IDs to a fixed length, and build the corresponding attention mask.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[int]</code>)           \u2013            <p>the original list of token IDs.</p> </li> <li> <code>max_length</code>               (<code>int</code>)           \u2013            <p>desired total length after padding/truncation.</p> </li> <li> <code>pad_token_id</code>               (<code>int</code>)           \u2013            <p>ID to use for padding.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>padded_ids</code> (              <code>any</code> )          \u2013            <p>list of length == max_length.</p> </li> <li> <code>attention_mask</code> (              <code>any</code> )          \u2013            <p>list of same length: 1 for non-pad tokens, 0 for pads.</p> </li> </ul>"},{"location":"algorithm-zoo/verl/#agentlightning.verl.get_right_padded_ids_and_attention_mask","title":"<code>get_right_padded_ids_and_attention_mask(ids, max_length, pad_token_id)</code>","text":"<p>Right-pad (or truncate) a sequence of token IDs to a fixed length, and build the corresponding attention mask.</p> <p>Parameters:</p> <ul> <li> <code>ids</code>               (<code>List[int]</code>)           \u2013            <p>the original list of token IDs.</p> </li> <li> <code>max_length</code>               (<code>int</code>)           \u2013            <p>desired total length after padding/truncation.</p> </li> <li> <code>pad_token_id</code>               (<code>int</code>)           \u2013            <p>ID to use for padding.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>padded_ids</code> (              <code>any</code> )          \u2013            <p>list of length == max_length.</p> </li> <li> <code>attention_mask</code> (              <code>any</code> )          \u2013            <p>list of same length: 1 for non-pad tokens, 0 for pads.</p> </li> </ul>"},{"location":"deep-dive/birds-eye-view/","title":"The Bird's Eye View of Agent-lightning","text":"<p>High Volume of Information Ahead</p> <p>This article provides an in-depth exploration of the Agent-lightning architecture. It is not intended as a beginner\u2019s guide or usage tutorial.</p> <p>This article summarizes how Agent-lightning (as of v0.2) wires the Algorithm, Runner, and LightningStore loop together and shows where auxiliary components (the Tracer, Adapter, and LLM Proxy) plug into the loop. Each section provides a diagram for a different perspective of the system.</p>"},{"location":"deep-dive/birds-eye-view/#algorithm-runner-store-data-flow","title":"Algorithm \u2194 Runner \u2194 Store data flow","text":"<p>At its heart, Agent-lightning is built on three main components that work in a coordinated loop:</p> <ul> <li>Algorithm: The \"brain\" of the system. It decides what tasks to run, learns from the results, and updates resources (like AI models or prompts).</li> <li>Runner: The \"worker\" of the system. It executes tasks assigned by the algorithm, runs the agent, and records the results.</li> <li>LightningStore: The central \"database\" and message queue. It acts as the single source of truth, storing tasks, results, and resources, and enabling communication between the Algorithm and Runner.</li> </ul> <p>The typical data flow in a training loop is as follows: The Algorithm enqueues tasks (called Rollouts) into the LightningStore. A Runner then dequeues a task, executes it, and streams the results (called Spans) back to the store. Once the task is complete, the algorithm can query the new data from the store to learn and update its resources.</p> <p>The diagram below shows this fundamental interaction in a simple, non-parallel setup.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Algo as Algorithm\n    participant Store as LightningStore\n    participant Runner\n    participant Agent\n\n    loop Over the dataset\n        Algo--&gt;&gt;Store: add_resources + enqueue_rollout\n        Store--&gt;&gt;Runner: dequeue_rollout \u2192 AttemptedRollout\n        Store--&gt;&gt;Runner: get_latest_resources\n        Runner--&gt;&gt;Store: update_attempt(\"running\", worker_id)\n        Runner-&gt;&gt;Agent: rollout + resources\n        Agent-&gt;&gt;Runner: reward / spans\n        Runner--&gt;&gt;Store: add_span or add_otel_span\n        Runner--&gt;&gt;Store: update_attempt(\"finished\", status)\n        Store--&gt;&gt;Algo: query_rollouts + spans\n        Algo--&gt;&gt;Algo: Update resources (optional)\n    end</code></pre> <p>Solid lines represent direct calls, while dashed lines are asynchronous or long-running operations.</p>"},{"location":"deep-dive/birds-eye-view/#key-terminology","title":"Key Terminology","text":"<p>We define the following terms, which may be helpful for understanding the diagram above.</p> <ul> <li>Resources: A collection of assets to be tuned or trained. Agents perform rollouts against resources and collect span data. Algorithms use those data to update the resources. In RL training, the resources are a tunable model. In prompt tuning, the resources are prompt templates.</li> <li>Rollout: A unit of work that an agent performs against a resource. A rollout (noun) can be incomplete, in which case it is also known as a task, sample, or job (these terms are used interchangeably). The agent executes its own defined workflow against the rollout \u2014 the process is also called \"to rollout\" (verb). After execution, the rollout (noun) is considered complete.</li> <li>Attempt: A single execution of a rollout. One rollout can have multiple attempts in case of failures or timeouts.</li> <li>Span: During the rollout, the agent can generate multiple spans (also known as \"traces\" or \"events\"). The recorded spans are collected in the store, which is crucial for understanding agent behavior and optimizing agents.</li> <li>Reward: A special span that is defined as a number judging the quality of the rollout during some period of the rollout.</li> <li>Dataset: A collection of incomplete rollouts (i.e., tasks) for the agent to process. The dual datasets (train, val) serve as the initial input for the algorithm to enqueue the first batch of rollouts.</li> </ul>"},{"location":"deep-dive/birds-eye-view/#store","title":"Store","text":"<p>As discussed previously, the LightningStore is the central hub for all data in Agent-lightning. The store exposes a set of APIs for algorithms and runners to interact with the data; the most important ones are:</p> <pre><code>from agentlightning.types import AttemptedRollout, ResourcesUpdate, Span, TaskInput\n\nclass LightningStore:\n\n    async def enqueue_rollout(self, input: TaskInput, ...) -&gt; Rollout: ...\n\n    async def dequeue_rollout(self) -&gt; AttemptedRollout | None: ...\n\n    async def add_span(self, span: Span) -&gt; Span: ...\n\n    async def get_latest_resources(self) -&gt; Optional[ResourcesUpdate]: ...\n\n    async def wait_for_rollouts(self, rollout_ids: List[str], ...): ...\n\n    async def query_spans(self, rollout_id: str, ...): ...\n\n    async def update_attempt(self, rollout_id: str, attempt_id: str, status: str, ...): ...\n\n    ...\n</code></pre> <p>These interfaces operate on <code>AttemptedRollout</code>, <code>ResourcesUpdate</code>, <code>Span</code>, and <code>TaskInput</code> instances from <code>agentlightning.types</code>.</p> <p>As the APIs show, the store essentially provides a queue for rollouts and storage for resources, spans, and attempts. Developers should implement the store carefully to ensure data integrity and consistency, especially when multiple runners work in parallel across multiple attempts.</p> <p>The store is designed to be extensible. Users can implement their own store by inheriting from <code>LightningStore</code> and overriding methods. Agent-lightning provides a few reference implementations, such as <code>InMemoryLightningStore</code> (default) and <code>SqliteLightningStore</code> (under construction). When parallelized, the store may need special wrappers to ensure thread/process safety or delegate computation to a store in another process or machine.</p>"},{"location":"deep-dive/birds-eye-view/#supporting-components-in-the-loop","title":"Supporting Components in the Loop","text":"<p>While the core loop is simple, Agent-lightning provides several components to make development easier and more powerful.</p>"},{"location":"deep-dive/birds-eye-view/#tracer","title":"Tracer","text":"<p>The <code>Tracer</code> is a component within the <code>Runner</code> that records detailed spans (events) during an agent's execution and sends them to the <code>LightningStore</code>. Instead of requiring the agent to manually log every span, the tracer automatically instruments key methods (e.g., LLM calls) and captures their inputs, outputs, and metadata. This provides a detailed log of the agent's behavior with minimal effort.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Store\n    participant Runner\n    participant Tracer\n    participant Agent\n\n    Note over Runner,Tracer: Runner manages tracer as member\n\n    Tracer-&gt;&gt;Agent: Apply instrumentation\n    loop Until no more rollouts\n        Store--&gt;&gt;Runner: dequeue_rollout \u2192 AttemptedRollout\n        Store--&gt;&gt;Runner: get_latest_resources\n        Runner-&gt;&gt;Agent: training_rollout / validation_rollout\n        loop For each finished span\n            Agent--&gt;&gt;Tracer: openai.chat.completion invoked&lt;br&gt;agent.execute invoked&lt;br&gt;...\n            Agent-&gt;&gt;Tracer: emit intermediate reward\n            Tracer--&gt;&gt;Store: add_otel_span(rollout_id, attempt_id, span)\n        end\n        Agent-&gt;&gt;Runner: final reward + extra spans (if any)\n        Runner--&gt;&gt;Store: add_span(rollout_id, attempt_id, span)\n        Runner--&gt;&gt;Store: update_attempt(status)\n    end\n    Tracer-&gt;&gt;Agent: Unapply instrumentation</code></pre> <p>The above diagram shows the overall data flow between store, tracer and agent. In realistic, it's a bit more complicated than that. Spans are not emitted actively by the agent; they are intercepted by the tracer by hooking and instrumenting key methods used in the agents.  The tracer uses a callback (called exporter) to monitor events and log to the store. Before a rollout starts, the runner enters a <code>trace_context</code> before invoking the agent, wiring store identifiers into the tracer. Each span completion streams back to the store through <code>LightningSpanProcessor</code>, so the agent\u2019s instrumentation lands in <code>add_otel_span</code>. If the agent\u2019s rollout method returns a numeric reward, the runner emits one more OpenTelemetry span before finalizing the attempt.</p>"},{"location":"deep-dive/birds-eye-view/#hooks","title":"Hooks","text":"<p><code>Hook</code> implementations are user-defined callback functions that allow you to augment a <code>Runner</code>'s behavior at specific points in its lifecycle. You can use hooks to add custom logging, set up resources before a rollout begins, or tear them down after it ends. Hooks can be triggered at four key moments: <code>on_rollout_start</code>, <code>on_trace_start</code>, <code>on_trace_end</code>, and <code>on_rollout_end</code>.</p> <p>Users should pay special attention to the difference between <code>on_trace_end</code> and <code>on_rollout_end</code>. The former is called right before the tracer exits the trace context, while the latter is called after the runner processes the final leftover rewards and spans, and finalizes the attempt in the store.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Store\n    participant Hooks\n    participant Runner\n    participant Tracer\n    participant Agent\n\n    Note over Runner,Hooks: Runner manages hooks as member\n\n    loop Until no more rollouts\n        Store--&gt;&gt;Runner: dequeue_rollout \u2192 AttemptedRollout\n        Store--&gt;&gt;Runner: get_latest_resources\n\n        Runner-&gt;&gt;Hooks: on_rollout_start(agent, runner, rollout)\n        Runner-&gt;&gt;Agent: training_rollout / validation_rollout\n        Tracer-&gt;&gt;Agent: enter_trace_context\n        activate Tracer\n        Runner-&gt;&gt;Hooks: on_trace_start(agent, runner, tracer, rollout)\n        Note over Runner,Agent: Agent rollout omitted\n        Runner-&gt;&gt;Hooks: on_trace_end(agent, runner, tracer, rollout)\n        Tracer-&gt;&gt;Agent: exit_trace_context\n        deactivate Tracer\n        Agent-&gt;&gt;Runner: final reward + extra spans (if any)\n        Runner--&gt;&gt;Store: add_span(rollout_id, attempt_id, span)\n        Runner-&gt;&gt;Hooks: on_rollout_end(agent, runner, rollout, status)\n    end</code></pre>"},{"location":"deep-dive/birds-eye-view/#adapter","title":"Adapter","text":"<p>The <code>Adapter</code> is a component used by the <code>Algorithm</code> to transform raw data from the <code>LightningStore</code> into a format suitable for learning. Runners stream raw spans into the store during execution. Later, the algorithm queries these spans and uses an adapter to convert them into structured data, like training examples for a reinforcement learning model.</p> <p>For instance, the <code>TracerTraceToTriplet</code> processes OpenTelemetry spans to create <code>(prompt, response, reward)</code> triplets, which are the fundamental data structure for many RL fine-tuning algorithms.</p> <pre><code>flowchart LR\n    Runner -- (1) add_otel_span --&gt; Store\n    Store -- (2) query_spans --&gt; Algorithm\n    Algorithm -- (3) spans --&gt; Adapter\n    Adapter -- (4) transformed data --&gt; Algorithm</code></pre>"},{"location":"deep-dive/birds-eye-view/#llm-proxy","title":"LLM Proxy","text":"<p>The <code>LLMProxy</code> is an optional bridge component that sits between an agent and the algorithms' resources. It acts as a centralized endpoint for all LLM calls. Usually the proxy URL is added to the store as a special resource, so that the <code>Runner</code> can fetch it along with other resources when dequeuing a rollout. During rollouts, the runner invokes the proxy's HTTP endpoint instead of calling a model backend directly.</p> <p>This design offers several benefits:</p> <ol> <li>Instrumentation: It automatically captures detailed traces of LLM interactions (prompts, responses, metadata) and sends them to the store, complementing the tracer, especially when the agent's code is hard to instrument directly.</li> <li>Backend Abstraction: It provides a unified interface for various LLM backends (OpenAI, Anthropic, local models) and can add features like retry logic, rate limiting, and caching.</li> <li>Resource Management: The algorithm can dynamically update which LLM the agent uses (e.g., swapping to a newly fine-tuned model) by simply swapping the backend model the proxy is using, without interrupting the agent's code.</li> </ol> <p>The benefits above seem to be all discussed within the context of model fine-tuning. As a matter of fact, the proxy can be useful for prompt tuning as well. The algorithm can register one of the following two types of endpoints into the proxy:</p> <ol> <li>Endpoint served by the algorithm: If the algorithm is internally updating the LLM weights (e.g., RL), it can launch an LLM inference engine (i.e., a model server) and register the endpoint URL with the proxy. The proxy then forwards all LLM calls to that endpoint.</li> <li>Third-party LLM endpoint: If the algorithm is not updating the LLM weights (e.g., prompt tuning), it can register a third-party LLM endpoint into the proxy.</li> </ol> <p>We show a diagram below that illustrates how the proxy fits into the overall data flow.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Algo as Algorithm\n    participant LLMProxy as LLM Proxy\n    participant Store\n    participant Runner\n    participant Agent\n\n    Note over Algo,LLMProxy: Algorithm manages LLMProxy as member\n\n    loop Over the Dataset\n        Algo-&gt;&gt;Algo: Launch LLM Inference Engine&lt;br&gt;(optional)\n        Algo-&gt;&gt;LLMProxy: Register Inference Engine&lt;br&gt;(optional)\n        Algo--&gt;&gt;Store: enqueue_rollout\n        LLMProxy-&gt;&gt;Store: Proxy URL added as Resource\n        Store--&gt;&gt;Runner: dequeue_rollout \u2192 AttemptedRollout\n        Store--&gt;&gt;Runner: get_latest_resources\n        Runner-&gt;&gt;Agent: rollout + resources&lt;br&gt;(LLM Proxy URL as resource)\n        loop Defined by Agent\n            Agent--&gt;&gt;LLMProxy: LLM calls\n            activate LLMProxy\n            LLMProxy--&gt;&gt;Store: add_span or add_otel_span\n            LLMProxy--&gt;&gt;Agent: LLM responses\n            deactivate LLMProxy\n            Agent--&gt;&gt;Runner: rewards\n            Runner--&gt;&gt;Store: add_span or add_otel_span\n        end\n        Runner--&gt;&gt;Store: update_attempt(\"finished\", status)\n        Store--&gt;&gt;Algo: query_rollouts + spans\n        Algo--&gt;&gt;Algo: Update LLM Weights&lt;br&gt;(optional)\n    end</code></pre> <p>In this diagram, the store receives spans from both the proxy and the runner. We will see a problem later with parallelism where the proxy and runner are in different machines, and spans need to obtain a special counter from the store to ensure the ordering of spans.</p>"},{"location":"deep-dive/birds-eye-view/#trainer","title":"Trainer","text":"<p>The Trainer is the high-level orchestrator that initializes and connects all major components -- Algorithm, Runner, LightningStore, Tracer, Adapter, LLM Proxy, and Hook. The components can have a lifecycle as long as the trainer. The trainer manages their lifecycles and handles dependency injection, ensuring that every part of the system operates within a consistent and shared environment.</p> <p>Below, we demonstrate how the components relate to each other and their roles. We first clarify the roles and relationships shown in the diagram:</p> <ol> <li>Owns: components that the trainer constructs and manages directly (e.g., runner, tracer).</li> <li>Injects: components passed into others as dependencies.</li> <li>References: weak links for coordination without ownership.</li> <li>Uses: components that are temporarily interacted with.</li> </ol> <p>For example, the LightningStore is injected into the Algorithm and Runner. The Tracer and LitAgent are injected into the runner. The Adapter and LLM Proxy are injected into the algorithm. The store is further injected into the tracer, adapter and LLM proxy by the runner and algorithm respectively.</p> <pre><code>flowchart TD\n    %% === Left side: Algorithm domain ===\n    subgraph L[\"Algorithm Side\"]\n        Algorithm[\"Algorithm&lt;br&gt;(no default)\"]\n        Adapter[\"Adapter&lt;br&gt;(TracerTraceToTriplet*)\"]\n        LLMProxy[\"LLM Proxy&lt;br&gt;(no default)\"]\n        Algorithm -.injects.-&gt; Adapter\n        Algorithm -.injects.-&gt; LLMProxy\n    end\n    linkStyle 0,1 stroke:#896978,stroke-width:2px;\n\n    %% === Middle: Core trainer and store ===\n    subgraph M[\"Core\"]\n        Trainer[\"Trainer\"]\n        Store[\"LightningStore&lt;br&gt;(InMemory* default)\"]\n        Trainer --has--&gt; Algorithm\n        Trainer --has--&gt; Store\n        Trainer --has--&gt; Adapter\n        Trainer --has--&gt; LLMProxy\n    end\n    linkStyle 2,3,4,5 stroke:#839791,stroke-width:2px;\n\n    %% === Right side: Runner side ===\n    subgraph R[\"Runner Side\"]\n        Runner[\"Runner&lt;br&gt;(LitAgentRunner* default)\"]\n        Tracer[\"Tracer&lt;br&gt;(AgentOpsTracer*)\"]\n        Hooks[\"Hooks (empty default)\"]\n        Agent[\"Agent&lt;br&gt;(LitAgent*)\"]\n        Runner -.injects.-&gt; Tracer\n        Runner -.injects.-&gt; Store\n        Runner -.injects.-&gt; Agent\n        Runner -.injects.-&gt; Hooks\n        Tracer -.injects.-&gt; Store\n        Hooks -.uses.-&gt; Runner\n        Hooks -.uses.-&gt; Agent\n        Hooks -.uses.-&gt; Tracer\n    end\n    linkStyle 6,7,8,9,10 stroke:#896978,stroke-width:2px;\n    linkStyle 11,12,13 stroke:#7a89c2,stroke-width:2px;\n\n    %% === Cross-section connections ===\n    Trainer --has--&gt; Runner\n    Trainer --has--&gt; Tracer\n    Trainer --has--&gt; Hooks\n    Trainer --uses--&gt; Agent\n    Algorithm -.injects.-&gt; Store\n    LLMProxy -.injects.-&gt; Store\n    Agent -.references.-&gt; Trainer\n    Runner -.references.-&gt; Trainer\n    Algorithm -.references.-&gt; Trainer\n    linkStyle 14,15,16 stroke:#839791,stroke-width:2px;\n    linkStyle 17,20,21,22 stroke:#7a89c2,stroke-width:2px;\n    linkStyle 18,19 stroke:#896978,stroke-width:2px;\n\n    style L fill:none;\n    style M fill:none;\n    style R fill:none;</code></pre>"},{"location":"deep-dive/birds-eye-view/#putting-it-all-together-a-reinforcement-learning-example-verl","title":"Putting It All Together: A Reinforcement Learning Example (VERL)","text":"<p>VERL shows how an algorithm consumes the shared infrastructure. For historical reasons, code lives in <code>agentlightning.algorithm.verl</code> and <code>agentlightning.verl</code>. The latter is legacy and reuses terms like <code>Trainer</code> in confusing ways. The former is a thin wrapper that conforms to the new algorithm interface. Future versions will merge the two.</p> <p>Reinforcement learning aims to learn a policy that takes actions in states to maximize expected reward. For agents, the policy is usually a language model. Inputs are prompts (state). Outputs are generated text (action). A numeric score judges quality (reward). The <code>(state, action, reward)</code> triplet is the basic learning unit.</p> <p>In Agent-lightning, the environment is implicit in the agent\u2019s workflow, which orchestrates one or more LLM calls and often self-judges using rules or additional model calls. During a rollout, the agent emits spans that contain everything needed for RL training, including LLM call traces and numeric judge/reward signals. The \"algorithm\", on the other hand, have more responsibilities.</p> <ol> <li>Providing a language model deployment that is currently learning and improving for the agent to interact with;</li> <li>Preparing the tasks that the agents will perform;</li> <li>Querying the spans generated, extracting triplets, and converting them into a format that the underlying RL library can consume;</li> <li>Updating the language model based on the learning signals.</li> </ol> <p>In the VERL integration, the algorithm launches a chat completion endpoint using <code>vLLM</code> and wraps training with <code>FSDP</code> for distributed optimization. It enqueues tasks from the dataset. After rollouts finish, it queries spans and converts them to triplets with <code>TracerTraceToTriplet</code>. VERL\u2019s native training loop then consumes these triplets to update model weights. The workflow can be summarized in the following diagram.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant vLLM as vLLM Chat&lt;br&gt;Completion Endpoint\n    participant FSDP as FSDP / Megatron&lt;br&gt;Weights Optimizer\n    participant Algo as Algorithm&lt;br&gt;Main Controller&lt;br&gt;(Main Process)\n    participant Adapter as TracerTraceToTriplet\n    participant LLMProxy as LLM Proxy\n    participant Store as LightningStore\n    participant Runner as Runner + Agent\n\n    Note over Algo,LLMProxy: LLMProxy and Adapter are injected by Trainer as member\n    Note over vLLM,Algo: Algorithm creates and owns vLLM and FSDP\n\n    loop Over the Dataset in Batches\n        Algo-&gt;&gt;vLLM: Create Chat Completion Endpoint\n        activate vLLM\n        vLLM-&gt;&gt;LLMProxy: Registered as Backend Endpoint\n        LLMProxy-&gt;&gt;Store: Proxy URL added as Resource\n        par Over data samples in the batch\n            Algo--&gt;&gt;Store: enqueue_rollout\n            Store--&gt;&gt;Runner: Dequeue Rollout +&lt;br&gt;Resources (i.e., URL)\n            loop One Rollout Attempt\n                Runner--&gt;&gt;LLMProxy: LLM calls\n                LLMProxy--&gt;&gt;vLLM: Forwarded LLM calls\n                vLLM--&gt;&gt;LLMProxy: LLM responses\n                LLMProxy--&gt;&gt;Store: add_span / add_otel_span\n                LLMProxy--&gt;&gt;Runner: Forwarded LLM responses\n                Runner--&gt;&gt;Store: add_span / add_otel_span &lt;br&gt; (by tracer, including rewards)\n            end\n            Runner--&gt;&gt;Store: update_attempt(\"finished\", status)\n        end\n        Algo--&gt;&gt;Store: Poll for completed rollouts + spans\n        Algo-&gt;&gt;vLLM: Chat Completion Endpoint Sleeps\n        deactivate vLLM\n        Algo-&gt;&gt;Adapter: adapt(spans)\n        Adapter-&gt;&gt;FSDP: Triplets (state, action, reward)\n        activate FSDP\n        FSDP--&gt;&gt;Algo: Updated LLM weights\n        deactivate FSDP\n    end</code></pre> <p>Notes:</p> <ol> <li> <p>There are interactions between different components injected into or owned by algorithms in the diagram, such as the output of the adapter feeding into the FSDP optimizer. This is for simplicity of illustration and slightly different from the actual implementation, where it's the algorithm main controller that orchestrates the data flow between components.</p> </li> <li> <p>On mapping to VERL. VERL uses a classic RLHF setup where each action is a single token, the state is the full conversation history up to that token, and reward is given at the end. This is very different from our setup where each action is actually  a chunk of text, although they are both called RL! Therefore, after the adapter produces triplets, the algorithm converts each <code>(state, action, reward)</code> into a VERL trajectory (<code>DataProto</code>) with keys like <code>input_ids</code>, <code>position_ids</code>, <code>attention_mask</code>, and <code>token_level_scores</code>. That conversion happens after triplet generation and is not shown in the diagram.</p> </li> </ol>"},{"location":"deep-dive/birds-eye-view/#execution-strategies-and-parallelism","title":"Execution Strategies and Parallelism","text":"<p>Readers might have observed from the diagram above that there is absolutely no communication between (1) runner and agents and (2) algorithm. The only overlap of them is the Trainer and LightningStore. This observation is very clear with the diagram within the trainer section. This design allows us to flexibly scale the runner and algorithm independently, which is crucial for large-scale training.</p> <p>Agent-lightning packages two executable bundles: a runner bundle (Runner, Tracer, Hook, LitAgent) and an algorithm bundle (Algorithm, Adapter, LLM Proxy). Both share the LightningStore. The trainer initializes and connects the bundles.</p> <pre><code>graph TD\n    subgraph Runner_Side[\"Runner Bundle\"]\n        direction LR\n        R[Runner] --- T[Tracer] --- H[Hooks] --- A1[Agent]\n    end\n\n    subgraph Algorithm_Side[\"Algorithm Bundle\"]\n        direction LR\n        ALG[Algorithm] --- AD[Adapter] --- LLM[LLM Proxy]\n    end\n\n    S[(Store)]\n    TR[Trainer]\n\n    Runner_Side &lt;--&gt; S\n    Algorithm_Side &lt;--&gt; S\n    TR --&gt; Runner_Side\n    TR --&gt; Algorithm_Side\n\n    linkStyle 0,1,2,3,4 opacity:0;</code></pre> <p>An execution strategy, defined and owned by the trainer, governs how algorithm and runner bundles are placed, connected, scaled, and aborted. It serves four primary purposes.</p> <p>Execution strategies first determine bundle placement \u2014 whether the two bundles run in the same thread, process, machine, or across separate machines. They also define store management, wrapping the store and specifying how data is shared between bundles.</p> <p>In terms of scalability, the strategy can replicate the runner bundle across multiple threads, processes, or machines to expand throughput on the runner side. The algorithm side remains single-process due to the complexity of parallelization. Mature frameworks such as DeepSpeed and Megatron already support distributed model training, so scaling of the algorithm bundle is delegated to those implementations.</p> <p>Abort handling is another core responsibility. Aborts may be triggered by normal exits, failures in either bundle, or user interrupts. The trainer must include cancellation interfaces for the bundles so that bundles can be cleanly aborted. When the algorithm bundle exits normally, the strategy signals the runner bundle to terminate. If the runner exits first, no signal is sent to the algorithm, as it may still be processing completed rollouts. In cases of failure or user interruption, the strategy signals both bundles to abort; if a bundle fails to respond, the strategy should attempt a forceful termination.</p> <p>Agent-lightning currently provides two execution strategies: shared-memory and client-server, described in the following sections.</p>"},{"location":"deep-dive/birds-eye-view/#shared-memory-strategy","title":"Shared-memory Strategy","text":"<p><code>SharedMemoryExecutionStrategy</code> runs algorithm and runner bundles as threads in one process. The strategy wraps the store with <code>LightningStoreThreaded</code>, which guards calls with a lock for safe concurrency.</p> <p>This is good for lightweight debugging because components share one Python heap and avoid serialization. It is not suitable for heavy RL training or compute-intensive agents.</p> <pre><code>flowchart TB\n    subgraph MainProcess\n        direction TB\n        subgraph AlgorithmThread [Thread 0]\n            Algorithm[Algorithm bundle]\n        end\n        subgraph RunnerThread1 [Thread 1]\n            Runner1[Runner bundle #1]\n        end\n        subgraph RunnerThread2 [Thread 2]\n            Runner2[Runner bundle #2]\n        end\n        subgraph RunnerThread3 [Thread 3]\n            RunnerN[Runner bundle #N]\n        end\n        LightningStoreFacade[LightningStoreThreaded]\n        BaseStore[Underlying LightningStore]\n    end\n    Algorithm -- async calls --&gt; LightningStoreFacade\n    Runner1 -- async calls --&gt; LightningStoreFacade\n    Runner2 -- async calls --&gt; LightningStoreFacade\n    RunnerN -- async calls --&gt; LightningStoreFacade\n    LightningStoreFacade --&gt;|thread-safe delegates| BaseStore</code></pre> <p>You can configure which role runs on the main thread. If the main thread runs the algorithm, it is able to spawn multiple runner threads. If it runs a runner, <code>n_runners</code> must be 1 and the runner lives on the main thread.</p>"},{"location":"deep-dive/birds-eye-view/#client-server-strategy","title":"Client-server Strategy","text":"<p><code>ClientServerExecutionStrategy</code> splits concerns across processes. The algorithm bundle starts a <code>LightningStoreServer</code> (HTTP API) that wraps the underlying store. Runners connect via <code>LightningStoreClient</code> to call the same interface over REST. The server embeds a client to support algorithm-launched subprocesses (e.g., an LLM proxy worker) that need to talk back to the algorithm\u2019s process through the same API.</p> <p>Currently this design introduces an extra wrapper in the Server side (as shown in the diagram), which helps debugging and improves fault tolerance. We might revisit this design in the future and enforce the client to be the only way to communicate with the store.</p> <pre><code>flowchart TD\n    subgraph Algorithm Process Group\n        subgraph StoreServer[LightningStoreServer]\n            StoreHttpClient[HTTP Client]\n            StoreHttpServer[HTTP Server]\n            StoreWrapper[LightningStore Wrapper]\n            StoreHttpClient -- HTTP --&gt; StoreHttpServer\n        end\n        subgraph Algorithm Bundle\n            Algorithm[Algorithm Main Process]\n            subgraph Another subprocess\n                LLMProxy[LLM Proxy]\n            end\n        end\n        LLMProxy -- async calls --&gt; StoreHttpClient\n        Algorithm -- async calls --&gt; StoreWrapper\n    end\n    subgraph RunnerSide [\"Runner Side\"]\n        subgraph Runner Process 1\n            Runner1[Runner bundle #1]\n            Runner1 -- async calls --&gt; LightningStoreClient1\n            LightningStoreClient1[LightningStoreClient]\n        end\n        subgraph Runner Process 2\n            Runner2[Runner bundle #2]\n            Runner2 -- async calls --&gt; LightningStoreClient2\n            LightningStoreClient2[LightningStoreClient]\n        end\n        subgraph Runner Process N\n            RunnerN[Runner bundle #N]\n            RunnerN -- async calls --&gt; LightningStoreClientN\n            LightningStoreClientN[LightningStoreClient]\n        end\n    end\n    LocalStore[Underlying LightningStore]\n    StoreHttpServer --&gt;|delegates| StoreWrapper\n    StoreWrapper --&gt;|delegates| LocalStore\n    LightningStoreClient1 -- HTTP --&gt; StoreHttpServer\n    LightningStoreClient2 -- HTTP --&gt; StoreHttpServer\n    LightningStoreClientN -- HTTP --&gt; StoreHttpServer\n\n    style RunnerSide fill:none;</code></pre>"},{"location":"deep-dive/birds-eye-view/#onlinecontinuous-learning","title":"Online/Continuous Learning","text":"<p>Continuous learning keeps the algorithm loop running while runners report tasks and spans opportunistically. Key differences from batch mode:</p> <ol> <li>The algorithm does not enqueue rollouts from a fixed dataset. Runners report tasks/rollouts and spans spontaneously.</li> <li>The algorithm can wait for rollouts with a expected set of rollout IDs, but more oftenly polls for new rollouts and spans or waits for a count to arrive.</li> <li>The <code>Runner</code> processes one rollout at a time via <code>step(task)</code> instead of exhausting a task queue. It notifies the store when starting a rollout so the store records it.</li> <li>A user or higher-level loop controls which resources the next step uses and when to retry.</li> </ol> <p>Spans, Adapter implementations, and the LLM Proxy work the same way.</p> <pre><code>sequenceDiagram\n    autonumber\n    actor User\n    participant Runner\n    participant Agent\n    participant Store as LightningStore\n    participant Algorithm\n\n    Note over Algorithm: Algorithm is long-running and loops continuously\n\n    loop Continuous Learning Loop\n        activate User\n        opt Decide what to do next\n            User--&gt;&gt;Store: get_resources_by_id\n            Store--&gt;&gt;User: Resources\n            User--&gt;&gt;User: Prepare input for next step\n        end\n        User-&gt;&gt;Runner: step(input, resources)\n        activate Runner\n        Runner--&gt;&gt;Store: Notify: start_rollout(input)\n        Runner-&gt;&gt;Agent: rollout(input, resources)\n        Agent--&gt;&gt;Runner: add_span / reward spans\n        Runner--&gt;&gt;Store: add_span or add_otel_span\n        Runner--&gt;&gt;Store: update_attempt(status=\"finished\")\n        deactivate Runner\n        deactivate User\n        Algorithm-&gt;&gt;Store: poll for new rollouts and spans\n        opt If there is enough new data\n            Store--&gt;&gt;Algorithm: new spans\n            Algorithm-&gt;&gt;Algorithm: adapt spans \u2192 learning signal\n            Algorithm-&gt;&gt;Store: update_resources\n        end\n    end</code></pre>"},{"location":"deep-dive/serving-llm/","title":"Serving LLMs under Agent-lightning","text":"<p>Agent-lightning focuses on data, learning signals, and control flow \u2014 not on running model inference. This deep dive explains how to serve a model alongside Agent-lightning so runners can call it reliably, how the LLM Proxy fits into the loop, and why token IDs matter if you care about correctness in training and evaluation.</p>"},{"location":"deep-dive/serving-llm/#general-background-on-llm-serving","title":"General background on LLM serving","text":"<p>Serving a model is essential if you want to train it, especially when you use the model\u2019s own generations as training data. We\u2019ll briefly review the general background to ensure all readers are aligned.</p> <p>Modern LLM servers solve a difficult scheduling problem: keeping GPUs fully utilized while handling prompts of different lengths, streaming tokens as they arrive, and fitting large KV caches into limited memory. Techniques like continuous batching and paged attention address these challenges. Continuous batching interleaves decoding across requests to reuse weights efficiently; with careful memory planning, it achieves major throughput gains without increasing latency. PagedAttention reduces KV-cache fragmentation so batching remains effective as sequences grow. See vLLM\u2019s PagedAttention paper and industry analyses for details. Balancing inference correctness and efficiency is difficult \u2014 a recent blog from Thinking Machines Labs highlights how inference nondeterminism ultimately affects training.</p> <p>Beyond scheduling, servers expose an HTTP API, often OpenAI-compatible (<code>/v1/chat/completions</code> and <code>/v1/responses</code>), which is itself a complex stack. In addition to text prompts and chat messages, the API defines many parameters and response fields such as tool calls, structured output, and multimodal support. Much effort has been put into implementing all these parameters for many frameworks. Popular engines like vLLM and SGLang ship with OpenAI-compatible frontends so you can reuse existing client code. Ollama and llama.cpp provide similar capabilities. However, because models differ internally, each framework interprets and implements the API slightly differently. Even with identical requests, the tokens passed to the model can vary substantially across frameworks.</p>"},{"location":"deep-dive/serving-llm/#what-agent-lightning-expects-from-a-served-llm","title":"What Agent-lightning expects from a served LLM","text":"<p>Most of the issues above either have workarounds or remain open research problems. Keep them in mind, but the key question is: what does Agent-lightning expect from a served LLM? The answer includes at least two things:</p> <ul> <li>An OpenAI-compatible Chat Completions or Responses endpoint the agent can call during rollouts.</li> <li>Optional training and debugging signals: logprobs, usage, and ideally token IDs. (OpenAI\u2019s public API exposes usage and logprobs, but not token IDs \u2014 more on why IDs matter later.)</li> </ul>"},{"location":"deep-dive/serving-llm/#launching-a-serving-framework","title":"Launching a serving framework","text":"<p>For many algorithms, you\u2019ll start an engine (e.g., vLLM or SGLang) before rollouts, then shut it down afterward to free GPU memory. Most frameworks provide a one-line \u201cserve\u201d command to launch the OpenAI-compatible server. You can use those to bring up <code>/v1/chat/completions</code> with your checkpoint, ensuring streaming and any required tool-calling features are enabled. A working example is shown in Unsloth SFT.</p> <p>Weight updates \u2014 which occur after each training step \u2014 are trickier. Some frameworks like vLLM support hot-updating model weights, but it\u2019s usually simpler and more reliable to restart the engine to load new weights. For medium-sized tasks (hundreds of rollouts taking 10+ minutes), the restart overhead (under 30 seconds) is typically negligible.</p> <p>If you\u2019re using Agent-lightning\u2019s VERL integration, the algorithm can manage the server automatically. The VERL framework intelligently allocates compute resources and wraps vLLM/SGLang behind an <code>AsyncLLMServer</code> abstraction. You can directly use this as the LLM endpoint for agents. Since VERL can spawn multiple vLLM replicas, using <code>LLMProxy</code> to manage them adds an additional safety layer.</p> <p>A full sequence diagram of how VERL interacts with the LLM server and proxy is available here.</p>"},{"location":"deep-dive/serving-llm/#llm-proxy","title":"LLM Proxy","text":"<p>The LLM Proxy is a utility class in Agent-lightning, built on LiteLLM, that sits between runners and your backend engine(s) or server(s). In Agent-lightning it acts as a single URL registered as a <code>Resource</code> in the store, offering three key benefits:</p> <ol> <li>Unified endpoint &amp; hot-swaps. You can redirect traffic between OpenAI, Anthropic, local vLLM/SGLang, or canary checkpoints without modifying agent code \u2014 simply repoint the proxy.</li> <li>First-class tracing. The proxy emits OpenTelemetry spans for every call and sends them to the <code>LightningStore</code>. It includes rollout and attempt identifiers in request headers so spans are correctly attributed. Sequence numbers are allocated monotonically via the store to prevent clock-skew issues and allow reliable reconstruction of execution trees.</li> <li>Token IDs. The proxy can return prompt and response token IDs along with the model output. More details are available in the next section.</li> </ol> <p>Operationally, running the proxy alongside the algorithm works best: the algorithm registers the backend (e.g., the vLLM URL) via <code>LLMProxy.update_model_list</code>, publishes the proxy URL as a resource via <code>LightningStore.add_resources</code>, and runners simply use that URL during rollouts. This mirrors many production client\u2013server setups.</p>"},{"location":"deep-dive/serving-llm/#token-ids-and-why-they-matter","title":"Token IDs and why they matter","text":"<p>This section explains how Agent-lightning handles and uses token IDs \u2014 a subtle but important detail for training stability and accuracy.</p> <p>Most agents interact with LLMs via Chat Completion APIs, exchanging chat messages. There are two main approaches to collecting training data from such agents.</p> <p>Note</p> <p>Tokenization here refers to the process of converting Chat Messages into Token IDs. Detokenization is the reverse process of converting Token IDs back to Chat Messages. Normally, the tokenizer is published along with the pretrained model, which includes a vocabulary, special tokens, and a chat template to dealing with chat messages.</p> <p>1. Retokenizing chat messages. In this approach, you store chat messages as text and let training algorithms retokenize them later, as done in many SFT workflows (e.g., HuggingFace SFT). In practice, we\u2019ve found this method unstable and less accurate. The chart below compares training results. The retokenization approach is run twice. All settings are the same except for the retokenization approach.</p> <p>This instability has three causes. Firstly, chat template used in different frameworks could be slightly different. For example, one single LLaMA model can work with multiple chat templates (multiple in vLLM and one in HuggingFace). It's possible that the chat template used in detokenization is different from the one used in tokenization (this is actually an implementation bug).</p> <p>Secondly, a word might be generated as two tokens (e.g., <code>H + AVING</code>) but later retokenized as <code>HAV + ING</code>. The text looks identical, but the token IDs differ from what the model originally produced.</p> <p>Thirdly, a generated tool call text like <code>&lt;tool_call&gt;{ \"name\": ... }&lt;/tool_call&gt;</code> is parsed by tool call parser into an object that is required by chat completion API. Later, the object is rendered back to <code>&lt;tool_call&gt;{ \"name\": ... }&lt;/tool_call&gt;</code> and retokenized again, tool call parsing and re-rendering might cause changes in whitespace and formatting. In some situations, JSON errors may even be auto-corrected by the tool call parser \u2014 masking the model\u2019s true generation errors and preventing them from being trained away.</p> <p>2. Saving token IDs directly. The alternative is to save the token IDs generated by the model, as done in RL setups like Tinker. This requires a training pipeline that treats tokens as first-class entities, meaning agents must communicate with the inference engine at the token level.</p> <p>However, most agents \u2014 especially those built with frameworks like LangChain \u2014 rely on OpenAI-compatible APIs and can\u2019t tokenize or detokenize themselves. As mentioned earlier, implementing this layer manually is complex and error-prone. Some frameworks implement custom solutions (e.g., VERL Agent Loop, Tinker Renderer), while others leave it to users (e.g., SkyRL Search-R1).</p> <p>A better solution is to use an OpenAI-compatible API that returns token IDs directly. This lets agents continue using familiar APIs while capturing token IDs via tracing for training. The limitation, of course, is that the serving framework must actually support this capability.</p> <p>When Agent-lightning was first released, we implemented an instrumented vLLM server that monkey-patched vLLM\u2019s OpenAI server to return token IDs. Since then, the Agent-lightning and vLLM teams have collaborated to add this feature directly to vLLM core. Starting with vLLM v0.10.2, the OpenAI-compatible API includes a <code>return_token_ids</code> parameter, allowing token IDs to be requested alongside chat messages. SGLang has tracked similar feature requests, though its OpenAI-compatible layer doesn\u2019t yet support them.</p> <p>In short, when using vLLM v0.10.2 or newer, <code>LLMProxy</code> automatically adds <code>return_token_ids</code> to each request so the engine includes token IDs in its response. For older vLLM versions, you still need the instrumented version (via <code>agl vllm</code> CLI command).</p> <p>Finally, if you only save token IDs in spans, it will have its own limitations \u2014 if you train one model using spans from another model with a different tokenizer, incompatibilities can arise. In practice, though, spans in Agent-lightning always store both chat messages and token IDs (actually the full request and response objects), allowing you to fall back to retokenization when necessary.</p>"},{"location":"deep-dive/store/","title":"Understanding Store","text":"<p>The <code>LightningStore</code> is the central coordination point for Agent-lightning. It holds the task queue, rollouts, attempts, spans, and versioned resources, and exposes a small API both Runners and Algorithms use to communicate. This document explains what\u2019s in the store, how statuses transition, how spans are recorded, and the concurrency model (threads &amp; processes).</p>"},{"location":"deep-dive/store/#whats-in-the-store","title":"What\u2019s in the Store?","text":"<p>At a high level:</p> <ul> <li>Task Queue \u2013 <code>enqueue_rollout</code> adds work; workers poll with <code>dequeue_rollout</code>. When a rollout is dequeued, it automatically creates a new attempt associated with itself.</li> <li>Rollouts \u2013 A rollout is one unit of work. It has input, metadata, links to resources, and a lifecycle (<code>queuing \u2192 preparing \u2192 running \u2192 ...</code>). Valid RolloutStatus are <code>queuing</code>, <code>preparing</code>, <code>running</code>, <code>succeeded</code>, <code>failed</code>, <code>requeuing</code>, <code>cancelled</code>. For algorithms and runners, the rollout can be seen as a whole, without worrying about the internal attempts.</li> <li>Attempts \u2013 Each rollout can have multiple executions (retries). Attempts track <code>status</code>, <code>start_time</code>, <code>end_time</code>, <code>last_heartbeat_time</code> and link to spans. Valid AttemptStatus are <code>preparing</code>, <code>running</code>, <code>succeeded</code>, <code>failed</code>, <code>requeuing</code>, <code>cancelled</code>.</li> <li>Spans \u2013 Structured trace events produced by the Tracer during an attempt. Spans are ordered by a monotonic sequence id per <code>(rollout_id, attempt_id)</code>.</li> <li>Resources \u2013 Versioned, named bundles (e.g., prompt templates) referenced by rollouts.</li> </ul> <p>Rollout and Task share the same surface in practice: <code>Rollout.input</code> is the task input. The queue stores rollouts that are not yet running; Runners dequeue them and update the same rollout\u2019s status as work progresses.</p> <p>All <code>LightningStore</code> implementations must inherit from <code>LightningStore</code> and override the methods to implement the storage logic.</p> <p>Before we look at status transitions, it helps to keep in mind that rollouts are the \u201coutside view,\u201d while attempts are the \u201cinside view.\u201d Attempts are what actually run; rollouts summarize the latest attempt plus a small set of control actions like queueing and cancellation.</p>"},{"location":"deep-dive/store/#attempt-status-transitions","title":"Attempt Status Transitions","text":"<p>The status model is intentionally small and operationally clear.</p> <pre><code>stateDiagram-v2\ndirection LR\n\n[*] --&gt; preparing: &lt;b&gt;Runner calls&lt;/b&gt; dequeue_rollout()&lt;br&gt;or start_rollout()&lt;br&gt;or start_attempt()\npreparing --&gt; running: &lt;b&gt;Runner calls&lt;/b&gt;&lt;br&gt;add_[otel_]span()&lt;br&gt;for the first time\n\nstate c_runner &lt;&lt;choice&gt;&gt;\nstate c_watch  &lt;&lt;choice&gt;&gt;\n\npreparing --&gt; c_runner: &lt;b&gt;Runner calls&lt;/b&gt;&lt;br&gt;update_attempt(...)&lt;/b&gt;\nrunning --&gt; c_runner: &lt;b&gt;Runner calls&lt;/b&gt;&lt;br&gt;update_attempt(...)\nrunning --&gt; c_watch: &lt;b&gt;Watchdog checks&lt;/b&gt;\npreparing --&gt; c_watch: &lt;b&gt;Watchdog checks&lt;/b&gt;\n\nstate \"Client-set outcome\" as Client {\n  direction TB\n  succeeded\n  failed\n}\nstate \"Watchdog / policy\" as Watch {\n  direction TB\n  timeout\n  unresponsive\n}\n\nc_runner --&gt; succeeded: update_attempt(status=succeeded)\nc_runner --&gt; failed: update_attempt(status=failed)\n\nc_watch --&gt; timeout: now - start_time &gt; timeout_seconds\nc_watch --&gt; unresponsive: now - last_heartbeat &gt; unresponsive_seconds\n\nunresponsive --&gt; running: &lt;b&gt;Runner calls&lt;/b&gt;&lt;br&gt;add_[otel_]span()</code></pre> <p>Each attempt begins in preparing, created either when a rollout is dequeued or explicitly started. It transitions to running the first time a span is recorded. From there, a few clear rules govern how it can change:</p> <ul> <li>When the runner explicitly marks completion, the attempt becomes succeeded or failed (when the runner catches exception thrown out by the agent).</li> <li>When the watchdog detects that the total elapsed time since start exceeds the configured limit, it marks the attempt as timeout.</li> <li>If heartbeats stop arriving for too long, the watchdog marks it unresponsive.</li> <li>A new span from the runner can immediately revive an unresponsive attempt back to running.</li> </ul> <p>What's a Watchdog?</p> <p>The watchdog enforces timing and liveness rules defined by each rollout\u2019s <code>RolloutConfig</code>. It\u2019s not a separate thread or service, but a function periodically invoked (e.g., before store mutations) to keep attempts healthy and consistent.</p> <p>This simple model allows the system to distinguish between normal termination, abnormal stalling, and recoverable interruption without additional state flags.</p>"},{"location":"deep-dive/store/#rollout-transition-map","title":"Rollout Transition Map","text":"<p>Rollout status is an aggregated view of its latest attempt\u2019s status, with additional transitions for queueing and explicit cancellation.</p> <p>A rollout\u2019s retry behavior is controlled by <code>Rollout.config</code> (a <code>RolloutConfig</code>). The key fields are:</p> <ul> <li><code>timeout_seconds</code> \u2013 maximum wall-clock time for an attempt before it is marked <code>timeout</code>.</li> <li><code>unresponsive_seconds</code> \u2013 maximum silence between heartbeats before an attempt is marked <code>unresponsive</code>.</li> <li><code>max_attempts</code> \u2013 total number of attempts allowed for the rollout (including the first).</li> <li><code>retry_condition</code> \u2013 which attempt terminal statuses should trigger a retry (e.g., <code>[\"failed\", \"timeout\", \"unresponsive\"]</code>).</li> </ul> <p>How it plays out: The runner works on attempt <code>k</code>. If the attempt ends in a status that is listed in <code>retry_condition</code>, and <code>k &lt; max_attempts</code>, the rollout moves to requeuing and the store creates attempt <code>k+1</code>. Otherwise, the rollout becomes failed (or succeeded if the runner marked it so). <code>timeout_seconds</code> and <code>unresponsive_seconds</code> are enforced by the watchdog and feed into the same decision flow.</p> <p>A minimal example of how to use <code>RolloutConfig</code>:</p> <pre><code>from agentlightning import RolloutConfig\n\n# Retry on explicit failures or timeouts, up to 3 attempts in total.\ncfg = RolloutConfig(\n    timeout_seconds=600,\n    unresponsive_seconds=120,\n    max_attempts=3,\n    retry_condition=[\"failed\", \"timeout\"]\n)\n\n# When creating/enqueuing a rollout, attach this config.\n# The store will propagate attempt outcomes according to cfg.\nrollout = await store.enqueue_rollout(input, config=cfg)\n</code></pre> Latest attempt status Rollout transition Notes / guards N/A <code>queuing</code> Created by <code>enqueue_rollout()</code>. <code>preparing</code> <code>queuing/requeuing</code> \u2192 <code>preparing</code> Typically <code>dequeue_rollout()</code> or <code>start_rollout()</code>/<code>start_attempt()</code> creates a new attempt. <code>running</code> <code>preparing/queuing/requeuing</code> \u2192 <code>running</code> First <code>add_[otel_]span()</code> flips the attempt to <code>running</code>; rollout follows via <code>propagate_status</code>. <code>succeeded</code> <code>*</code> \u2192 <code>succeeded</code> Terminal. Rollout <code>end_time</code> set. <code>failed</code> / <code>timeout</code> / <code>unresponsive</code> <code>*</code> \u2192 <code>requeuing</code> Only if <code>status \u2208 retry_condition \u2227 sequence_id &lt; max_attempts</code>. <code>failed</code> / <code>timeout</code> / <code>unresponsive</code> <code>*</code> \u2192 <code>failed</code> Otherwise (no retries left or retries disabled). <code>*</code> <code>*</code> \u2192 <code>cancelled</code> Explicitly set by <code>update_rollout(status=cancelled)</code>. <p>Why aggregation?</p> <p>In code, we use <code>propagate_status()</code> which actively updates the rollout based on the latest attempt. Reading the table above is usually easier than reverse-engineering the propagation logic in the code: think of the rollout\u2019s transitions as callbacks on attempt state changes, plus queue/cancel paths.</p>"},{"location":"deep-dive/store/#spans","title":"Spans","text":"<p>Every traceable operation in a rollout is stored as a Span. Spans not only capture fine-grained instrumentation but also act as periodic heartbeats that demonstrate liveness. The first span marks activation; each subsequent one both extends the trace and refreshes the attempt\u2019s <code>last_heartbeat_time</code>. If no span arrives within the configured <code>unresponsive_seconds</code>, the watchdog downgrades the attempt to unresponsive until activity resumes.</p> <p>Spans are indexed by <code>(rollout_id, attempt_id, sequence_id)</code> where the sequence is monotonic. Tracing analysis tools like Adapter usually rely on \"time order\" to reconstruct the trace. However, in a distributed system, the recorded start time and end time of a span are not necessarily in the right order when they aggregated into a central store. Therefore, we enforce every span creation to retrieve a monotonically increasing <code>sequence_id</code> from the store before adding the span.</p> <p>Note</p> <p>In practice, one <code>sequence_id</code> can be used to create multiple spans. In that case, the orders between the multiple spans are determined by the order of <code>start_time</code> and <code>end_time</code> of the spans.</p>"},{"location":"deep-dive/store/#opentelemetry-conversion","title":"OpenTelemetry conversion","text":"<p>Runners often produce OpenTelemetry <code>ReadableSpan</code> objects directly. The store normalizes them into <code>Span</code> as follows:</p> <ol> <li>The runner first requests <code>get_next_span_sequence_id</code> via <code>sequence_id = await store.get_next_span_sequence_id(rollout_id, attempt_id)</code>. This guarantees ordering within the attempt regardless of clock skew.</li> <li><code>trace_id</code>, <code>span_id</code>, <code>parent_id</code>, <code>name</code>, <code>status</code>, timestamps, attributes, events, links, and resource are copied from the OTEL span. Timestamps are auto-normalized to seconds (nanoseconds are converted).</li> <li>OTEL <code>SpanContext</code> and parent context are preserved so downstream tools can correlate traces across systems.</li> <li>Any additional serializable fields present on the <code>ReadableSpan</code> are retained in the stored span (after safe JSON serialization), which keeps the representation forward-compatible.</li> </ol> <p>Programmatically this is encapsulated by <code>Span.from_opentelemetry(readable_span, rollout_id, attempt_id, sequence_id)</code>; <code>store.add_otel_span(...)</code> simply wraps the fetch-then-add flow. The end result is a store span that is stable to sort, merge, and query, while still preserving the richness of the original OTEL payload.</p> <p>Tip</p> <p><code>add_span</code> or <code>add_otel_span</code> both appends a span and acts as a heartbeat that can revive <code>unresponsive</code> \u2192 <code>running</code>.</p>"},{"location":"deep-dive/store/#store-implementations","title":"Store Implementations","text":"<p>Currently, the only out-of-the-box implementation is <code>InMemoryLightningStore</code>:</p> <ul> <li>Fast startup, zero external dependencies, and ideal for local development, CI, and unit tests.</li> <li>Fully asyncio-safe for writes; most reader operations can iterate without locks, except those that need to perform multiple queries.</li> <li>Includes a best-effort span eviction policy once memory crosses a configured watermark; querying evicted spans raises a clear error so callers can fall back.</li> </ul> <p>For production you will likely want persistence. We\u2019re actively building a SQLite-backed store that keeps the same API surface while adding durability, crash recovery, and better historical span queries. If you need something sooner, implement your own store by subclassing <code>LightningStore</code> and providing concrete storage for the small set of abstract methods (<code>enqueue_rollout</code>, <code>dequeue_rollout</code>, <code>update_attempt</code>, <code>add_span</code>, etc.). This document plus the tests in <code>tests/store/</code> illustrate the expected behavior.</p>"},{"location":"deep-dive/store/#thread-safety","title":"Thread Safety","text":"<p><code>LightningStoreThreaded</code> is a subclass of <code>LightningStore</code> that wraps another underlying store to make a store instance safe for multi-threaded callers. It wraps every state-mutating call in a mutex. Specifically:</p> <ul> <li>Methods like <code>start_rollout</code>, <code>enqueue_rollout</code>, <code>update_attempt</code>, <code>add_span</code>, etc. are guarded by a lock.</li> <li>Non-mutating, potentially blocking calls remain pass-through by design (e.g., <code>wait_for_rollouts</code>), as they don\u2019t modify shared state and should not hold the lock for long periods.</li> </ul>"},{"location":"deep-dive/store/#process-safety-and-client-server-store","title":"Process Safety and Client-server Store","text":"<p><code>LightningStoreServer</code> wraps another underlying store and runs a FastAPI app to expose the store API over HTTP. <code>LightningStoreClient</code> is a small <code>LightningStore</code> implementation that talks to the HTTP API.</p> <p>Warning</p> <p>The server HTTP API is not considered a stable API at this moment. Users are encouraged to use the <code>LightningStoreClient</code> to communicate with the server as a stable interface.</p> <p>The server tracks the creator PID. In the owner process it delegates directly to the in-memory store; in other processes it lazily constructs a <code>LightningStoreClient</code> to talk to the HTTP API. This prevents accidental cross-process mutation of the wrong memory image. When the server is pickled (e.g., via <code>multiprocessing</code>), only the minimal fields are serialized, but NOT the FastAPI/uvicorn objects. Subprocesses won\u2019t accidentally carry live server state. Forked subprocess should also use <code>LightningStoreClient</code> to communicate with the server in the main process.</p> <p>On the client side, the client retries network/5xx failures using a small backoff, and probes <code>/health</code> between attempts. Application exceptions inside the server are wrapped as HTTP 400 with a traceback\u2014these are not retried. The client also maintains a per-event-loop <code>aiohttp.ClientSession</code> map so that tracer callbacks (often on separate loops/threads) don\u2019t hang by reusing a session from another loop.</p> <p>Minimal lifecycle:</p> <pre><code>import agentlightning as agl\n\n# Server (owner process)\nin_memory_store = agl.InMemoryLightningStore()\nserver = agl.LightningStoreServer(store=in_memory_store, host=\"0.0.0.0\", port=4747)\nawait server.start()      # starts uvicorn in a daemon thread and waits for /health\n# or keep your own event loop and stop via await server.stop()\n# await server.run_forever()\n\n# Client (same or different process)\nclient = agl.LightningStoreClient(\"http://localhost:4747\")\n\nprint(await client.query_rollouts(status=[\"queuing\"]))\n\nawait client.close()\nawait server.stop()\n</code></pre> <p>Another approach is to use a dedicated command line to start a long running server process, possibly sharable across multiple processes. In the main process, you can always use <code>LightningStoreClient</code> to communicate with the server.</p> <pre><code>agl store --port 4747\n</code></pre> <p>Note</p> <p><code>LightningStoreClient.wait_for_rollouts</code> intentionally enforces a tiny timeout (\u2264 0.1s) to avoid blocking event loops. Poll with short timeouts or compose with <code>asyncio.wait_for</code> at a higher layer.</p>"},{"location":"how-to/train-first-agent/","title":"Train the First Agent with Agent-lightning","text":"<p>Welcome! This tutorial is your first step into making AI agents smarter using the Agent-lightning framework. We'll show you how to take a simple agent and automatically improve its performance through a process called Automatic Prompt Optimization (APO).</p> <p>The main goal of Agent-lightning is to provide a structured way to train your agents. Just like you train a machine learning model on data, you can train an agent on a task dataset. This could involve using Reinforcement Learning (RL) to teach it new behaviors or, as we'll do today, optimizing its prompts to make it more accurate and reliable.</p> <p>Tip</p> <p>You can open the sample code room_selector_apo.py and room_selector.py as you go through this tutorial.</p>"},{"location":"how-to/train-first-agent/#our-example-the-room-selector-agent","title":"Our Example: The Room Selector Agent","text":"<p>Today, we'll work with an agent whose job is to book a meeting room. It's a common but tricky task with multiple constraints.</p> <p>Here's how the agent works:</p> <ul> <li>Input: It receives a task with specific requirements, like \"<code>Find a room for 4 people at 10:00 AM with a whiteboard.</code>\"</li> <li>Action: The agent uses a Large Language Model (LLM) to understand the request. It can also use tools, which are pre-defined functions it can call, to get more information, such as checking room availability in an external database.</li> <li>Output: Its final decision is the ID of the best room it found, like \"<code>A103</code>\".</li> <li>Reward: After the agent makes its choice, a separate \"grader\" function scores its performance on a scale of 0 to 1. This score is called its reward. A perfect choice gets a 1.0, while a wrong one gets a 0.0.</li> </ul> <p>The agent's logic is sound, but its performance heavily depends on its initial prompt. A poorly worded prompt will confuse the LLM, leading to bad decisions. Our goal is to use Agent-lightning to find the best possible prompt automatically.</p> <p>A Closer Look at the Agent's Logic</p> <p>Modern LLMs can do more than just generate text; they can decide to call functions you provide. This is often called tool use or function calling. Our agent uses this capability to make informed decisions. If you're new to this concept, you can read more about it in OpenAI's documentation.</p> <p>Here is a sketch of the agent's logic, adhering closely to the OpenAI API:</p> <pre><code># Pseudo-code for the Room Selector agent\n\nimport openai\nimport json\n\ndef room_selector_agent(task, prompt):\n    client = openai.OpenAI()\n    messages = [{\"role\": \"user\", \"content\": prompt.format(**task)}]\n    tools = [ ... ] # Tool definition for the LLM\n\n    # 1. First LLM call to decide if a tool is needed.\n    response = client.chat.completions.create(\n        model=\"gpt-5-mini\",\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\",\n    )\n    response_message = response.choices[0].message\n    tool_calls = response_message.tool_calls\n\n    # 2. Check if the LLM wants to use a tool.\n    if tool_calls:\n        messages.append(response_message) # Append assistant's reply\n\n        # 3. Execute the tool and get the real-world data.\n        for tool_call in tool_calls:\n            function_name = tool_call.function.name\n            if function_name == \"get_rooms_and_availability\":\n                function_args = json.loads(tool_call.function.arguments)\n                # Query the local room database\n                function_response = get_rooms_and_availability(\n                    date=function_args.get(\"date\"),\n                    time_str=function_args.get(\"time\"),\n                    duration_min=function_args.get(\"duration_min\"),\n                )\n                messages.append({\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": function_name,\n                    \"content\": json.dumps(function_response),\n                })\n\n        # 4. Second LLM call with the tool's output to get a final choice.\n        second_response = client.chat.completions.create(\n            model=\"gpt-5-mini\",\n            messages=messages,\n        )\n        final_choice = second_response.choices[0].message.content\n    else:\n        final_choice = response_message.content\n\n    # 5. Grade the final choice to get a reward.\n    reward = grade_the_choice(final_choice, task[\"expected_choice\"])\n    return reward\n</code></pre> <p>In Agent-lightning, you wrap this logic in a Python function marked with the <code>@rollout</code> decorator, so that the agent can be managed and tuned by Agent-lightning's runner and trainer. The <code>prompt_template</code> that the APO algorithm tunes is passed in as an argument:</p> <pre><code>import agentlightning as agl\n\n@agl.rollout\ndef room_selector(task: RoomSelectionTask, prompt_template: agl.PromptTemplate) -&gt; float:\n    # ... agent logic using the prompt_template ...\n\n    # The final reward is determined by a grader function\n    reward = room_selection_grader(client, final_message, task[\"expected_choice\"])\n    return reward\n</code></pre>"},{"location":"how-to/train-first-agent/#core-concepts-tasks-rollouts-spans-and-prompt-templates","title":"Core Concepts: Tasks, Rollouts, Spans, and Prompt Templates","text":"<p>To understand how Agent-lightning works, you need to know these key terms.</p>"},{"location":"how-to/train-first-agent/#task","title":"Task","text":"<p>A task is a specific input or problem statement given to the agent. It defines what the agent needs to accomplish.</p> <p>Analogy: Task</p> <p>If the agent is a chef, a task is the recipe request: \"Bake a chocolate cake.\"</p>"},{"location":"how-to/train-first-agent/#rollout","title":"Rollout","text":"<p>A rollout is a single, complete execution of an agent attempting to solve a given task. It's the entire story from receiving the task to producing a final result and receiving a reward. A rollout captures a full trace of the agent's execution.</p> <p>Analogy: Rollout</p> <p>A rollout is one full attempt by the chef to bake the chocolate cake, from gathering ingredients to the final taste test.</p>"},{"location":"how-to/train-first-agent/#span","title":"Span","text":"<p>A span represents a single unit of work or an operation within a rollout. Spans are the building blocks of a trace. They have a start and end time and contain details about the specific operation, like an LLM call, a tool execution, or a reward calculation. For a more precise definition, see the OpenTelemetry documentation.</p> <p>Analogy: Span</p> <p>If the rollout is \"baking a cake,\" a span could be \"preheating the oven,\" \"mixing flour and sugar,\" or \"adding frosting.\" Each is a distinct step or unit of work.</p> <p>The picture below from ADK shows a typical rollout, where each rectangle in the waterfall visualizes a span. As can be seen in the visualization, spans can be sequential, parallel or nested among each other. In other frameworks, the terminilogy might be slightly different. Agent-lightning follows the terminalogies used by OpenTelemetry to avoid confusion.</p> <p></p>"},{"location":"how-to/train-first-agent/#prompt-template","title":"Prompt Template","text":"<p>A prompt template is a reusable instruction for the agent, often containing placeholders that can be filled in with specific details from a task. It is a key \"resource\" that the algorithm learns and improves over time.</p> <p>Analogy: Resource (Prompt Template)</p> <p>If the task is the recipe request, the prompt template is the master recipe card that the chef follows. The algorithm's job is to edit this recipe card to make the instructions clearer and the final dish better.</p>"},{"location":"how-to/train-first-agent/#the-training-loop-how-the-magic-happens","title":"The Training Loop: How the Magic Happens","text":"<p>Training in Agent-lightning revolves around a clear, managed loop, orchestrated by the Trainer. The diagram below illustrates this core interaction:</p> <p></p> <p>The Loop Explained:</p> <ul> <li>Algorithm to Agent (via Trainer): The Algorithm (the \"brain\") creates an improved Prompt Template and selects Tasks. The Trainer then sends both to the Trainer.</li> <li>Agent to Algorithm (via Trainer): For each task it receives, the Agent uses the provided prompt template to perform a Rollout, executing its logic and potentially using tools. During this rollout, the runner that runs the agent captures Spans that detail every step. The agent also calculates a Reward for its performance on the task. These spans and rewards are then sent back to the Algorithm via the Trainer.</li> <li>Algorithm Learning: The Algorithm then analyzes these spans and rewards to learn how to improve the agent's behavior, for example, by generating a better prompt. This improved prompt is then used in the next iteration of tasks.</li> </ul> <p>This cycle continues, allowing the agent to continuously learn and get better at solving tasks.</p> <p>Note</p> <p>In the next tutorial, we will see that the \"via Trainer\" here is not accurate. It's actually via the runner and store.</p>"},{"location":"how-to/train-first-agent/#the-algorithm","title":"The Algorithm","text":"<p>The algorithm is the smart part of the system that drives the improvement. In this tutorial, we use APO (Automatic Prompt Optimization). It works in a few steps:</p> <ol> <li>Evaluate: The algorithm first asks for rollouts to be run using the current prompt template to see how well it performs.</li> <li>Critique: It then looks at the detailed spans from those rollouts. Using a powerful LLM (<code>gpt-5-mini</code>), it generates a \"textual gradient\", which is a natural language critique of the prompt. For example: \"The prompt is ambiguous about how to handle tie-breakers for equally good rooms.\"</li> <li>Rewrite: Finally, it gives the critique and the original prompt to another LLM (<code>gpt-4.1-mini</code>) and asks it to apply the edits, generating a new, improved prompt template.</li> </ol> <p>This cycle repeats, with each round producing a slightly better prompt. To use it, you simply initialize the APO class with your desired hyperparameters.</p> <pre><code># In the main training script: run_apo.py\nfrom openai import AsyncOpenAI\n\nopenai = AsyncOpenAI()\nalgo = agl.APO(openai)\n</code></pre> <p>Tip</p> <p>Make sure you have <code>OPENAI_API_KEY</code> set in your environment variables.</p>"},{"location":"how-to/train-first-agent/#the-trainer","title":"The Trainer","text":"<p>The Trainer is the central component you'll interact with. It connects everything and manages the entire workflow by running the loop described above. You configure the Trainer, providing the algorithm, the number of parallel runners, and the initial prompt. A single call to <code>trainer.fit()</code> kicks off the entire process!</p> <pre><code># 1. Configure the Trainer with the algorithm and initial prompt\ntrainer = agl.Trainer(\n    algorithm=algo,\n    n_runners=8, # Run 8 agents in parallel to try out the prompts\n    initial_resources={\n        # The initial prompt template to be tuned\n        \"prompt_template\": prompt_template_baseline()\n    },\n    # This is used to convert the span data into a message format consumable by APO algorithm\n    adapter=agl.TraceToMessages(),\n)\n\n# 2. Load datasets: They can be list of task objects consumable by `room_selector`.\ndataset_train, dataset_val = ...\n\n# 3. Start the training process!\ntrainer.fit(\n    agent=room_selector,\n    train_dataset=dataset_train,\n    val_dataset=dataset_val\n)\n</code></pre> <p>Tip</p> <p><code>TraceToMessages</code> is a convenience adapter that converts spans into OpenAI chat messages. It requires <code>openai &gt;= 1.100.0</code> to be installed.</p>"},{"location":"how-to/train-first-agent/#training-results","title":"Training Results","text":"<p>The APO algorithm successfully improved the agent's performance. We ran the example with the following hyper-parameters:</p> <ul> <li><code>val_batch_size</code> = 10</li> <li><code>gradient_batch_size</code> = 4</li> <li><code>beam_width</code> = 2</li> <li><code>branch_factor</code> = 2</li> <li><code>beam_rounds</code> = 2</li> </ul> <p>The validation accuracy on the 29 samples of datasets steadily increase from 0.569 (baseline) to 0.721 (after round 2). The tuning takes around 10 minutes with 8 runners. We ran twice, and the results are shown in the chart below.</p> <p>This demonstrates how Agent-lightning can efficiently and automatically enhance your agent's capabilities with just a few lines of code.</p>"},{"location":"how-to/train-sql-agent/","title":"Train SQL Agent with Agent-lightning and VERL","text":"<p>This walkthrough builds upon the Agent-lightning v0.2 SQL Agent example and explains how the system components integrate: a LangGraph-based SQL agent wrapped as a <code>LitAgent</code>, the <code>VERL</code> reinforcement learning (RL) algorithm, and the <code>Trainer</code>, which coordinates both training and debugging.</p> <p>The command-line interface in <code>examples/spider/train_sql_agent.py</code> provides a complete runnable example. However, this document focuses on understanding the underlying architecture so you can effectively adapt the workflow to your own agents.</p>"},{"location":"how-to/train-sql-agent/#sql-agent-architecture","title":"SQL Agent Architecture","text":"<p>Agent-lightning integrates seamlessly with various orchestration frameworks, including Agent Framework, AutoGen, CrewAI, LangGraph, and the OpenAI Agents SDK. It can also interoperate with custom Python logic.</p> <p>In this example, LangGraph defines a cyclic workflow that mirrors an analyst\u2019s iterative SQL development process. The following graph (rendered directly from <code>sql_agent.py</code>) illustrates how the agent drafts, executes, critiques, and refines queries until a satisfactory result is achieved.</p> <pre><code>---\nconfig:\n  flowchart:\n    curve: linear\n---\ngraph LR;\n        __start__([&lt;p&gt;__start__&lt;/p&gt;]):::first\n        write_query(write_query)\n        execute_query(execute_query)\n        check_query(check_query)\n        rewrite_query(rewrite_query)\n        __end__([&lt;p&gt;__end__&lt;/p&gt;]):::last\n        __start__ --&gt; write_query;\n        check_query -.-&gt; __end__;\n        check_query -.-&gt; rewrite_query;\n        execute_query --&gt; check_query;\n        rewrite_query --&gt; execute_query;\n        write_query --&gt; execute_query;\n        classDef default fill:#f2f2f2,line-height:1.2\n        classDef first fill-opacity:0\n        classDef last fill:#cccccc</code></pre> <p>Note</p> <p>The workflow proceeds through the following stages:</p> <ol> <li>write_query \u2013 Generates an initial SQL query from the user\u2019s question and the database schema.</li> <li>execute_query \u2013 Executes the generated query against the target database.</li> <li>check_query \u2013 Evaluates the query and its results (or errors) using a specialized prompt (<code>CHECK_QUERY_PROMPT</code>) to detect issues.</li> <li>rewrite_query \u2013 If issues are identified, the agent rewrites the query using feedback from the previous step and re-enters the loop.</li> <li>END \u2013 The cycle terminates when the query is validated or the maximum iteration count (<code>max_turns</code>) is reached. Each turn consists of one full loop through the <code>write_query</code>, <code>execute_query</code>, <code>check_query</code>, and (if applicable) <code>rewrite_query</code> stages.</li> </ol> <p>In this tutorial, reinforcement learning (RL) is used to optimize the <code>write_query</code> and <code>rewrite_query</code> stages. While the <code>check_query</code> step shares the same underlying LLM weights, its trace data is not used for learning.</p> <p>To keep the design modular and maintainable, it is recommended to define the LangGraph-based SQL Agent in a separate file and expose it via a builder function such as:</p> <pre><code>def build_langgraph_sql_agent(\n    database_path: str,\n    openai_base_url: str,\n    model: str,\n    sampling_parameters: Dict[str, Any],\n    max_turns: int,\n    truncate_length: int\n):\n    builder = StateGraph(State)\n    builder.add_node(write_query)\n    ...\n\n    builder.add_edge(START, \"write_query\")\n    ...\n\n    return builder.compile().graph()\n</code></pre> <p>This approach isolates your LangGraph logic from Agent-lightning version changes, improving both readability and debuggability.</p>"},{"location":"how-to/train-sql-agent/#bridging-langgraph-and-agent-lightning","title":"Bridging LangGraph and Agent-lightning","text":"<p>Tip</p> <p>Keep <code>sql_agent.py</code> open on the side while reading this section. This will help you understand how the code snippets shown here work in practice.</p> <p>The <code>LitSQLAgent</code> class defined in <code>sql_agent.py</code> acts as the bridge. It subclasses <code>agl.LitAgent</code>, allowing the runner to provision shared resources (e.g., LLMs) for each rollout.</p> <p>Below is a simplified illustration of the key logic (note: this is conceptual pseudocode; the actual implementation includes dataset-specific details):</p> <pre><code>class LitSQLAgent(agl.LitAgent[Dict[str, Any]]):\n\n    def __init__(self, max_turns: int, truncate_length: int):\n        # Every turn here refers to a full cycle of write/exe/check/rewrite\n        self.max_turns = max_turns\n        self.truncate_length = truncate_length\n\n    def rollout(\n        self,\n        task: Dict[str, Any],\n        resources: agl.NamedResources,\n        rollout: agl.Rollout\n    ) -&gt; float | None:\n        llm: agl.LLM = resources[\"main_llm\"]\n        agent = build_langgraph_sql_agent(\n            database_path=\"sqlite:///\" + task[\"db_id\"],\n            max_turns=self.max_turns,\n            truncate_length=self.truncate_length,\n            openai_base_url=llm.get_base_url(rollout.rollout_id, rollout.attempt.attempt_id),\n            model=llm.model,\n            sampling_parameters=llm.sampling_parameters,\n        )\n        result = agent.invoke({\"question\": question}, {\n            \"callbacks\": [self.tracer.get_langchain_handler()],\n            \"recursion_limit\": 100,\n        })\n        reward = evaluate_query(result[\"query\"], ground_truth, db_path, raise_on_error=False)\n        return reward\n</code></pre> <p>The <code>LitSQLAgent</code> serves as a lightweight wrapper around the LangGraph agent, providing the correct interface for the <code>rollout</code> method. It constructs the LangGraph agent, invokes it, and returns the evaluation result as a reward signal.</p> <p>The <code>\"main_llm\"</code> resource key is a convention between the agent and VERL. It is used to inject an OpenAI-compatible endpoint from the VERL algorithm during rollout. Two approaches are supported to use this agentlightning.LLM resource:</p> <ol> <li>Direct access \u2013 Use <code>llm.endpoint</code> for a simple integration (identical to the v0.1 example).</li> <li>Context-aware access \u2013 Use <code>get_base_url</code> with <code>rollout.rollout_id</code> and <code>rollout.attempt.attempt_id</code>.    This approach enables per-caller trace attribution, improving trace collection per rollout or attempt when runner-side tracers are unavailable. For details, see Working with Traces.</li> </ol>"},{"location":"how-to/train-sql-agent/#reward-signal-and-evaluation","title":"Reward Signal and Evaluation","text":"<p>The <code>evaluate_query</code> function provides the reward mechanism for RL training. In agent training, obtaining a consistent and meaningful reward signal is often challenging. Fortunately, this is simplified when using the Spider dataset. The dataset includes ~8k samples containing natural-language questions, database schemas, and ground-truth SQL queries.</p> <p>Using the Spider evaluator, the agent's generated query is executed and compared to the ground-truth query on the target database. The two queries are considered equivalent if they produce identical execution results.</p> <p>Attention</p> <p>The ground-truth queries must never be exposed to the agent during training to prevent data leakage.</p> <p>In this setup, the reward is returned directly from the <code>rollout</code> method, enabling the runner to forward it back to the RL algorithm.</p> <p>Warning</p> <p>Avoid using <code>emit_reward</code> in conjunction with returning a reward value. Doing both will cause the algorithm to receive duplicate reward signals, leading to inconsistent training behavior.</p>"},{"location":"how-to/train-sql-agent/#configuring-verl-for-reinforcement-learning","title":"Configuring VERL for Reinforcement Learning","text":"<p>View <code>examples/spider/train_sql_agent.py</code> for a full reinforcement learning configuration, which is a plain Python dictionary. It mirrors (and actually is) the shell arguments used to launch training in the VERL framework but is easier to tweak programmatically:</p> <pre><code>verl_config: Dict[str, Any] = {\n    \"algorithm\": {\"adv_estimator\": \"grpo\", \"use_kl_in_reward\": False},\n    \"data\": {\n        # train_files and val_files are no longer needed here\n        # because data are read in agl.Trainer\n        ...,\n        # Controls how many tasks are pooled per step\n        # (multiplied by actor_rollout_ref.rollout.n)\n        \"train_batch_size\": 32,\n        # Prompt and responses larger than these lengths are truncated\n        \"max_prompt_length\": 4096,\n        \"max_response_length\": 2048,\n    },\n    \"actor_rollout_ref\": {\n        \"rollout\": {\n            # Only vLLM is supported currently\n            \"name\": \"vllm\",\n            # Equals to group size of GRPO\n            \"n\": 4,\n            # Used to enable tool call parser in vLLM\n            \"multi_turn\": {\"format\": \"hermes\"},\n            ...\n        },\n        \"actor\": {\"ppo_mini_batch_size\": 32, \"optim\": {\"lr\": 1e-6}, ...},\n        \"model\": {\n            # Config your preferred LLM here\n            \"path\": \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n            ...\n        },\n    },\n    \"trainer\": {\n        \"n_gpus_per_node\": 1,\n        # Validation once before training starts\n        \"val_before_train\": True,\n        # Validation every N training steps\n        \"test_freq\": 32,\n        # Save checkpoints every N training steps\n        \"save_freq\": 64,\n        # Go through the train dataset this many times\n        \"total_epochs\": 2\n    },\n}\n</code></pre> <p>This is equivalent to the following CLI invocation:</p> <pre><code>python3 -m verl.trainer.main_ppo \\\n    algorithm.adv_estimator=grpo \\\n    algorithm.use_kl_in_reward=False \\\n    data.train_batch_size=32 \\\n    data.max_prompt_length=4096 \\\n    data.max_response_length=2048 \\\n    actor_rollout_ref.rollout.name=vllm \\\n    actor_rollout_ref.rollout.n=4 \\\n    actor_rollout_ref.rollout.multi_turn.format=hermes \\\n    actor_rollout_ref.actor.ppo_mini_batch_size=32 \\\n    actor_rollout_ref.actor.optim.lr=1e-6 \\\n    actor_rollout_ref.model.path=Qwen/Qwen2.5-Coder-1.5B-Instruct \\\n    trainer.n_gpus_per_node=1 \\\n    trainer.val_before_train=True \\\n    trainer.test_freq=32 \\\n    trainer.save_freq=64 \\\n    trainer.total_epochs=2\n</code></pre> <p>Warning</p> <p>We used to provide a CLI called <code>python -m agentlightning.verl</code> to launch training in v0.1. This is no longer the recommended approach. Instead, use <code>agl.Trainer</code> to run VERL and agent runners together, or follow the debugging tutorial if you want an isolated experience similar to v0.1.</p>"},{"location":"how-to/train-sql-agent/#orchestrating-training-with-trainer","title":"Orchestrating Training with <code>Trainer</code>","text":"<p><code>Trainer</code> is the high-level orchestrator that integrates the agent, algorithm, dataset, and distributed runners. The key benefits of using the <code>Trainer</code> are:</p> <ol> <li>It allows you to launch everything with a single line of code: <code>trainer.fit(...)</code>.</li> <li>It exposes configuration options such as <code>n_runners</code> to control parallelism and <code>adapter</code> to define how algorithms interpret the trace data produced by the agent.</li> </ol> <p>An example usage is shown below:</p> <pre><code>import agentlightning as agl\n\nagent = LitSQLAgent()\nalgorithm = agl.VERL(verl_config)\ntrainer = agl.Trainer(\n    n_runners=10,\n    algorithm=algorithm,\n    adapter={\"agent_match\": active_agent},\n)\ntrain_data = pd.read_parquet(\"data/train_spider.parquet\").to_dict(\"records\")\nval_data = pd.read_parquet(\"data/test_dev_500.parquet\").to_dict(\"records\")\ntrainer.fit(agent, train_dataset=train_data, val_dataset=val_data)\n</code></pre> <p>First, <code>agl.VERL(verl_config)</code> launches the <code>VERL</code> algorithm and its OpenAI-compatible proxy. The <code>train_data</code> and <code>val_data</code> are passed into <code>VERL</code>, which enqueues tasks to a centralized task queue managed by the <code>LightningStore</code>, accessible to all runners.</p> <p>When <code>Trainer.fit</code> is called, it launches 10 concurrent runners (as specified by <code>n_runners=10</code>). Each runner pulls tasks from the centralized task queue, executes the agent\u2019s <code>rollout</code> method, collects traces, and returns rewards to VERL for training.</p> <p>The <code>Adapter</code>, as discussed earlier, is used at the algorithm side, and receives the traces emitted by the agent and runners. The <code>agent_match</code> parameter ensures <code>VERL</code> only ingests spans from the specific agent you want to optimize. In the example above, there are at least three agents\u2014<code>write_query</code>, <code>rewrite_query</code>, and <code>check_query</code>. By setting <code>agent_match</code> to a regex like <code>\"write\"</code>, both <code>write_query</code> and <code>rewrite_query</code> agents are optimized simultaneously. You can also set it to <code>\"write|check\"</code> or <code>None</code> to include all agents if desired.</p>"},{"location":"how-to/train-sql-agent/#dry-run-the-pipeline-with-trainerdev","title":"Dry-Run the Pipeline with <code>Trainer.dev</code>","text":"<p>Before committing hours of GPU time, you can dry-run the agent with <code>Trainer.dev()</code>. This method swaps in the lightweight <code>Baseline</code> algorithm, enqueues up to ten tasks, and prints every span emitted by the agent. Because it uses the same runner stack as full training, it\u2019s ideal for verifying database connections and LangGraph control flow.</p> <p>To begin, the agent needs a valid OpenAI-compatible endpoint since VERL is not active in this mode. You can use OpenAI\u2019s official API or your own local LLM endpoint. Wrap it as follows:</p> <pre><code>trainer = agl.Trainer(\n    n_workers=1,\n    initial_resources={\n        \"main_llm\": agl.LLM(\n            endpoint=os.environ[\"OPENAI_API_BASE\"],\n            model=\"gpt-4.1-nano\",\n            sampling_parameters={\"temperature\": 0.7},\n        )\n    },\n)\n</code></pre> <p>Then, call <code>trainer.dev(...)</code> with a small number of tasks:</p> <pre><code>dev_data = pd.read_parquet(\"data/test_dev_500.parquet\").to_dict(\"records\")[:10]\ntrainer.dev(agent, dev_dataset=dev_data)\n</code></pre> <p>Run this in a Python session or adapt your script to include a <code>--dev</code> flag. Once the spans appear healthy and the rewards are non-zero, switch back to <code>trainer.fit(...)</code> for full RL training.</p>"},{"location":"how-to/train-sql-agent/#running-the-sample-code","title":"Running the Sample Code","text":"<p>The following tutorial explains how to run the complete example in <code>examples/spider</code>.</p>"},{"location":"how-to/train-sql-agent/#dataset","title":"Dataset","text":"<p>The trainer expects three Parquet files inside <code>examples/spider/data</code>: <code>train_spider.parquet</code>, <code>test_dev_500.parquet</code>, and <code>test_dev.parquet</code>.</p> <p>Download the curated dataset bundle provided with the repository:</p> <pre><code>cd examples/spider\npip install gdown  # included in the 'experiment' optional dependency\ngdown --fuzzy https://drive.google.com/file/d/1oi9J1jZP9TyM35L85CL3qeGWl2jqlnL6/view\nunzip -q spider-data.zip -d data\nrm spider-data.zip\n</code></pre> <p>If you prefer to generate the files yourself, download Spider 1.0 and run:</p> <pre><code>python spider_eval/convert_dataset.py\n</code></pre> <p>Set <code>VERL_SPIDER_DATA_DIR</code> if you store the dataset outside the default <code>data</code> directory.</p>"},{"location":"how-to/train-sql-agent/#dependencies","title":"Dependencies","text":"<p>Create a clean virtual environment, activate it, and install Agent-lightning with the VERL extras required by this tutorial. Install LangChain-related dependencies as needed.</p> <p>For full training profiles, plan to use a GPU with at least 40 GB of memory.</p>"},{"location":"how-to/train-sql-agent/#launch-training","title":"Launch Training","text":"<p>From <code>examples/spider</code>, run one of the helper scripts depending on your model preference:</p> <pre><code>python train_sql_agent.py qwen   # Default Qwen-2.5-Coder-1.5B run\npython train_sql_agent.py llama  # LLaMA-3.2-1B with llama3_json tool parser\n</code></pre> <p>The script instantiates <code>LitSQLAgent</code> and launches <code>trainer.fit</code>. Provide <code>--active-agent my_agent_variant</code> if you only want to train one of the agents in the graph.</p> <p>For the LLaMA profile, export an <code>HF_TOKEN</code> before running so VERL can download the model weights.</p> <p>Troubleshooting</p> <p>If you have got some Ray worker errors on either <code>WANDB_API_KEY</code> not set, or <code>HF_TOKEN</code> not set, or data not found, please try to restart the Ray cluster with the helper script: scripts/restart_ray.sh, which essentially stops the ray cluster if any, and starts a new one:</p> <pre><code>env RAY_DEBUG=legacy HYDRA_FULL_ERROR=1 VLLM_USE_V1=1 ray start --head --dashboard-host=0.0.0.0\n</code></pre>"},{"location":"how-to/train-sql-agent/#debugging-the-agent-without-verl","title":"Debugging the Agent without VERL","text":"<p><code>sql_agent.py</code> also provides a <code>debug_sql_agent()</code> helper to run the LangGraph workflow directly against a local or hosted OpenAI-compatible endpoint before using VERL.</p> <p>Set the following environment variables, then execute the file:</p> <pre><code>export OPENAI_API_BASE=&lt;your_api_base&gt;\nexport OPENAI_API_KEY=&lt;your_api_key&gt;\ncd examples/spider\npython sql_agent.py\n</code></pre> <p>This allows you to verify that the workflow and prompts behave as expected before reinforcement learning is introduced.</p>"},{"location":"how-to/train-sql-agent/#evaluation","title":"Evaluation","text":"<p>The following results were obtained by running <code>python train_sql_agent.py qwen</code> on a single 80 GB GPU. Training completes in approximately 12 hours. The training curves below are smoothed by aggregating every 16 steps for better visualization.</p> <p>Additional evaluation results were collected with a legacy version \u2014 Agent-lightning v0.1.1, <code>verl==0.5.0</code>, and <code>vllm==0.10.0</code>. You can find them in this write-up: Training AI Agents to Write and Self-Correct SQL with Reinforcement Learning</p>"},{"location":"how-to/unsloth-sft/","title":"Fine-tune with Unsloth SFT","text":"<p>Prerequisites</p> <p>Please make sure you have read Write the First Algorithm. Although that recipe is based on a simple prompt tuning algorithm, it introduces the core concepts of Agent-lightning and you should be familiar with them before proceeding.</p> <p>This recipe builds on Write the First Algorithm. Instead of iterating on a prompt, we will fine-tune a large language model with Unsloth's SFT Trainer and keep the whole loop inside Agent-lightning. The new pieces you will meet are the LLM proxy, the trace-to-triplet adapter, a vLLM inference endpoint, and an agent implemented with the OpenAI Agents SDK. The full sample code is available in the <code>examples/unsloth</code> folder.</p> <p>Warning</p> <p>You need a GPU that can host the Unsloth base model and run vLLM. The sample defaults to <code>unsloth/Qwen3-4B-Instruct-2507</code>, which requires at least 16GB of GPU memory under 4-bit quantization.</p>"},{"location":"how-to/unsloth-sft/#the-data-and-serving-loop","title":"The Data and Serving Loop","text":"<p>To tune a large language model in Supervised Fine-Tuning (SFT), we commonly need a dataset with input/output samples. For example, the TRL SFT Trainer expects a dataset with samples like the following:</p> <pre><code>{\"messages\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"},\n              {\"role\": \"assistant\", \"content\": \"It is blue.\"}]}\n</code></pre> <p>With supervised fine-tuning, the LLM learns to generate the \"assistant\" response as close as possible to the completion in the dataset.</p> <p>Typically, the dataset used in SFT should be a curated set of samples. The samples can be either hand-written by humans, or generated by a more powerful model, which is known as data distillation. However, in this recipe, we use a different setup that relies on samples generated by the model itself. We use the reward emitted by the agent to select the top-performing samples.</p> <p>Overall, the flow of the algorithm is an iteration of the following steps:</p> <ol> <li>Serve the current checkpoint (with vLLM).</li> <li>Publish the vLLM endpoint through the LLM proxy and let runners roll out some tasks with the current model.</li> <li>Collect the traces from the rollouts and transform the highest-rewarded ones into a dataset that is acceptable for Unsloth SFT Trainer.</li> <li>Launch Unsloth to fine-tune on the dataset and save a new checkpoint.</li> </ol> <p>You will find the full source code of this iteration in <code>sft_one_iter</code> in sft_algorithm.py. We will elaborate on each part below.</p>"},{"location":"how-to/unsloth-sft/#serving-the-model-with-vllm-and-proxy","title":"Serving the Model with vLLM and Proxy","text":"<p>Most modern agents do not use the model directly; instead, they use an API like the OpenAI chat completions API to interact with the model. Therefore, we need a vLLM-based inference server launched before rollouts. The serving code looks like the following. See the <code>vllm_server</code> function in sft_algorithm.py if you want to see a more robust version.</p> <pre><code>from openai import OpenAI\n\nvllm_process = subprocess.Popen([\n    \"vllm\", \"serve\", model_path, \"--port\", str(port),\n    \"--enable-auto-tool-choice\", \"--tool-call-parser\", \"hermes\"\n])\n\n# Wait for the server to be ready\nurl = f\"http://localhost:{port}/health\"\nstart = time.time()\nclient = httpx.Client()\n\nwhile True:\n    if client.get(url).status_code == 200:\n        break\n\nserver_address = f\"http://localhost:{port}/v1\"\n\n# Try using the vLLM server\nopenai = OpenAI(base_url=server_address)\n...\n</code></pre> <p>In this recipe, we do not expose the server address directly to the agent runners, because we want to install a \"middleware\" to collect the prompts and responses of all the requests. In general, it's up to you to decide whether to hide the vLLM server behind a proxy or not.</p> <p>The \"middleware\" here is <code>LLMProxy</code>, which is an independent LiteLLM server that forwards the requests to the vLLM server. It also exposes an OpenAI-compatible API that the runners can target without caring about where the model lives. The benefits of using the proxy are:</p> <ol> <li>Traces: The proxy automatically logs the prompts and responses of all the requests into the store.</li> <li>Token IDs: The proxy augments the requests so that the vLLM server can return the prompt and response token IDs (see more details in Serving LLM).</li> </ol> <p>The <code>LLMProxy</code> accepts a list of model configurations, in the same syntax as LiteLLM's <code>model_list</code>. Include a <code>hosted_vllm/</code> prefix to the models to activate LiteLLM's vLLM integration.</p> <pre><code>import agentlightning as agl\n\nllm_proxy = agl.LLMProxy(port=port, store=store)\nmodel_list = [\n    {\n        \"model_name\": \"Qwen3-4B-Instruct\",\n        \"litellm_params\": {\"model\": f\"hosted_vllm/{model_path}\", \"api_base\": server_address},\n    }\n]\nllm_proxy.update_model_list(model_list)\n# If the proxy is not running, it will start automatically.\nllm_proxy.restart()\n# Add the proxy as a resource to the store so that the runners can access it via URL.\nresource_update = await store.add_resources({\"main_llm\": llm_proxy.as_resource()})\n</code></pre>"},{"location":"how-to/unsloth-sft/#spawn-rollout-and-collect-spans","title":"Spawn Rollout and Collect Spans","text":"<p>Once the proxy is registered as a resource, the algorithm schedules work for the rollout runners. Each problem from a training dataset becomes a rollout with the proxy baked into its resources:</p> <pre><code>rollouts: list[Rollout] = []\nfor sample in train_dataset:\n    rollouts.append(\n        await store.enqueue_rollout(\n            input=sample,\n            mode=\"train\",\n            resources_id=resources_update.resources_id,\n        )\n    )\n</code></pre> <p><code>resources_id</code> ties every rollout to the <code>main_llm</code> proxy resource we just uploaded. The runners on the other side poll the store (<code>LitAgentRunner.iter()</code>) and execute the agent for each rollout. On the algorithm side we wait for completions with a non-blocking polling loop:</p> <pre><code>completed_rollouts: list[Rollout] = []\nwhile True:\n    completed_rollouts = await store.wait_for_rollouts(\n        rollout_ids=[r.rollout_id for r in rollouts],\n        timeout=0.0,\n    )\n    if len(completed_rollouts) == len(rollouts):\n        break\n    await asyncio.sleep(5.0)\n</code></pre> <p>Note</p> <p>The <code>timeout=0.0</code> is needed here because this example uses a <code>LightningStoreClient</code>, and <code>wait_for_rollouts</code> establishes an HTTP connection to that store. Currently, only non-blocking wait requests are supported, which avoids holding the store connection open.</p> <p>Once the rollouts complete, we terminate the vLLM server to free up GPU memory.</p> <pre><code>vllm_process.terminate()\nvllm_process.join(timeout=10.0)\n</code></pre>"},{"location":"how-to/unsloth-sft/#adapt-the-spans-to-huggingface-dataset","title":"Adapt the Spans to HuggingFace Dataset","text":"<p><code>LlmProxyTraceToTriplet</code> converts the proxy\u2019s spans (which might be dozens to hundreds per rollout) into <code>Triplet</code> objects that contain prompt/response token IDs plus an optional reward. The adapter may return multiple triplets per rollout (one per chat-completion call). To bias training toward successful reasoning chains the algorithm walks the triplets in reverse order, keeps the most recent reward, and turns each prompt/response pair into Hugging Face dataset rows:</p> <pre><code>all_triplets = []\ndata_adapter = agl.LlmProxyTraceToTriplet()\n\nfor rollout in completed_rollouts:\n    spans = await store.query_spans(rollout.rollout_id, \"latest\")\n    triplets = data_adapter.adapt(spans)\n\n    recent_reward = None\n    for triplet in reversed(triplets):\n        if triplet.reward is not None:\n            recent_reward = triplet.reward\n        if recent_reward is None:\n            continue\n\n        input_ids = triplet.prompt[\"token_ids\"] + triplet.response[\"token_ids\"]\n        # We don't train on prompt tokens, so they are masked out by setting to -100.\n        labels = [-100] * len(triplet.prompt[\"token_ids\"]) + triplet.response[\"token_ids\"]\n        # This matches the dataset format required by the Unsloth SFT trainer.\n        all_triplets.append(\n            {\n                \"input_ids\": input_ids,\n                \"attention_mask\": [1] * len(input_ids),\n                \"labels\": labels,\n                \"reward\": recent_reward,\n            }\n        )\n</code></pre> <p>Note</p> <p>You might notice that the dataset format used here differs from the format described in the SFT Trainer documentation. According to the documentation, dataset samples should be provided as plain text strings or message objects.</p> <p>As a matter of fact, this example leverages some undocumented behavior in the SFT Trainer implementation. When the dataset already includes a <code>\"input_ids\"</code> column, the Trainer automatically marks it as <code>is_processed</code> and skips the internal tokenization step.</p> <p>Since we already have spans with token IDs generated by the <code>LLMProxy</code>, providing them directly avoids unnecessary re-tokenization and related complications. This approach will both save processing time and increase consistency between training and inference.</p> <p>After aggregating every rollout we shuffle, sort by reward, and keep the top fraction (e.g., 50%) before shuffling again. The resulting list feeds directly into <code>datasets.Dataset.from_list</code>, which is the format Unsloth\u2019s SFT trainer expects.</p> <pre><code>from datasets import Dataset as HuggingFaceDataset\n\nrandom.shuffle(all_triplets)\nall_triplets.sort(key=lambda x: x[\"reward\"], reverse=True)\nsliced_triplets = all_triplets[: max(1, int(len(all_triplets) * triplet_fraction))]\n# Shuffle the sliced triplets again\nrandom.shuffle(sliced_triplets)\n\nsft_dataset = HuggingFaceDataset.from_list(sliced_triplets)\n</code></pre>"},{"location":"how-to/unsloth-sft/#launch-unsloth-training","title":"Launch Unsloth Training","text":"<p>The heavy lifting happens in <code>trl.SFTTrainer</code> (see unsloth_helper.py on how it's used). We launch it in a fresh process created with <code>multiprocessing.get_context(\"spawn\")</code> so CUDA memory is reliably reclaimed when training ends. Launching it in the same process will also work for the first iteration, but we found that the memory won't be freed properly for subsequent vLLM serving.</p> <pre><code>context = multiprocessing.get_context(\"spawn\")\nunsloth_process = context.Process(\n    target=unsloth_training,\n    args=(model_path, sft_dataset, next_model_path),\n    daemon=True,\n)\nunsloth_process.start()\nunsloth_process.join(timeout=600.0)\n</code></pre> <p>Inside the <code>unsloth_training</code> subprocess, Unsloth loads the previous checkpoint in 4-bit, applies LoRA adapters, and forwards the Hugging Face dataset to <code>trl.SFTTrainer</code> with the configuration defined in <code>SFTConfig</code> (batch size, accumulation steps, learning rate, etc.). The merged 16-bit weights are saved under <code>models/version_&lt;iteration + 1&gt;</code> so the next iteration can immediately serve them with vLLM.</p> <pre><code>from unsloth import FastLanguageModel\n# TRL is patched by unsloth.\nfrom trl import SFTConfig, SFTTrainer\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_path,\n    load_in_4bit=True,  # 4 bit quantization to reduce memory\n)\n\n# Config the model to use LoRA\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,\n    ...\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sft_dataset,\n    ...\n)\n\n# This is the heaviest step.\ntrainer_stats = trainer.train()\n\n# Save in 16-bit for vLLM inference later\nmodel.save_pretrained_merged(next_model_path, tokenizer, save_method=\"merged_16bit\")\n</code></pre>"},{"location":"how-to/unsloth-sft/#math-agent-openai-agents-sdk-with-mcp","title":"Math Agent: OpenAI Agents SDK with MCP","text":"<p>We build an agent with the OpenAI Agents SDK to wire a calculator MCP tool and an OpenAI-compatible chat completion model together. The agent aims to solve a math problem and returns a reward indicating whether the answer is correct or not. The runner injects the <code>LLM</code> resource supplied by the algorithm side:</p> <pre><code>import os\nfrom typing import TypedDict\n\nimport agentlightning as agl\nfrom agents import Agent, ModelSettings, OpenAIChatCompletionsModel, Runner as OpenAIRunner\nfrom agents.mcp import MCPServerStdio\nfrom openai import AsyncOpenAI\n\nclass GsmProblem(TypedDict):\n    input: str\n    target: float\n\ndef compute_reward(result: str, target: float) -&gt; float:\n    ...\n\n@agl.rollout\nasync def math_agent(task: GsmProblem, llm: agl.LLM) -&gt; float:\n    async with MCPServerStdio(\n        name=\"Calculator via uvx\",\n        params={\"command\": \"uvx\", \"args\": [\"mcp-server-calculator\"]},\n    ) as server:\n        agent = Agent(\n            name=\"Assistant\",\n            instructions=(\n                \"Use the calculator tool for every question. \"\n                \"Return only the numeric answer wrapped like ### &lt;answer&gt; ###.\"\n            ),\n            mcp_servers=[server],\n            model=OpenAIChatCompletionsModel(\n                model=llm.model,\n                openai_client=AsyncOpenAI(\n                    base_url=llm.endpoint,\n                    api_key=llm.api_key or \"dummy\",\n                ),\n            ),\n            model_settings=ModelSettings(\n                temperature=llm.sampling_parameters.get(\"temperature\", 0.0),\n            ),\n        )\n        result = await OpenAIRunner.run(agent, task[\"input\"])\n    return compute_reward(result.final_output, task[\"target\"])\n</code></pre> <p>Tip</p> <p>You can test the agent with a dry run:</p> <pre><code>import asyncio\n\nllm = agl.LLM(\n    endpoint=os.environ[\"OPENAI_BASE_URL\"],\n    api_key=os.environ[\"OPENAI_API_KEY\"],\n    model=\"gpt-4.1-mini\",\n)\nasyncio.run(math_agent({\"input\": \"What is 1 + 1?\", \"target\": 2.0}, llm))\n</code></pre>"},{"location":"how-to/unsloth-sft/#run-this-recipe","title":"Run this Recipe","text":"<p>The full runnable script for this recipe resides in <code>examples/unsloth</code> folder.</p> <p>Before running this example, install <code>unsloth</code>, <code>vllm</code>, and the other libraries used in the examples (the project uses CUDA tooling, TRL, rich, datasets, etc.). We tested with <code>unsloth==2025.10.1</code>. <code>unsloth==2025.10.2</code> and <code>2025.10.3</code> are not working because of an issue we have been investigating with the unsloth team.</p> <p>It's recommended to download the base model before running the example, such that the first iteration and subsequent iterations can both load from local checkpoints.</p> <pre><code>hf download unsloth/Qwen3-4B-Instruct-2507 --local-dir models/version_0\n</code></pre> <p>The repository already contains <code>examples/unsloth/data_gsmhard.jsonl</code> (which is a very small subset of the GSM-hard math dataset for demonstration purposes).</p>"},{"location":"how-to/unsloth-sft/#run-manually","title":"Run Manually","text":"<p>Similar to the Write the First Algorithm recipe, you can open three terminals and start each component in parallel.</p> <pre><code>agl store --port 4747\npython examples/unsloth/sft_rollout_runners.py\npython examples/unsloth/sft_algorithm.py\n</code></pre> <p>In this case, <code>sft_rollout_runners.py</code> is a simple spawner implemented in Python that spawns 4 runners in parallel. The runners all connect to the same store server executing in another terminal.</p> <pre><code>import agentlightning as agl\n\ndef run_rollout(store: agl.LightningStore, worker_id: int) -&gt; None:\n    # Since the server side has already used LiteLLM proxy to collect traces,\n    # a simple OtelTracer to collect the rewards is enough.\n    tracer = agl.OtelTracer()\n\n    runner = agl.LitAgentRunner(tracer=tracer)\n\n    with runner.run_context(agent=math_agent, store=store, worker_id=worker_id):\n        asyncio.run(runner.iter())\n\n\ndef spawn_runners(store: agl.LightningStore, n_runners: int) -&gt; None:\n    runners = [\n        multiprocessing.Process(target=run_rollout, args=(store, worker_id))\n        for worker_id in range(n_runners)\n    ]\n    for runner in runners:\n        runner.start()\n\n    for runner in runners:\n        runner.join()\n\n\nstore = agl.LightningStoreClient(\"http://localhost:4747\")\nspawn_runners(store=store, n_runners=4)\n</code></pre> <p>Tip</p> <p>Try to swap <code>OtelTracer</code> in the runners with other tracers like <code>AgentOpsTracer</code>. Try to use a different adapter at the algorithm side such as <code>TracerTraceToTriplet</code> to see what happens.</p>"},{"location":"how-to/unsloth-sft/#run-everything-with-trainer","title":"Run Everything with Trainer","text":"<p>We also show how to wrap everything into a single script using <code>Trainer</code>. <code>sft_allinone.py</code> wires the same components together, replacing the manual management of runners above.</p> <pre><code>class UnslothSupervisedFinetuning(agl.Algorithm):\n\n    async def run(\n        self,\n        train_dataset: Optional[Dataset[GsmProblem]] = None,\n        val_dataset: Optional[Dataset[GsmProblem]] = None,\n    ):\n        # Use the store, llm_proxy, and adapter from the trainer\n        store = self.get_store()\n        llm_proxy = self.get_llm_proxy()\n        data_adapter = self.get_adapter()\n\n        for iteration in range(self.max_iterations):\n            ...  # Same logic as sft_algorithm.py\n\nalgo = UnslothSupervisedFinetuning(\n    max_iterations=2,\n    vllm_port=12316,\n    train_triplet_fraction=0.5,\n    initial_model_path=\"models/version_0\",\n)\n\n# The LLM proxy can be created before Trainer\ntrainer = Trainer(\n    n_runners=4,\n    algorithm=algo,\n    llm_proxy=LLMProxy(port=12358),\n)\n\ntrainer.fit(math_agent, load_math_dataset())\n</code></pre> <p>You might wonder where the initialization of <code>Adapter</code> happens in this code. It turns out that <code>TracerTraceToTriplet</code> is the default adapter in <code>Trainer</code>, so we don't need to create one manually.</p> <p>Now you can run the example with:</p> <pre><code>python examples/unsloth/sft_allinone.py\n</code></pre> <p>It starts an <code>InMemoryLighningStore</code> for you, launches four worker processes, iterates the SFT loop, and prints the final checkpoint path when done. Adjust <code>max_iterations</code>, <code>train_triplet_fraction</code>, <code>n_runners</code>, or the proxy port to match your hardware or training goals. If you already run an external store or proxy you can also pass those objects into <code>Trainer</code> instead of relying on the Trainer-managed defaults.</p> <p>Info</p> <p>As a future plan, we might graduate this example into a more powerful SFT algorithm bundled into Algorithm Zoo. Currently, this <code>UnslothSupervisedFinetuning</code> is still for demo purposes.</p>"},{"location":"how-to/write-first-algorithm/","title":"Write the First Algorithm with Agent-lightning","text":"<p>In the first tutorial, \"Train the First Agent,\" we introduced the Trainer and showed how to use a pre-built algorithm like Automatic Prompt Optimization (APO) to improve an agent's performance. The Trainer handled all the complex interactions, letting us focus on the agent's logic.</p> <p>Now, we'll go a step deeper. What if you have a unique training idea that doesn't fit a standard algorithm? This tutorial will show you how to write your own custom algorithm from scratch. We'll build a simple algorithm that systematically tests a list of prompt templates and identifies the one with the highest reward.</p> <p>By the end, you'll understand the core mechanics of how the Algorithm, Runner, and a new component, the Store, work together to create the powerful training loop at the heart of Agent-lightning.</p> <p>Tip</p> <p>This tutorial helps you build a basic understanding of how to interact with Agent-lightning's core components. It's recommended that all users customizing algorithms should read this tutorial, even for those who are not planning to do prompt optimization.</p>"},{"location":"how-to/write-first-algorithm/#core-concepts-for-training","title":"Core Concepts for Training","text":"<p>Before diving into the LightningStore, let's define two key concepts that are central to any training process in Agent-lightning: Resources and the Tracer.</p>"},{"location":"how-to/write-first-algorithm/#resources-the-tunable-assets","title":"Resources: The Tunable Assets","text":"<p>Resources are the assets your algorithm is trying to improve. Think of them as the \"recipe\" an agent uses to perform its task. This recipe can be:</p> <ul> <li>A prompt template that guides an LLM.</li> <li>The weights of a machine learning model.</li> <li>Any other configuration or data your agent needs.</li> </ul> <p>The algorithm's job is to run experiments and iteratively update these resources to find the best-performing version.</p>"},{"location":"how-to/write-first-algorithm/#tracer-the-data-collector","title":"Tracer: The Data Collector","text":"<p>How does the algorithm know if a change was an improvement? It needs data. This is where the Tracer comes in.</p> <p>The Tracer automatically instruments (aka modifies / patches) the agent's code. This means it watches for important events, like an LLM call, a tool being used, or reward signals, and records a detailed log of what happened. Each of these logs is called a Span (which has already been introduced in the last tutorial).</p> <p>A collection of spans from a single task execution gives the algorithm a complete, step-by-step trace of the agent's behavior, which is essential for learning and making improvements. Our default tracer is built on the AgentOps SDK to support instrumenting code written in various Agent/non-agent frameworks.</p>"},{"location":"how-to/write-first-algorithm/#the-central-hub-the-lightningstore","title":"The Central Hub: The LightningStore","text":"<p>Now, where do all these resources, tasks, and spans live? They are all managed by the LightningStore.</p> <p>The LightningStore acts as the central database and message queue for the entire system. It's the single source of truth that decouples the Algorithm from the Runners.</p> <p>Note</p> <p>In the last tutorial we simplified the training loop, saying the Algorithm and Agent communicate \"via the Trainer.\" That's true at a high level, but the component that makes it all possible is actually the LightningStore.</p> <ul> <li>The Algorithm connects to the Store to <code>enqueue_rollout</code> (tasks) and <code>update_resources</code> (like new prompt templates). It also queries the Store to retrieve the resulting spans and rewards from completed rollouts.</li> <li>The Runners connect to the Store to <code>dequeue_rollout</code> (polling for available tasks). After executing a task, they use the <code>Tracer</code> to write the resulting spans and status updates back to the Store.</li> </ul> <p>This architecture is key to Agent-lightning's scalability. Since the Algorithm and Runners only talk to the Store, they can run in different processes or even on different machines.</p> <p></p> <p>A Mental Model of What the Store Contains</p> <p>The LightningStore isn't just a simple database; it's an organized system for managing the entire training lifecycle. Here's what it keeps track of:</p> <ul> <li>Task Queue: A queue of pending Rollouts waiting for a Runner to pick them up, interactable via <code>enqueue_rollout</code> and <code>dequeue_rollout</code>.</li> <li>Rollouts: The record of a single task. A rollout contains metadata about the task and tracks all Attempts to complete it, interactable via <code>query_rollouts</code> and <code>wait_for_rollouts</code>.</li> <li>Attempts: A single execution of a rollout. If an attempt fails (e.g., due to a network error), the Store can automatically schedules a retry if it's configured. Each attempt is linked to its parent rollout and contains the status and timing information. The rollout status is synced with its children's status. For beginners, you can assume each rollout has only one attempt unless you have explicitly configure the retry.</li> <li>Spans: The detailed, structured logs generated by the <code>Tracer</code> during an attempt. Each span is linked to its parent attempt and rollout.</li> <li>Resources: A versioned collection of the assets (like prompt templates) that the algorithm creates. Each rollout is linked to the specific version of the resources it should use.</li> </ul>"},{"location":"how-to/write-first-algorithm/#building-a-custom-algorithm","title":"Building a Custom Algorithm","text":"<p>Let's build an algorithm that finds the best system prompt from a predefined list. The logic is straightforward:</p> <ol> <li>Start with a list of candidate prompt templates.</li> <li>For each template, create a \"resource\" bundle in the Store.</li> <li>Enqueue a rollout (a task), telling the Runner to use this specific resource.</li> <li>Wait for a Runner to pick up the task and complete it.</li> <li>Query the Store to get the final reward from the rollout's spans.</li> <li>After testing all templates, compare the rewards and declare the best one.</li> </ol> <p>We can implement this as a simple Python function that interacts directly with the LightningStore.</p> <pre><code>async def find_best_prompt(store, prompts_to_test, task_input):\n    \"\"\"A simple algorithm to find the best prompt from a list.\"\"\"\n    results = []\n\n    # Iterate through each prompt to test it\n    for prompt in prompts_to_test:\n        print(f\"[Algo] Updating prompt template to: '{prompt}'\")\n\n        # 1. Update the resources in the store with the new prompt\n        resources_update = await store.add_resources(\n            resources={\"prompt_template\": prompt}\n        )\n\n        # 2. Enqueue a rollout task for a runner to execute\n        print(\"[Algo] Queuing task for clients...\")\n        rollout = await store.enqueue_rollout(\n            input=task_input,\n            resources_id=resources_update.resources_id,\n        )\n        print(f\"[Algo] Task '{rollout.rollout_id}' is now available for clients.\")\n\n        # 3. Wait for the rollout to be completed by a runner\n        await store.wait_for_rollouts([rollout.rollout_id])\n\n        # 4. Query the completed rollout and its spans\n        completed_rollout = (await store.query_rollouts([rollout.rollout_id]))[0]\n        print(f\"[Algo] Received Result: {completed_rollout.model_dump_json(indent=None)}\")\n\n        spans = await store.query_spans(rollout.rollout_id)\n        # We expect at least two spans: one for the LLM call and one for the final reward\n        print(f\"[Algo] Queried Spans:\\n  - \" + \"\\n  - \".join(str(span) for span in spans))\n        # find_final_reward is a helper function to extract the reward span\n        final_reward = find_final_reward(spans)\n        print(f\"[Algo] Final reward: {final_reward}\\n\")\n\n        results.append((prompt, final_reward))\n\n    # 5. Find and print the best prompt based on the collected rewards\n    print(f\"[Algo] All prompts and their rewards: {results}\")\n    best_prompt, best_reward = max(results, key=lambda item: item[1])\n    print(f\"[Algo] Best prompt found: '{best_prompt}' with reward {best_reward}\")\n</code></pre> <p>Asynchronous Operations</p> <p>You'll notice the <code>async</code> and <code>await</code> keywords. Agent-lightning is built on asyncio to handle concurrent operations efficiently. All interactions with the store are asynchronous network calls, so they must be awaited.</p>"},{"location":"how-to/write-first-algorithm/#the-agent-and-runner","title":"The Agent and Runner","text":"<p>Our algorithm needs an agent to execute the tasks and a runner to manage the process.</p> <p>The runner is a long-lived worker process. Its job is simple:</p> <ol> <li>Connect to the LightningStore via a LightningStoreClient.</li> <li>Enter a loop, constantly asking the LightningStore for new tasks (<code>dequeue_rollout</code>).</li> <li>When it gets a task, it runs the <code>simple_agent</code> function.</li> <li>Crucially, the runner wraps the agent execution with a Tracer. The tracer automatically captures all the important events (like the LLM call and the final reward) as spans and sends them back to the LightningStore.</li> </ol> <pre><code># Connecting to Store\nstore = agl.LightningStoreClient(\"http://localhost:4747\")  # or some other address\nrunner = LitAgentRunner[str](tracer=AgentOpsTracer())\nwith runner.run_context(agent=simple_agent, store=store):  # &lt;-- where the wrapping and instrumentation happens\n    await runner.iter()  # polling for new tasks forever\n</code></pre> <p>For this example, the agent's job is to take the prompt from the resources, use it to ask an LLM a question, and return a score.</p> <pre><code>def simple_agent(task: str, prompt_template: PromptTemplate) -&gt; float:\n    \"\"\"An agent that answers a question and gets judged by an LLM.\"\"\"\n    client = OpenAI()\n\n    # Generate a response using the provided prompt template\n    prompt = prompt_template.format(any_question=task)\n    response = client.chat.completions.create(\n        model=\"gpt-4.1-nano\", messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    llm_output = response.choices[0].message.content\n    print(f\"[Rollout] LLM returned: {llm_output}\")\n\n    # This llm_output and the final score are automatically logged as spans by the Tracer\n    score = random.uniform(0, 1)  # Replace with actual scoring logic if needed\n    return score\n</code></pre>"},{"location":"how-to/write-first-algorithm/#running-the-example","title":"Running the Example","text":"<p>To see everything in action, you'll need three separate terminal windows.</p> <p>Tip</p> <p>If you want to follow along, you can find the complete code for this example in the apo_custom_algorithm.py file.</p> <p>1. Start the Store: In the first terminal, start the LightningStore server. This component will wait for connections from the algorithm and the runner. The store will be listening on port <code>4747</code> \u26a1 by default.</p> <pre><code>agl store\n</code></pre> <p>2. Start the Runner: In the second terminal, start the runner process. It will connect to the store and wait for tasks.</p> <p>The code to start the runner looks like the following:</p> <pre><code>export OPENAI_API_KEY=sk-... # Your OpenAI API key\npython apo_custom_algorithm.py runner\n</code></pre> <p>You will see output indicating the runner has started and is waiting for rollouts.</p> <pre><code>2025-10-14 22:23:41,339 [INFO] ... [Worker 0] Setting up tracer...\n2025-10-14 22:23:41,343 [INFO] ... [Worker 0] Instrumentation applied.\n2025-10-14 22:23:41,494 [INFO] ... [Worker 0] AgentOps client initialized.\n2025-10-14 22:23:41,494 [INFO] ... [Worker 0] Started async rollouts (max: unlimited).\n</code></pre> <p>3. Start the Algorithm: In the third terminal, run the algorithm. This will kick off the entire process.</p> <p>For example, we run the algorithm code shown above with the following parameters:</p> <pre><code>prompts_to_test = [\n    \"You are a helpful assistant. {any_question}\",\n    \"You are a knowledgeable AI. {any_question}\",\n    \"You are a friendly chatbot. {any_question}\",\n]\ntask_input = \"Why is the sky blue?\"\nstore = agl.LightningStoreClient(\"http://localhost:4747\")\nfind_best_prompt(store, prompts_to_test, task_input)\n</code></pre> <p>Or you can simply use our pre-written script to try out:</p> <pre><code>python apo_custom_algorithm.py algo\n</code></pre>"},{"location":"how-to/write-first-algorithm/#understanding-the-output","title":"Understanding the Output","text":"<p>As the algorithm runs, you'll see logs appear across all three terminals, showing the components interacting in real-time.</p> <p>Algorithm Output: The algorithm terminal shows the main control flow: updating prompts, queuing tasks, and receiving the final results. You can also see the raw span data it retrieves from the store.</p> <pre><code>[Algo] Updating prompt template to: 'You are a helpful assistant. {any_question}'\n[Algo] Queuing task for clients...\n[Algo] Task 'ro-1d18988581cd' is now available for clients.\n[Algo] Received Result: rollout_id='ro-1d18988581cd' ... status='succeeded' ...\n[Algo] Queried Spans:\n  - Span(name='openai.chat.completion', attributes={'gen_ai.prompt.0.content': 'You are a helpful assistant...', 'gen_ai.completion.0.content': 'The sky appears blue...'})\n  - Span(name='reward', attributes={'value': 0.95})\n[Algo] Final reward: 0.95\n\n[Algo] Updating prompt template to: 'You are a knowledgeable AI. {any_question}'\n...\n[Algo] Final reward: 0.95\n\n[Algo] Updating prompt template to: 'You are a friendly chatbot. {any_question}'\n...\n[Algo] Final reward: 1.0\n\n[Algo] All prompts and their rewards: [('You are a helpful assistant. {any_question}', 0.95), ('You are a knowledgeable AI. {any_question}', 0.95), ('You are a friendly chatbot. {any_question}', 1.0)]\n[Algo] Best prompt found: 'You are a friendly chatbot. {any_question}' with reward 1.0\n</code></pre> <p>Runner Output: The runner terminal shows it picking up each task, executing the agent logic, and reporting the completion.</p> <pre><code>[Rollout] LLM returned: The sky appears blue due to Rayleigh scattering...\n2025-10-14 22:25:50,803 [INFO] ... [Worker 0 | Rollout ro-a9f54ac19af5] Completed in 4.24s. ...\n\n[Rollout] LLM returned: The sky looks blue because of a process called Rayleigh scattering...\n2025-10-14 22:25:59,863 [INFO] ... [Worker 0 | Rollout ro-c67eaa9016b6] Completed in 4.06s. ...\n</code></pre> <p>Store Server Output: The store terminal shows a detailed log of every interaction, confirming its role as the central hub. You can see requests to enqueue and dequeue rollouts, add spans, and update statuses.</p> <pre><code>... \"POST /enqueue_rollout HTTP/1.1\" 200 ...\n... \"GET /dequeue_rollout HTTP/1.1\" 200 ...\n... \"POST /add_span HTTP/1.1\" 200 ...\n... \"POST /update_attempt HTTP/1.1\" 200 ...\n... \"POST /wait_for_rollouts HTTP/1.1\" 200 ...\n... \"GET /query_spans/ro-c67eaa9016b6 HTTP/1.1\" 200 ...\n</code></pre> <p>So Where is Trainer?</p> <p>You might be wondering why the last tutorial focused on the Trainer class, but we haven't used it here.</p> <p>Think of the Trainer as a convenient wrapper that manages the entire training process for you. It's perfect when you want to apply a pre-built algorithm to your agent without worrying about the underlying mechanics. The Trainer handles starting the LightningStore, coordinating the Runners, managing their lifecycles, and handling errors.</p> <p>In this tutorial, however, our goal is to build a new algorithm. To do that, we need to interact directly with the core components: the Store, the Runner, and the algorithm logic itself. Running them separately gives you more control and clearer, isolated logs, which is ideal for development and debugging.</p> <p>Once your custom algorithm is mature, you can package it to comply with our standard interface (@algo or Algorithm). This allows you to use it with the Trainer again, getting all the benefits of automated lifecycle management while using your own custom logic. A sample code doing this is available in apo_custom_algorithm_trainer.py.</p>"},{"location":"reference/agent/","title":"Agent Developer APIs","text":""},{"location":"reference/agent/#agent-decorators","title":"Agent Decorators","text":"<p>Tip</p> <p>These are convenient helpers for creating agents from functions. First-time users are recommended to use these decorators to create agents.</p> <p>Warning</p> <p>The following two decorators are implementations of <code>agentlightning.rollout</code>. They are not recommended for new users.</p>"},{"location":"reference/agent/#agentlightning.rollout","title":"<code>agentlightning.rollout(func)</code>","text":"<p>Create a <code>FunctionalLitAgent</code> from an arbitrary rollout function.</p> <p>This function inspects the provided callable and creates the appropriate agent type based on its signature. It supports both LLM-based and prompt-template-based agents. The returned agent instance is callable, preserving the original function's behavior and type hints.</p> <p>See <code>llm_rollout</code> and <code>prompt_rollout</code> for more details.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Union[LlmRolloutFunc[T], PromptRolloutFunc[T], Callable[..., Any]]</code>)           \u2013            <p>Callable that implements the rollout. Supported signatures:</p> <ul> <li><code>[async ](task, llm[, rollout])</code> for LLM-based agents</li> <li><code>[async ](task, prompt_template[, rollout])</code> for prompt-template-based agents</li> </ul> <p>The supported output types of <code>func</code> is same as the return type of <code>rollout</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code># LLM-based agent\n@rollout\ndef my_llm_agent(task, llm):\n    client = OpenAI(base_url=llm.endpoint)\n    response = client.chat.completions.create(\n        model=llm.model,\n        messages=[{\"role\": \"user\", \"content\": task.input}],\n    )\n    return response\n\n# Prompt-template-based agent\n@rollout\ndef my_prompt_agent(task, prompt_template):\n    messages = prompt_template.format(task=task.input)\n    # ... perform rollout with the formatted prompt\n    return response\n\n# Function is still callable with original behavior\nresult = my_llm_agent(task, llm)\n\n# Agent methods are also available\nresult = my_llm_agent.rollout(task, resources, rollout)\n</code></pre> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>If the function signature doesn't match any known patterns.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.llm_rollout","title":"<code>agentlightning.llm_rollout(func=None, *, strip_proxy=True)</code>","text":"<pre><code>llm_rollout(\n    func: LlmRolloutFunc[T],\n) -&gt; FunctionalLitAgent[T]\n</code></pre><pre><code>llm_rollout(\n    *, strip_proxy: bool = True\n) -&gt; Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]\n</code></pre> <p>Create a <code>FunctionalLitAgent</code> for LLM-based rollouts.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>LlmRolloutFunc[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Callable defining the agent's behaviour. Supported signatures include:</p> <ul> <li><code>(task, llm) -&gt; result</code></li> <li><code>(task, llm, rollout) -&gt; result</code></li> <li><code>async (task, llm) -&gt; result</code></li> <li><code>async (task, llm, rollout) -&gt; result</code></li> </ul> </li> <li> <code>strip_proxy</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, convert proxy resources into concrete <code>LLM</code> instances before calling the function. Defaults to <code>True</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T] | Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T] | Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code>@llm_rollout\ndef my_agent(task, llm):\n    return llm.endpoint\n\n@llm_rollout(strip_proxy=False)\ndef my_agent_no_strip(task, llm):\n    return llm.model\n\nresult = my_agent(task, llm)\nresult = my_agent.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/agent/#agentlightning.prompt_rollout","title":"<code>agentlightning.prompt_rollout(func=None)</code>","text":"<pre><code>prompt_rollout(\n    func: PromptRolloutFunc[T],\n) -&gt; FunctionalLitAgent[T]\n</code></pre><pre><code>prompt_rollout() -&gt; (\n    Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]\n)\n</code></pre> <p>Create a <code>FunctionalLitAgent</code> for prompt-based rollouts.</p> <p>This decorator is designed for agents that work with tunable prompt templates. It enables a workflow where algorithms manage and optimize the prompt template, while agents consume the template to perform rollouts. This is particularly useful for prompt optimization scenarios.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>PromptRolloutFunc[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Callable defining the agent's behavior. Supported signatures include:</p> <ul> <li><code>(task, prompt_template) -&gt; result</code></li> <li><code>(task, prompt_template, rollout) -&gt; result</code></li> <li><code>async (task, prompt_template) -&gt; result</code></li> <li><code>async (task, prompt_template, rollout) -&gt; result</code></li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T] | Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T] | Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code>@prompt_rollout\ndef my_agent(task, prompt_template):\n    messages = prompt_template.format(task=task.input)\n    return messages\n\nresult = my_agent(task, prompt_template)\nresult = my_agent.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/agent/#class-based-agents","title":"Class-based Agents","text":""},{"location":"reference/agent/#agentlightning.LitAgent","title":"<code>agentlightning.LitAgent</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Base class for implementing agent rollouts.</p> <p>Subclasses override the rollout methods to process tasks while the trainer and runner infrastructure manages orchestration, tracing, and persistence.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.runner","title":"<code>runner</code>  <code>property</code>","text":"<p>Return the runner responsible for executing rollouts.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.tracer","title":"<code>tracer</code>  <code>property</code>","text":"<p>Return the tracer configured for this agent.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.trainer","title":"<code>trainer</code>  <code>property</code>","text":"<p>Return the trainer associated with this agent.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.__init__","title":"<code>__init__(*, trained_agents=None)</code>","text":"<p>Initialize the agent instance.</p> <p>Parameters:</p> <ul> <li> <code>trained_agents</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional identifier used by legacy tooling to mark trained agents.</p> </li> </ul> <p>Deprecated</p> <p>The <code>trained_agents</code> flag is deprecated. Configure <code>agent_match</code> in the adapter layer instead. See <code>TracerTraceToTriplet</code> for more details.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.get_runner","title":"<code>get_runner()</code>","text":"<p>Return the runner responsible for executing rollouts.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.get_tracer","title":"<code>get_tracer()</code>","text":"<p>Return the tracer configured for this agent.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.get_trainer","title":"<code>get_trainer()</code>","text":"<p>Return the trainer associated with this agent.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.is_async","title":"<code>is_async()</code>","text":"<p>Return <code>True</code> when the agent overrides any asynchronous rollout methods.</p> <p>Override this method for customized async detection logic.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.on_rollout_end","title":"<code>on_rollout_end(task, rollout, runner, tracer)</code>","text":"<p>Hook invoked after a rollout completes.</p> <p>Subclasses can override this method for cleanup or additional logging. The default implementation is a no-op.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>Task</code>)           \u2013            <p><code>Task</code> that was processed.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Resulting <code>Rollout</code>.</p> </li> <li> <code>runner</code>               (<code>Runner[T]</code>)           \u2013            <p><code>Runner</code> managing the rollout.</p> </li> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p><code>Tracer</code> associated with the runner.</p> </li> </ul> <p>Deprecated</p> <p>Override <code>Hook.on_rollout_end</code> instead of this method when extending agents.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.on_rollout_start","title":"<code>on_rollout_start(task, runner, tracer)</code>","text":"<p>Hook invoked immediately before a rollout begins.</p> <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. The default implementation is a no-op.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>Task</code>)           \u2013            <p><code>Task</code> that will be processed.</p> </li> <li> <code>runner</code>               (<code>Runner[T]</code>)           \u2013            <p><code>Runner</code> managing the rollout.</p> </li> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p><code>Tracer</code> associated with the runner.</p> </li> </ul> <p>Deprecated</p> <p>Override <code>Hook.on_rollout_start</code> instead of this method when extending agents.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.rollout","title":"<code>rollout(task, resources, rollout)</code>","text":"<p>Execute a rollout synchronously.</p> <p>If you don't wish to implement both training rollout and validation rollout separately, you can just implement <code>rollout</code> which will work for both.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>T</code>)           \u2013            <p>Task payload provided by the scheduler.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of named resources (for example LLMs or prompt templates).</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Rollout metadata. Avoid mutating this object directly unless a subclass needs to override defaults.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RolloutRawResult</code>           \u2013            <p>One of the following values:</p> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>None</code> when tracing is handled by the runner.</li> </ul> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>float</code> representing the final reward.</li> </ul> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>List[ReadableSpan]</code> with OpenTelemetry spans.</li> </ul> </li> <li> <code>RolloutRawResult</code>           \u2013            <ul> <li><code>List[Span]</code> with Agent Lightning spans.</li> </ul> </li> </ul>"},{"location":"reference/agent/#agentlightning.LitAgent.rollout_async","title":"<code>rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Execute a rollout asynchronously.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>T</code>)           \u2013            <p>Task payload provided by the scheduler.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of named resources (for example LLMs or prompt templates).</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Rollout metadata. Avoid mutating this object directly unless a subclass needs to override defaults.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RolloutRawResult</code>           \u2013            <p>Same possible return values as</p> </li> <li> <code>RolloutRawResult</code>           \u2013            <p><code>rollout</code>.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.LitAgent.set_runner","title":"<code>set_runner(runner)</code>","text":"<p>Attach the runner responsible for executing rollouts.</p> <p>Parameters:</p> <ul> <li> <code>runner</code>               (<code>Runner[T]</code>)           \u2013            <p><code>Runner</code> coordinating execution.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.LitAgent.set_trainer","title":"<code>set_trainer(trainer)</code>","text":"<p>Attach the trainer responsible for orchestration.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>Trainer</code>)           \u2013            <p><code>Trainer</code> that manages the agent.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.LitAgent.training_rollout","title":"<code>training_rollout(task, resources, rollout)</code>","text":"<p>Process a single training task synchronously.</p> <p>By default, this method delegates to <code>rollout</code>.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.training_rollout_async","title":"<code>training_rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Process a single training task asynchronously.</p> <p>By default, this method delegates to <code>rollout_async</code>.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.validation_rollout","title":"<code>validation_rollout(task, resources, rollout)</code>","text":"<p>Process a single validation task synchronously.</p> <p>Override this method when validation should differ from training. The default implementation delegates to <code>training_rollout</code>.</p>"},{"location":"reference/agent/#agentlightning.LitAgent.validation_rollout_async","title":"<code>validation_rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Process a single validation task asynchronously.</p> <p>Override this method when validation should differ from training. The default implementation delegates to <code>training_rollout_async</code>.</p>"},{"location":"reference/agent/#emitter","title":"Emitter","text":""},{"location":"reference/agent/#agentlightning.emit_reward","title":"<code>agentlightning.emit_reward(reward)</code>","text":"<p>Emit a reward value as an OpenTelemetry span.</p> <p>Parameters:</p> <ul> <li> <code>reward</code>               (<code>float</code>)           \u2013            <p>Numeric reward to record. Integers and booleans are converted to floating point numbers for consistency.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ReadableSpan</code>           \u2013            <p>Readable span capturing the recorded reward.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the provided reward cannot be interpreted as a float or the resulting span is not a <code>ReadableSpan</code> instance.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.emit_message","title":"<code>agentlightning.emit_message(message)</code>","text":"<p>Emit a textual message as an OpenTelemetry span.</p> <p>Parameters:</p> <ul> <li> <code>message</code>               (<code>str</code>)           \u2013            <p>Human readable message to attach as a span attribute.</p> </li> </ul> <p>Note</p> <p>OpenTelemetry distinguishes between logs and spans. Emitting the message as a span keeps all Agent Lightning telemetry in a single data store for analysis.</p>"},{"location":"reference/agent/#agentlightning.emit_object","title":"<code>agentlightning.emit_object(object)</code>","text":"<p>Emit an object's serialized representation as an OpenTelemetry span.</p> <p>Parameters:</p> <ul> <li> <code>object</code>               (<code>Any</code>)           \u2013            <p>Data structure to encode as JSON and attach to the span payload.</p> </li> </ul> <p>Note</p> <p>The payload must be JSON serializable. Non-serializable objects are ignored and an error is logged to aid debugging.</p>"},{"location":"reference/agent/#agentlightning.emit_exception","title":"<code>agentlightning.emit_exception(exception)</code>","text":"<p>Record an exception with OpenTelemetry metadata.</p> <p>Parameters:</p> <ul> <li> <code>exception</code>               (<code>BaseException</code>)           \u2013            <p>Raised exception instance to serialize into telemetry attributes.</p> </li> </ul> <p>Note</p> <p>The helper validates its input. Non-exception values are ignored to prevent noisy telemetry and indicate programming mistakes via the logger.</p>"},{"location":"reference/agent/#reward-helpers","title":"Reward Helpers","text":""},{"location":"reference/agent/#agentlightning.find_final_reward","title":"<code>agentlightning.find_final_reward(spans)</code>","text":"<p>Return the last reward value present in the provided spans.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>Sequence[SpanLike]</code>)           \u2013            <p>Sequence containing <code>ReadableSpan</code> objects or mocked span-like values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[float]</code>           \u2013            <p>Reward value from the latest reward span, or <code>None</code> when none are found.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.find_reward_spans","title":"<code>agentlightning.find_reward_spans(spans)</code>","text":"<p>Return all reward spans in the provided sequence.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>Sequence[SpanLike]</code>)           \u2013            <p>Sequence containing <code>ReadableSpan</code> objects or mocked span-like values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[SpanLike]</code>           \u2013            <p>List of spans that could be parsed as rewards.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.get_reward_value","title":"<code>agentlightning.get_reward_value(span)</code>","text":"<p>Extract the reward value from a span, if available.</p> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>SpanLike</code>)           \u2013            <p>Span object produced by AgentOps or Agent Lightning emitters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[float]</code>           \u2013            <p>The reward encoded in the span or <code>None</code> when the span does not represent a reward.</p> </li> </ul>"},{"location":"reference/agent/#agentlightning.is_reward_span","title":"<code>agentlightning.is_reward_span(span)</code>","text":"<p>Return <code>True</code> when the provided span encodes a reward value.</p>"},{"location":"reference/agent/#legacy-emitter-decorators","title":"Legacy Emitter Decorators","text":""},{"location":"reference/agent/#agentlightning.reward.reward","title":"<code>agentlightning.reward.reward(fn)</code>","text":"<p>Decorate a reward function so its outputs are tracked as spans.</p> <p>The decorator integrates with AgentOps when it is available and falls back to the built-in telemetry otherwise. Both synchronous and asynchronous functions are supported transparently.</p> Deprecated <p>This decorator is deprecated. Use <code>emit_reward</code> instead.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>               (<code>FnType</code>)           \u2013            <p>Callable that produces a numeric reward.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FnType</code>           \u2013            <p>Wrapped callable that preserves the original signature.</p> </li> </ul>"},{"location":"reference/algorithm/","title":"Algorithm","text":""},{"location":"reference/algorithm/#algorithm-side-references","title":"Algorithm-side References","text":"<p>Note</p> <p>This reference covers APIs that are designed to be used at \"Algorithm Side\". For built-in algorithms, see Algorithm Zoo.</p>"},{"location":"reference/algorithm/#base-class-and-decorators","title":"Base Class and Decorators","text":""},{"location":"reference/algorithm/#agentlightning.Algorithm","title":"<code>agentlightning.Algorithm</code>","text":"<p>Algorithm is the strategy, or tuner to train the agent.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_adapter","title":"<code>get_adapter()</code>","text":"<p>Retrieve the adapter for this algorithm to communicate with the runners.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_client","title":"<code>get_client()</code>","text":"<p>Get the client to communicate with the algorithm.</p> <p>If the algorithm does not require a server-client communication, it can also create a mock client that never communicates with itself.</p> <p>Deprecated and will be removed in a future version.</p> <p>Returns:</p> <ul> <li> <code>AgentLightningClient</code>           \u2013            <p>The AgentLightningClient instance associated with this algorithm.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_initial_resources","title":"<code>get_initial_resources()</code>","text":"<p>Get the initial resources for this algorithm.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_llm_proxy","title":"<code>get_llm_proxy()</code>","text":"<p>Retrieve the configured LLM proxy instance, if one has been set.</p> <p>Returns:</p> <ul> <li> <code>Optional[LLMProxy]</code>           \u2013            <p>The active LLMProxy instance or None when not configured.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_store","title":"<code>get_store()</code>","text":"<p>Retrieve the store for this algorithm to communicate with the runners.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.get_trainer","title":"<code>get_trainer()</code>","text":"<p>Get the trainer for this algorithm.</p> <p>Returns:</p> <ul> <li> <code>Trainer</code>           \u2013            <p>The Trainer instance associated with this agent.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.is_async","title":"<code>is_async()</code>","text":"<p>Return True if the algorithm is asynchronous.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.run","title":"<code>run(train_dataset=None, val_dataset=None)</code>","text":"<p>Subclasses should implement this method to implement the algorithm.</p> <p>Parameters:</p> <ul> <li> <code>train_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>The dataset to train on. Not all algorithms require a training dataset.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>The dataset to validate on. Not all algorithms require a validation dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[None, Awaitable[None]]</code>           \u2013            <p>Algorithm should refrain from returning anything. It should just run the algorithm.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_adapter","title":"<code>set_adapter(adapter)</code>","text":"<p>Set the adapter for this algorithm to collect and convert traces.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_initial_resources","title":"<code>set_initial_resources(resources)</code>","text":"<p>Set the initial resources for this algorithm.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_llm_proxy","title":"<code>set_llm_proxy(llm_proxy)</code>","text":"<p>Set the LLM proxy for this algorithm to reuse when available.</p> <p>Parameters:</p> <ul> <li> <code>llm_proxy</code>               (<code>LLMProxy | None</code>)           \u2013            <p>The LLMProxy instance configured by the trainer, if any.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_store","title":"<code>set_store(store)</code>","text":"<p>Set the store for this algorithm to communicate with the runners.</p> <p>Store is set directly instead of using weakref because its copy is meant to be maintained throughout the algorithm's lifecycle.</p>"},{"location":"reference/algorithm/#agentlightning.Algorithm.set_trainer","title":"<code>set_trainer(trainer)</code>","text":"<p>Set the trainer for this algorithm.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>Trainer</code>)           \u2013            <p>The Trainer instance that will handle training and validation.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.algo","title":"<code>agentlightning.algo(func)</code>","text":"<pre><code>algo(\n    func: AlgorithmFuncAsync,\n) -&gt; FunctionalAlgorithm[Literal[True]]\n</code></pre><pre><code>algo(\n    func: AlgorithmFuncAsyncFallback,\n) -&gt; FunctionalAlgorithm[Any]\n</code></pre><pre><code>algo(\n    func: AlgorithmFuncSync,\n) -&gt; FunctionalAlgorithm[Literal[False]]\n</code></pre><pre><code>algo(\n    func: AlgorithmFuncSyncFallback,\n) -&gt; FunctionalAlgorithm[Any]\n</code></pre> <p>Convert a callable into a <code>FunctionalAlgorithm</code>.</p> <p>The decorator inspects the callable signature to decide which dependencies to inject at runtime, enabling concise algorithm definitions that still leverage the full training runtime.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Union[AlgorithmFuncSync, AlgorithmFuncAsync, AlgorithmFuncSyncFallback, AlgorithmFuncAsyncFallback]</code>)           \u2013            <p>Function implementing the algorithm logic. May be synchronous or asynchronous. The function can expect all of, or a subset of the following parameters:</p> <ul> <li><code>store</code>: <code>LightningStore</code>,</li> <li><code>train_dataset</code>: <code>Dataset</code>,</li> <li><code>val_dataset</code>: <code>Dataset</code>,</li> <li><code>llm_proxy</code>: <code>LLMProxy</code>,</li> <li><code>adapter</code>: <code>TraceAdapter</code>,</li> <li><code>initial_resources</code>: <code>NamedResources</code>,</li> </ul> <p>If the function does not expect a parameter, the wrapper will not inject it into the call. Using <code>*args</code> and <code>**kwargs</code> will not work and no parameters will be injected.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[FunctionalAlgorithm[Literal[False]], FunctionalAlgorithm[Literal[True]]]</code>           \u2013            <p>FunctionalAlgorithm that proxies the callable while exposing the</p> </li> <li> <code>Union[FunctionalAlgorithm[Literal[False]], FunctionalAlgorithm[Literal[True]]]</code>           \u2013            <p><code>Algorithm</code> interface.</p> </li> </ul> <p>Examples:</p> <pre><code>from agentlightning.algorithm.decorator import algo\n\n@algo\ndef batching_algorithm(*, store, train_dataset, val_dataset):\n    for sample in train_dataset:\n        store.enqueue_rollout(input=sample, mode=\"train\")\n\n@algo\nasync def async_algorithm(*, store, train_dataset=None, val_dataset=None):\n    await store.enqueue_rollout(input={\"prompt\": \"hello\"}, mode=\"train\")\n</code></pre>"},{"location":"reference/algorithm/#fast-algorithms-for-debugging","title":"Fast Algorithms (for Debugging)","text":""},{"location":"reference/algorithm/#agentlightning.FastAlgorithm","title":"<code>agentlightning.FastAlgorithm</code>","text":"<p>               Bases: <code>Algorithm</code></p> <p>Base class for lightweight algorithms optimised for developer workflows.</p> <p>Fast algorithms prioritise short feedback loops so an agent developer can run small-scale experiments without waiting for long-running training jobs to finish.</p>"},{"location":"reference/algorithm/#agentlightning.Baseline","title":"<code>agentlightning.Baseline</code>","text":"<p>               Bases: <code>FastAlgorithm</code></p> <p>Reference implementation that streams the full dataset through the rollout queue.</p> <p>The baseline algorithm batches task submissions, waits for each rollout to finish, and logs every collected span and reward. It is primarily useful as a smoke test for the platform plumbing rather than a performant trainer.</p> <p>Parameters:</p> <ul> <li> <code>n_epochs</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of dataset passes to execute for both the train and val splits during developer experiments.</p> </li> <li> <code>train_split</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Fraction of the concatenated dataset to treat as training data. Must be strictly between 0 and 1.</p> </li> <li> <code>polling_interval</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Interval, in seconds, to poll the store for queue depth and rollout completion.</p> </li> <li> <code>max_queue_length</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of rollouts allowed to wait in the queue before throttling additional submissions.</p> </li> <li> <code>span_verbosity</code>               (<code>Literal['keys', 'key_values', 'none']</code>, default:                   <code>'keys'</code> )           \u2013            <p>Level of detail to include when logging span metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>train_split</code> falls outside the <code>(0, 1)</code> interval.</p> </li> </ul> <p>Examples:</p> <pre><code>from agentlightning.algorithm.fast import Baseline\n\nalgorithm = Baseline(n_epochs=2, train_split=0.8, span_verbosity=\"key_values\")\ntrainer.fit(algorithm, train_dataset=my_train, val_dataset=my_val)\n</code></pre>"},{"location":"reference/algorithm/#agentlightning.Baseline.run","title":"<code>run(train_dataset=None, val_dataset=None)</code>  <code>async</code>","text":"<p>Execute the baseline loop across the provided datasets.</p>"},{"location":"reference/algorithm/#adapter","title":"Adapter","text":""},{"location":"reference/algorithm/#agentlightning.Adapter","title":"<code>agentlightning.Adapter</code>","text":"<p>               Bases: <code>Generic[T_from, T_to]</code></p> <p>Base class for synchronous adapters that convert data from one format to another.</p> <p>The class defines a minimal protocol so that adapters can be treated like callables while still allowing subclasses to supply the concrete transformation logic.</p> <p>Note</p> <p>Subclasses must override <code>adapt()</code> to provide the actual conversion.</p> <p>Type Variables:</p> <pre><code>T_from: Source data type supplied to the adapter.\n\nT_to: Target data type produced by the adapter.\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class IntToStrAdapter(Adapter[int, str]):\n...     def adapt(self, source: int) -&gt; str:\n...         return str(source)\n...\n&gt;&gt;&gt; adapter = IntToStrAdapter()\n&gt;&gt;&gt; adapter(42)\n'42'\n</code></pre>"},{"location":"reference/algorithm/#agentlightning.Adapter.__call__","title":"<code>__call__(source)</code>","text":"<p>Convert the data to the target format.</p> <p>This method delegates to <code>adapt()</code> so that an instance of <code>Adapter</code> can be used like a standard function.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>T_from</code>)           \u2013            <p>Input data in the source format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T_to</code>           \u2013            <p>Data converted to the target format.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.Adapter.adapt","title":"<code>adapt(source)</code>","text":"<p>Convert the data to the target format.</p> <p>Subclasses must override this method with the concrete transformation logic. The base implementation raises <code>NotImplementedError</code> to make the requirement explicit.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>T_from</code>)           \u2013            <p>Input data in the source format.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T_to</code>           \u2013            <p>Data converted to the target format.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TraceAdapter","title":"<code>agentlightning.TraceAdapter</code>","text":"<p>               Bases: <code>Adapter[List[Span], T_to]</code>, <code>Generic[T_to]</code></p> <p>Base class for adapters that convert trace spans into other formats.</p> <p>This class specializes <code>Adapter</code> for working with <code>Span</code> instances emitted by Agent Lightning instrumentation. Subclasses receive entire trace slices and return a format suited for the downstream consumer, for example reinforcement learning training data or observability metrics.</p>"},{"location":"reference/algorithm/#agentlightning.OtelTraceAdapter","title":"<code>agentlightning.OtelTraceAdapter</code>","text":"<p>               Bases: <code>Adapter[List[ReadableSpan], T_to]</code>, <code>Generic[T_to]</code></p> <p>Base class for adapters that convert OpenTelemetry trace spans into other formats.</p> <p>This specialization of <code>Adapter</code> expects a list of <code>opentelemetry.sdk.trace.ReadableSpan</code> instances and produces any target format, such as reinforcement learning trajectories, structured logs, or analytics-ready payloads.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class TraceToDictAdapter(OtelTraceAdapter[dict]):\n...     def adapt(self, spans: List[ReadableSpan]) -&gt; dict:\n...         return {\"count\": len(spans)}\n...\n&gt;&gt;&gt; adapter = TraceToDictAdapter()\n&gt;&gt;&gt; adapter([span1, span2])\n{'count': 2}\n</code></pre>"},{"location":"reference/algorithm/#agentlightning.TraceToTripletBase","title":"<code>agentlightning.TraceToTripletBase</code>","text":"<p>               Bases: <code>TraceAdapter[List[Triplet]]</code></p> <p>Base class for adapters that emit <code>Triplet</code> trajectories.</p>"},{"location":"reference/algorithm/#agentlightning.TracerTraceToTriplet","title":"<code>agentlightning.TracerTraceToTriplet</code>","text":"<p>               Bases: <code>TraceToTripletBase</code></p> <p>Convert tracer-emitted spans into triplet trajectories.</p> <p>Attributes:</p> <ul> <li> <code>repair_hierarchy</code>           \u2013            <p>When <code>True</code>, repair the span tree using <code>TraceTree.repair_hierarchy()</code> before matching calls and rewards.</p> </li> <li> <code>llm_call_match</code>           \u2013            <p>Regular expression pattern that selects LLM call span names.</p> </li> <li> <code>agent_match</code>           \u2013            <p>Optional regular expression pattern for agent span names. When omitted, spans from any agent are considered.</p> </li> <li> <code>exclude_llm_call_in_reward</code>           \u2013            <p>When <code>True</code>, ignore matches under reward spans while searching for rewards.</p> </li> <li> <code>reward_match</code>           \u2013            <p>Strategy used to associate rewards with LLM calls.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TracerTraceToTriplet.adapt","title":"<code>adapt(source)</code>","text":"<p>Convert tracer spans into <code>Triplet</code> trajectories.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Union[List[Span], List[ReadableSpan]]</code>)           \u2013            <p>Agent Lightning spans or raw OpenTelemetry spans that form a trace.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Triplet]</code>           \u2013            <p>Ordered list of trajectory transitions with prompt, response, and reward information.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TracerTraceToTriplet.visualize","title":"<code>visualize(source, /, filename='trace_tree', interested_span_match=None)</code>","text":"<p>Visualize the trace tree built from the supplied spans.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>Union[List[Span], List[ReadableSpan]]</code>)           \u2013            <p>Collection of Agent Lightning <code>Span</code> objects or raw <code>opentelemetry.sdk.trace.ReadableSpan</code> instances.</p> </li> <li> <code>filename</code>               (<code>str</code>, default:                   <code>'trace_tree'</code> )           \u2013            <p>Base filename for the generated image; <code>.png</code> is appended automatically.</p> </li> <li> <code>interested_span_match</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional regular expression used to highlight a subset of spans.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TraceTree</code>           \u2013            <p>The <code>TraceTree</code> built from the provided</p> </li> <li> <code>TraceTree</code>           \u2013            <p>spans.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LlmProxyTraceToTriplet","title":"<code>agentlightning.LlmProxyTraceToTriplet</code>","text":"<p>               Bases: <code>TraceToTripletBase</code></p> <p>Convert telemetry emitted by the LLM Proxy into triplet trajectories.</p> <p>Warning</p> <p>This adapter is experimental and might be merged with <code>TracerTraceToTriplet</code> in the future.</p> <p>Danger</p> <p>Do not rely on timestamps when using this adapter. Proxy spans can originate on different machines with unsynchronised clocks, so <code>sequence_id</code> is treated as the sole source of ordering.</p> <p>Strategy:</p> <ol> <li>Sort spans by <code>(sequence_id, start_time)</code> for deterministic processing.</li> <li>Extract token identifiers from <code>litellm_request</code> or <code>raw_gen_ai_request</code> spans.</li> <li>Extract rewards from spans exposing AgentOps-style payloads or explicit reward spans.</li> <li>Match each reward to the most recent unmatched LLM call whose sequence is smaller.</li> </ol>"},{"location":"reference/algorithm/#agentlightning.LlmProxyTraceToTriplet.adapt","title":"<code>adapt(source)</code>","text":"<p>Convert LLM Proxy spans into <code>Triplet</code> trajectories.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>List[Span]</code>)           \u2013            <p>Spans emitted by the LLM Proxy containing prompt, response, and reward data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Triplet]</code>           \u2013            <p>Ordered trajectory transitions matched purely by <code>sequence_id</code>.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TraceToMessages","title":"<code>agentlightning.TraceToMessages</code>","text":"<p>               Bases: <code>TraceAdapter[List[OpenAIMessages]]</code></p> <p>Convert trace spans into OpenAI-compatible conversation messages.</p> <p>The adapter reconstructs prompts, completions, tool calls, and function definitions from <code>gen_ai.*</code> span attributes. The resulting objects match the JSONL structure expected by the OpenAI fine-tuning pipeline.</p> <p>Warning</p> <p>The adapter assumes all spans share a common trace and that tool call spans are direct children of the associated completion span.</p>"},{"location":"reference/algorithm/#agentlightning.TraceToMessages.adapt","title":"<code>adapt(source)</code>","text":"<p>Transform trace spans into OpenAI chat payloads.</p> <p>Parameters:</p> <ul> <li> <code>source</code>               (<code>List[Span]</code>)           \u2013            <p>Spans containing <code>gen_ai.*</code> attributes emitted by the tracing pipeline.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[OpenAIMessages]</code>           \u2013            <p>A list of <code>OpenAIMessages</code> entries that</p> </li> <li> <code>List[OpenAIMessages]</code>           \u2013            <p>capture prompts, completions, tools, and metadata.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.TraceToMessages.get_tool_calls","title":"<code>get_tool_calls(completion, all_spans)</code>","text":"<p>Yield tool call payloads for a completion span.</p> <p>Parameters:</p> <ul> <li> <code>completion</code>               (<code>Span</code>)           \u2013            <p>The completion span whose descendants should be inspected.</p> </li> <li> <code>all_spans</code>               (<code>List[Span]</code>)           \u2013            <p>The complete span list belonging to the trace.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Iterable[Dict[str, Any]]</code>           \u2013            <p>Dictionaries describing tool calls with identifiers, names, and arguments.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a candidate tool span cannot be converted into a dictionary.</p> </li> </ul>"},{"location":"reference/algorithm/#llm-proxy","title":"LLM Proxy","text":""},{"location":"reference/algorithm/#agentlightning.LLMProxy","title":"<code>agentlightning.LLMProxy</code>","text":"<p>Host a LiteLLM OpenAI-compatible proxy bound to a LightningStore.</p> <p>The proxy:</p> <ul> <li>Serves an OpenAI-compatible API via uvicorn.</li> <li>Adds rollout/attempt routing and headers via middleware.</li> <li>Registers OTEL export and token-id callbacks.</li> <li>Writes a LiteLLM worker config file with <code>model_list</code> and settings.</li> </ul> <p>Lifecycle:</p> <ul> <li><code>start()</code> writes config, starts uvicorn server in a thread, and waits until ready.</li> <li><code>stop()</code> tears down the server and removes the temp config file.</li> <li><code>restart()</code> convenience wrapper to stop then start.</li> </ul> <p>Usage Note: As the LLM Proxy sets up an OpenTelemetry tracer, it's recommended to run it in a different process from the main runner (i.e., tracer from agents).</p> <p>Warning</p> <p>The LLM Proxy does support streaming, but the tracing is still problematic when streaming is enabled.</p> <p>Danger</p> <p>Do not run LLM proxy in the same process as the main runner. It's easy to cause conflicts in the tracer provider with tracers like <code>AgentOpsTracer</code>.</p> <p>Parameters:</p> <ul> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>TCP port to bind.</p> </li> <li> <code>model_list</code>               (<code>List[ModelConfig] | None</code>, default:                   <code>None</code> )           \u2013            <p>LiteLLM <code>model_list</code> entries.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>LightningStore used for span sequence and persistence.</p> </li> <li> <code>host</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Publicly reachable host used in resource endpoints. Defaults to best-guess IPv4.</p> </li> <li> <code>litellm_config</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Extra LiteLLM proxy config merged with <code>model_list</code>.</p> </li> <li> <code>num_retries</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Default LiteLLM retry count injected into <code>litellm_settings</code>.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.as_resource","title":"<code>as_resource(rollout_id=None, attempt_id=None, model=None, sampling_parameters=None)</code>","text":"<p>Create an <code>LLM</code> resource pointing at this proxy with rollout context.</p> The returned endpoint is <p><code>http://{host}:{port}/rollout/{rollout_id}/attempt/{attempt_id}</code></p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Rollout identifier used for span attribution. If None, will instantiate a ProxyLLM resource.</p> </li> <li> <code>attempt_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Attempt identifier used for span attribution. If None, will instantiate a ProxyLLM resource.</p> </li> <li> <code>model</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Logical model name to use. If omitted and exactly one model is configured or all models have the same name, that model is used.</p> </li> <li> <code>sampling_parameters</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional default sampling parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LLM</code> (              <code>LLM</code> )          \u2013            <p>Configured resource ready for OpenAI-compatible calls.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>model</code> is omitted and zero or multiple models are configured.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.get_store","title":"<code>get_store()</code>","text":"<p>Get the store used by the proxy.</p> <p>Returns:</p> <ul> <li> <code>Optional[LightningStore]</code>           \u2013            <p>The store used by the proxy.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.initialize","title":"<code>initialize()</code>","text":"<p>Initialize global middleware and LiteLLM callbacks.</p> <p>Installs:</p> <ul> <li>A FastAPI middleware that rewrites /rollout/{rid}/attempt/{aid}/... paths, injects rollout/attempt/sequence headers, and forwards downstream.</li> <li>LiteLLM callbacks for token ids and OpenTelemetry export.</li> </ul> <p>The middleware can only be installed once because once the FastAPI app has started, the middleware cannot be changed any more.</p> <p>This function does not start any server. It only wires global hooks.</p>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.is_running","title":"<code>is_running()</code>","text":"<p>Return whether the uvicorn server is active.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if server was started and did not signal exit.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.restart","title":"<code>restart(*, _port=None)</code>","text":"<p>Restart the proxy if running, else start it.</p> <p>Convenience wrapper calling <code>stop()</code> followed by <code>start()</code>.</p>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.set_store","title":"<code>set_store(store)</code>","text":"<p>Set the store for the proxy.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>The store to use for the proxy.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.start","title":"<code>start()</code>","text":"<p>Start the proxy server thread and initialize global wiring.</p> <p>Side effects:</p> <ul> <li>Sets the module-level global store for middleware/exporter access.</li> <li>Calls <code>initialize()</code> once to register middleware and callbacks.</li> <li>Writes a temporary YAML config consumed by LiteLLM worker.</li> <li>Launches uvicorn in a daemon thread and waits for readiness.</li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.stop","title":"<code>stop()</code>","text":"<p>Stop the proxy server and clean up temporary artifacts.</p> <p>This is a best-effort graceful shutdown with a bounded join timeout.</p>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.update_model_list","title":"<code>update_model_list(model_list)</code>","text":"<p>Replace the in-memory model list and hot-restart if running.</p> <p>Parameters:</p> <ul> <li> <code>model_list</code>               (<code>List[ModelConfig]</code>)           \u2013            <p>New list of model entries.</p> </li> </ul>"},{"location":"reference/algorithm/#agentlightning.LLMProxy.update_port","title":"<code>update_port(port)</code>","text":"<p>Update the port for the proxy.</p> <p>Parameters:</p> <ul> <li> <code>port</code>               (<code>int</code>)           \u2013            <p>The new port to use for the proxy.</p> </li> </ul>"},{"location":"reference/cli/","title":"Command Line Interface","text":"<p>Warning</p> <p>This document is a work in progress and might not be updated with the latest changes. Try to use <code>agl -h</code> to get the latest help message.</p> <p>Tip</p> <p>Agent-lightning also provides utilities to help you build your own CLI for LitAgent and Trainer. See Trainer for references.</p>"},{"location":"reference/cli/#agl","title":"agl","text":"<pre><code>usage: agl [-h] {vllm,store,agentops}\n\nAgent Lightning CLI entry point.\n\nAvailable subcommands:\n  vllm      Run the vLLM CLI with Agent Lightning instrumentation.\n  store     Run a LightningStore server.\n  agentops  Start the AgentOps server manager.\n\npositional arguments:\n  {vllm,store,agentops}\n                        Subcommand to run.\n\noptions:\n  -h, --help            show this help message and exit\n</code></pre>"},{"location":"reference/cli/#agl-vllm","title":"agl vllm","text":"<p>Agent-lightning's instrumented vLLM CLI.</p> <pre><code>usage: agl vllm [-h] [-v] {chat,complete,serve,bench,collect-env,run-batch} ...\n\nvLLM CLI\n\npositional arguments:\n  {chat,complete,serve,bench,collect-env,run-batch}\n    chat                Generate chat completions via the running API server.\n    complete            Generate text completions based on the given prompt via the running API server.\n    collect-env         Start collecting environment information.\n    run-batch           Run batch prompts and write results to file.\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         show program's version number and exit\n\nFor full list:            vllm [subcommand] --help=all\nFor a section:            vllm [subcommand] --help=ModelConfig    (case-insensitive)\nFor a flag:               vllm [subcommand] --help=max-model-len  (_ or - accepted)\nDocumentation:            https://docs.vllm.ai\n</code></pre>"},{"location":"reference/cli/#agl-store","title":"agl store","text":"<p>Agent-lightning's LightningStore CLI. Use it to start an independent LightningStore server.</p> <p>Currently the store data are stored in memory and will be lost when the server is stopped.</p> <pre><code>usage: agl store [-h] [--port PORT]\n\nRun a LightningStore server\n\noptions:\n  -h, --help   show this help message and exit\n  --port PORT  Port to run the server on\n</code></pre>"},{"location":"reference/cli/#agl-agentops","title":"agl agentops","text":"<p>Start a mock AgentOps server to bypass the online service of AgentOps.</p> <pre><code>usage: agl agentops [-h] [--daemon] [--port PORT]\n\nStart AgentOps server\n\noptions:\n  -h, --help   show this help message and exit\n  --daemon     Run server as a daemon\n  --port PORT  Port to run the server on\n</code></pre>"},{"location":"reference/instrumentation/","title":"Instrumentation API","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.instrument_all","title":"<code>agentlightning.instrumentation.instrument_all()</code>","text":"<p>Instrument all the instrumentation libraries.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.uninstrument_all","title":"<code>agentlightning.instrumentation.uninstrument_all()</code>","text":"<p>Uninstrument all the instrumentation libraries.</p>"},{"location":"reference/instrumentation/#agentops-langchain","title":"AgentOps LangChain","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops_langchain","title":"<code>agentlightning.instrumentation.agentops_langchain</code>","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops_langchain.instrument_agentops_langchain","title":"<code>instrument_agentops_langchain()</code>","text":"<p>Bypass AgentOp's native support for Langchain.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops_langchain.uninstrument_agentops_langchain","title":"<code>uninstrument_agentops_langchain()</code>","text":"<p>Restore AgentOp's native support for Langchain.</p>"},{"location":"reference/instrumentation/#agentops","title":"AgentOps","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops","title":"<code>agentlightning.instrumentation.agentops</code>","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.AgentOpsServerManager","title":"<code>AgentOpsServerManager</code>","text":"<p>Manages a AgentOps local server to bypass the online service of AgentOps.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.agentops_local_server","title":"<code>agentops_local_server()</code>","text":"<p>Returns a Flask app that can be used to test agentops integration. This server provides endpoints for token fetching and a catch-all endpoint.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.instrument_agentops","title":"<code>instrument_agentops()</code>","text":"<p>Instrument agentops to capture token IDs. Automatically detects and uses the appropriate patching method based on the installed agentops version.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.agentops.uninstrument_agentops","title":"<code>uninstrument_agentops()</code>","text":"<p>Uninstrument agentops to stop capturing token IDs.</p>"},{"location":"reference/instrumentation/#litellm","title":"LiteLLM","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.litellm","title":"<code>agentlightning.instrumentation.litellm</code>","text":"<p>LiteLLM instrumentations.</p> <p>It's unclear whether or not this file is useful. It seems that LiteLLM owns its own telemetry from their own entrance</p> <p>Related documentation.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.litellm.instrument_litellm","title":"<code>instrument_litellm()</code>","text":"<p>Instrument litellm to capture token IDs.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.litellm.uninstrument_litellm","title":"<code>uninstrument_litellm()</code>","text":"<p>Uninstrument litellm to stop capturing token IDs.</p>"},{"location":"reference/instrumentation/#vllm","title":"vLLM","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.vllm","title":"<code>agentlightning.instrumentation.vllm</code>","text":""},{"location":"reference/instrumentation/#agentlightning.instrumentation.vllm.instrument_vllm","title":"<code>instrument_vllm()</code>","text":"<p>Instrument vLLM to capture token IDs generated by engine.</p> <p>This instrumentation has been merged to upstream vLLM since v0.10.2.</p>"},{"location":"reference/instrumentation/#agentlightning.instrumentation.vllm.uninstrument_vllm","title":"<code>uninstrument_vllm()</code>","text":"<p>Uninstrument vLLM to stop capturing token IDs generated by engine.</p>"},{"location":"reference/internal/","title":"Internal API References","text":"<p>Danger</p> <p>The following APIs should be used with extra caution because they are very likely to change in the future.</p>"},{"location":"reference/internal/#agentlightning.adapter.messages.OpenAIMessages","title":"<code>agentlightning.adapter.messages.OpenAIMessages</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>OpenAI-style chat messages with optional tool definitions.</p> <p>Attributes:</p> <ul> <li> <code>messages</code>               (<code>List[ChatCompletionMessageParam]</code>)           \u2013            <p>Ordered chat messages that describe the conversation.</p> </li> <li> <code>tools</code>               (<code>Optional[List[ChatCompletionFunctionToolParam]]</code>)           \u2013            <p>Tool specifications available to the assistant, if any.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree","title":"<code>agentlightning.adapter.triplet.TraceTree</code>","text":"<p>Tree representation of a trace span and its descendants.</p> <p>Attributes:</p> <ul> <li> <code>id</code>           \u2013            <p>Unique identifier for the span node.</p> </li> <li> <code>span</code>           \u2013            <p><code>Span</code> backing this node.</p> </li> <li> <code>children</code>           \u2013            <p>Child nodes connected to the current span.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.agent_name","title":"<code>agent_name()</code>","text":"<p>Return the agent name associated with the span, if any.</p> <p>Returns:</p> <ul> <li> <code>Optional[str]</code>           \u2013            <p>Agent name extracted from known attributes, otherwise <code>None</code>.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.find_llm_calls","title":"<code>find_llm_calls(*, llm_call_match, agent_match, within_matching_subtree=None, within_reward=None, within_llm_call=None, existing_llm_call_response_ids=None)</code>","text":"<p>Find LLM call spans matching the supplied filters.</p> <p>Parameters:</p> <ul> <li> <code>llm_call_match</code>               (<code>str</code>)           \u2013            <p>Regular expression used to match span names that qualify as LLM calls.</p> </li> <li> <code>agent_match</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional regular expression that must match the enclosing agent span name.</p> </li> <li> <code>within_matching_subtree</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Marker propagated through recursive calls to record matching agents.</p> </li> <li> <code>within_reward</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>When <code>True</code>, suppresses LLM matches under reward spans.</p> </li> <li> <code>within_llm_call</code>               (<code>Optional[bool]</code>, default:                   <code>None</code> )           \u2013            <p>When <code>True</code>, prevents duplicate matches for nested LLM calls.</p> </li> <li> <code>existing_llm_call_response_ids</code>               (<code>Optional[set[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Known response identifiers used to deduplicate spans.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Tuple['TraceTree', str]]</code>           \u2013            <p>A list of tuples pairing the matching node with the agent subtree label that triggered the</p> </li> <li> <code>List[Tuple['TraceTree', str]]</code>           \u2013            <p>match.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.from_spans","title":"<code>from_spans(spans)</code>  <code>classmethod</code>","text":"<p>Construct a tree from a flat list of spans.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>List[Span]</code>)           \u2013            <p>Spans that collectively form a single trace segment.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'TraceTree'</code>           \u2013            <p>A <code>TraceTree</code> rooted at either the</p> </li> <li> <code>'TraceTree'</code>           \u2013            <p>discovered root span or a synthetic root when multiple roots are present.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the span list is empty or no root span can be inferred.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.is_reward_span","title":"<code>is_reward_span()</code>","text":"<p>Return whether the span explicitly encodes a reward.</p> <p>Returns:</p> <ul> <li> <code>bool</code>           \u2013            <p><code>True</code> when the span payload describes a reward, otherwise <code>False</code>.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.match_rewards","title":"<code>match_rewards(reward_match, llm_calls)</code>","text":"<p>Assign rewards to previously matched LLM calls.</p> <p>Parameters:</p> <ul> <li> <code>reward_match</code>               (<code>str</code>)           \u2013            <p>Strategy identifier from <code>RewardMatchPolicy</code>.</p> </li> <li> <code>llm_calls</code>               (<code>List['TraceTree']</code>)           \u2013            <p>Trace nodes representing LLM call spans.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Optional[float]]</code>           \u2013            <p>Mapping from span identifier to reward value or <code>None</code> when no reward is available.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.maybe_reward_dict","title":"<code>maybe_reward_dict()</code>","text":"<p>Return a reward payload if the span encodes one.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Dictionary containing reward metadata, or an empty dictionary when no reward is found.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.names_tuple","title":"<code>names_tuple()</code>","text":"<p>Return the span name alongside nested child names.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A tuple of the current span name and a list of tuples for each child containing the</p> </li> <li> <code>List[Any]</code>           \u2013            <p>child name and its descendants.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.repair_hierarchy","title":"<code>repair_hierarchy()</code>","text":"<p>Repair missing parent-child relationships introduced by mixed tracing systems.</p> <p>Some agent frameworks emit spans via multiple subsystems, which can cause LLM completion spans to float directly under the root span instead of being nested under the correct agent. The method re-parents those spans to the closest ancestor that fully envelopes the child in time.</p> <p>If we don't, when we want to select the LLM completion span with agent as filter. We will never get the correct span underneath.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.span_to_triplet","title":"<code>span_to_triplet(span, agent_name)</code>","text":"<p>Convert a span to a triplet.</p> <p>Subclass can override this method to add more fields to the triplet, such as chat messages and tool calls.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.to_json","title":"<code>to_json()</code>","text":"<p>Convert the tree node into a JSON-serialisable structure.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.to_trajectory","title":"<code>to_trajectory(llm_call_match='openai\\\\.chat\\\\.completion', agent_match=None, exclude_llm_call_in_reward=True, dedup_llm_call=True, reward_match=RewardMatchPolicy.FIRST_OCCURRENCE, final_reward=None, _skip_empty_token_spans=False)</code>","text":"<p>Convert the trace tree into a trajectory of <code>Triplet</code> items.</p> <p>Parameters:</p> <ul> <li> <code>llm_call_match</code>               (<code>str</code>, default:                   <code>'openai\\\\.chat\\\\.completion'</code> )           \u2013            <p>Regular expression for LLM call span names.</p> </li> <li> <code>agent_match</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional regular expression for agent span names.</p> </li> <li> <code>exclude_llm_call_in_reward</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, prevents searching for rewards under the LLM call subtree.</p> </li> <li> <code>dedup_llm_call</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, deduplicates spans using the LLM response identifier.</p> </li> <li> <code>reward_match</code>               (<code>RewardMatchPolicy</code>, default:                   <code>FIRST_OCCURRENCE</code> )           \u2013            <p>Reward matching policy used to associate reward spans with LLM calls.</p> </li> <li> <code>final_reward</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Optional reward appended to the final transition when provided.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Triplet]</code>           \u2013            <p>A list of <code>Triplet</code> objects ordered by call sequence.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.traverse","title":"<code>traverse()</code>","text":"<p>Traverse the tree depth first and return every node.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.TraceTree.visualize","title":"<code>visualize(filename, interested_span_match=None)</code>","text":"<p>Render the trace tree with Graphviz for debugging purposes.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>str</code>)           \u2013            <p>Base filename for the generated <code>.png</code> diagram.</p> </li> <li> <code>interested_span_match</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional regular expression used to keep only matching spans (and their ancestors) in the output.</p> </li> </ul> <p>Note</p> <p>The method requires the optional <code>graphviz</code> dependency to be available in the runtime environment.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.Transition","title":"<code>agentlightning.adapter.triplet.Transition</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single transition within a reinforcement learning trajectory.</p> <p>Attributes:</p> <ul> <li> <code>state</code>               (<code>List[int]</code>)           \u2013            <p>Token identifiers describing the model input state.</p> </li> <li> <code>action</code>               (<code>List[int]</code>)           \u2013            <p>Token identifiers representing the model output.</p> </li> <li> <code>response_id</code>               (<code>Optional[str]</code>)           \u2013            <p>Identifier of the LLM response used to deduplicate spans.</p> </li> <li> <code>agent_name</code>               (<code>str</code>)           \u2013            <p>Human-readable agent name captured from the trace.</p> </li> <li> <code>reward</code>               (<code>Optional[float]</code>)           \u2013            <p>Scalar reward associated with the transition, if available.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.adapter.triplet.RewardMatchPolicy","title":"<code>agentlightning.adapter.triplet.RewardMatchPolicy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Strategies for matching rewards to LLM call spans.</p> <p>Note</p> <p>Each reward span must expose a payload shaped like <code>{\"type\": \"reward\", \"value\": &lt;float&gt;|None}</code> as described in <code>reward.py</code>.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.RewardMatchPolicy.FIRST_OCCURRENCE","title":"<code>FIRST_OCCURRENCE = 'first_occurrence'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the first reward encountered in chronological order after the current LLM call match.</p>"},{"location":"reference/internal/#agentlightning.adapter.triplet.RewardMatchPolicy.FIRST_SIBLING","title":"<code>FIRST_SIBLING = 'first_sibling'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the first sibling in the current trace subtree as the reward unless another LLM call match is found.</p>"},{"location":"reference/internal/#agentlightning.algorithm.decorator.FunctionalAlgorithm","title":"<code>agentlightning.algorithm.decorator.FunctionalAlgorithm</code>","text":"<p>               Bases: <code>Algorithm</code>, <code>Generic[AF]</code></p> <p>An algorithm wrapper built from a callable implementation.</p> <p>Functional algorithms let you provide an ordinary function instead of subclassing <code>Algorithm</code>. The wrapper inspects the callable signature to supply optional dependencies such as the store, adapter, and LLM proxy.</p>"},{"location":"reference/internal/#agentlightning.algorithm.decorator.FunctionalAlgorithm.__init__","title":"<code>__init__(algorithm_func)</code>","text":"<pre><code>__init__(algorithm_func: AlgorithmFuncSyncLike) -&gt; None\n</code></pre><pre><code>__init__(algorithm_func: AlgorithmFuncAsyncLike) -&gt; None\n</code></pre> <p>Wrap a function that implements algorithm behaviour.</p> <p>Parameters:</p> <ul> <li> <code>algorithm_func</code>               (<code>Union[AlgorithmFuncSyncLike, AlgorithmFuncAsyncLike]</code>)           \u2013            <p>Sync or async callable implementing the algorithm contract. Arguments are detected automatically based on the function signature.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.algorithm.decorator.FunctionalAlgorithm.run","title":"<code>run(train_dataset=None, val_dataset=None)</code>","text":"<pre><code>run(\n    train_dataset: Optional[Dataset[Any]] = None,\n    val_dataset: Optional[Dataset[Any]] = None,\n) -&gt; None\n</code></pre><pre><code>run(\n    train_dataset: Optional[Dataset[Any]] = None,\n    val_dataset: Optional[Dataset[Any]] = None,\n) -&gt; Awaitable[None]\n</code></pre> <p>Execute the wrapped function with injected dependencies.</p> <p>Parameters:</p> <ul> <li> <code>train_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional training dataset passed through when the callable declares a <code>train_dataset</code> parameter.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[Any]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional validation dataset passed through when the callable declares a <code>val_dataset</code> parameter.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Union[None, Awaitable[None]]</code>           \u2013            <p>None for sync callables or an awaitable when the callable is async.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If a dataset is provided but the function signature does not accept the corresponding argument.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent","title":"<code>agentlightning.litagent.decorator.FunctionalLitAgent</code>","text":"<p>               Bases: <code>LitAgent[T]</code></p> <p>Adapter that turns plain rollout functions into <code>LitAgent</code> instances.</p> <p>The helper inspects the wrapped function to determine which resources to inject, allowing both synchronous and asynchronous callables to participate in the training loop without writing a dedicated subclass.</p>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent.__call__","title":"<code>__call__(*args, **kwargs)</code>","text":"<p>Make the agent instance callable, preserving the original function behavior.</p>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent.__init__","title":"<code>__init__(rollout_func, *, strip_proxy=True)</code>","text":"<p>Initialize the wrapper around a rollout function.</p> <p>Parameters:</p> <ul> <li> <code>rollout_func</code>               (<code>FunctionalLitAgentFunc[T]</code>)           \u2013            <p>Callable that implements the rollout. It may be synchronous or asynchronous and can optionally receive a <code>Rollout</code> alongside resources such as <code>llm</code> or <code>prompt_template</code>.</p> </li> <li> <code>strip_proxy</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, convert <code>ProxyLLM</code> inputs into <code>LLM</code> instances before calling the rollout function. Defaults to <code>True</code>.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent.rollout","title":"<code>rollout(task, resources, rollout)</code>","text":"<p>Execute a synchronous rollout using the wrapped function.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>T</code>)           \u2013            <p>Task input data.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of named resources available to the agent.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Rollout metadata provided by the runtime.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RolloutRawResult</code>           \u2013            <p>Result produced by the wrapped rollout function.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the wrapped function is asynchronous.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.litagent.decorator.FunctionalLitAgent.rollout_async","title":"<code>rollout_async(task, resources, rollout)</code>  <code>async</code>","text":"<p>Execute an asynchronous rollout using the wrapped function.</p> <p>Parameters:</p> <ul> <li> <code>task</code>               (<code>T</code>)           \u2013            <p>Task input data.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of named resources available to the agent.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>Rollout metadata provided by the runtime.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>RolloutRawResult</code>           \u2013            <p>Result produced by the wrapped rollout coroutine.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>If the wrapped function is synchronous.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.litagent.decorator.llm_rollout","title":"<code>agentlightning.litagent.decorator.llm_rollout(func=None, *, strip_proxy=True)</code>","text":"<pre><code>llm_rollout(\n    func: LlmRolloutFunc[T],\n) -&gt; FunctionalLitAgent[T]\n</code></pre><pre><code>llm_rollout(\n    *, strip_proxy: bool = True\n) -&gt; Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]\n</code></pre> <p>Create a <code>FunctionalLitAgent</code> for LLM-based rollouts.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>LlmRolloutFunc[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Callable defining the agent's behaviour. Supported signatures include:</p> <ul> <li><code>(task, llm) -&gt; result</code></li> <li><code>(task, llm, rollout) -&gt; result</code></li> <li><code>async (task, llm) -&gt; result</code></li> <li><code>async (task, llm, rollout) -&gt; result</code></li> </ul> </li> <li> <code>strip_proxy</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>When <code>True</code>, convert proxy resources into concrete <code>LLM</code> instances before calling the function. Defaults to <code>True</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T] | Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T] | Callable[[LlmRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code>@llm_rollout\ndef my_agent(task, llm):\n    return llm.endpoint\n\n@llm_rollout(strip_proxy=False)\ndef my_agent_no_strip(task, llm):\n    return llm.model\n\nresult = my_agent(task, llm)\nresult = my_agent.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/internal/#agentlightning.litagent.decorator.prompt_rollout","title":"<code>agentlightning.litagent.decorator.prompt_rollout(func=None)</code>","text":"<pre><code>prompt_rollout(\n    func: PromptRolloutFunc[T],\n) -&gt; FunctionalLitAgent[T]\n</code></pre><pre><code>prompt_rollout() -&gt; (\n    Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]\n)\n</code></pre> <p>Create a <code>FunctionalLitAgent</code> for prompt-based rollouts.</p> <p>This decorator is designed for agents that work with tunable prompt templates. It enables a workflow where algorithms manage and optimize the prompt template, while agents consume the template to perform rollouts. This is particularly useful for prompt optimization scenarios.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>PromptRolloutFunc[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Callable defining the agent's behavior. Supported signatures include:</p> <ul> <li><code>(task, prompt_template) -&gt; result</code></li> <li><code>(task, prompt_template, rollout) -&gt; result</code></li> <li><code>async (task, prompt_template) -&gt; result</code></li> <li><code>async (task, prompt_template, rollout) -&gt; result</code></li> </ul> </li> </ul> <p>Returns:</p> <ul> <li> <code>FunctionalLitAgent[T] | Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p><code>FunctionalLitAgent</code> that</p> </li> <li> <code>FunctionalLitAgent[T] | Callable[[PromptRolloutFunc[T]], FunctionalLitAgent[T]]</code>           \u2013            <p>wraps the supplied function.</p> </li> </ul> <p>Examples:</p> <pre><code>@prompt_rollout\ndef my_agent(task, prompt_template):\n    messages = prompt_template.format(task=task.input)\n    return messages\n\nresult = my_agent(task, prompt_template)\nresult = my_agent.rollout(task, resources, rollout)\n</code></pre>"},{"location":"reference/internal/#agentlightning.llm_proxy.ModelConfig","title":"<code>agentlightning.llm_proxy.ModelConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>LiteLLM model registration entry.</p> <p>This mirrors the items in LiteLLM's <code>model_list</code> section.</p> <p>Attributes:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>Logical model name exposed by the proxy.</p> </li> <li> <code>litellm_params</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Parameters passed to LiteLLM for this model (e.g., backend model id, api_base, additional options).</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningSpanExporter","title":"<code>agentlightning.llm_proxy.LightningSpanExporter</code>","text":"<p>               Bases: <code>SpanExporter</code></p> <p>Buffered OTEL span exporter with subtree flushing and training-store sink.</p> <p>Design:</p> <ul> <li>Spans are buffered until a root span's entire subtree is available.</li> <li>A private event loop on a daemon thread runs async flush logic.</li> <li>Rollout/attempt/sequence metadata is reconstructed by merging headers   from any span within a subtree.</li> </ul> <p>Thread-safety:</p> <ul> <li>Buffer access is protected by a re-entrant lock.</li> <li>Export is synchronous to the caller yet schedules an async flush on the   internal loop, then waits for completion.</li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningSpanExporter.export","title":"<code>export(spans)</code>","text":"<p>Export spans via buffered subtree flush.</p> <p>Appends spans to the internal buffer, then triggers an async flush on the private event loop. Blocks until that flush completes.</p> <p>Parameters:</p> <ul> <li> <code>spans</code>               (<code>Sequence[ReadableSpan]</code>)           \u2013            <p>Sequence of spans to export.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SpanExportResult</code> (              <code>SpanExportResult</code> )          \u2013            <p>SUCCESS on flush success, else FAILURE.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningSpanExporter.shutdown","title":"<code>shutdown()</code>","text":"<p>Shut down the exporter event loop.</p> <p>Safe to call at process exit.</p>"},{"location":"reference/internal/#agentlightning.llm_proxy.LightningOpenTelemetry","title":"<code>agentlightning.llm_proxy.LightningOpenTelemetry</code>","text":"<p>               Bases: <code>OpenTelemetry</code></p> <p>OpenTelemetry integration that exports spans to the Lightning store.</p> <p>Responsibilities:</p> <ul> <li>Ensures each request is annotated with a per-attempt sequence id so spans   are ordered deterministically even with clock skew across nodes.</li> <li>Uses <code>LightningSpanExporter</code> to persist spans for analytics and training.</li> </ul>"},{"location":"reference/internal/#agentlightning.llm_proxy.AddReturnTokenIds","title":"<code>agentlightning.llm_proxy.AddReturnTokenIds</code>","text":"<p>               Bases: <code>CustomLogger</code></p> <p>LiteLLM logger hook to request token ids from vLLM.</p> <p>This mutates the outgoing request payload to include <code>return_token_ids=True</code> for backends that support token id return (e.g., vLLM).</p> See also <p>vLLM PR #22587</p>"},{"location":"reference/internal/#agentlightning.llm_proxy.AddReturnTokenIds.async_pre_call_hook","title":"<code>async_pre_call_hook(*args, **kwargs)</code>  <code>async</code>","text":"<p>Async pre-call hook to adjust request payload.</p> <p>Parameters:</p> <ul> <li> <code>args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Positional args from LiteLLM.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword args from LiteLLM.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Union[Exception, str, Dict[str, Any]]]</code>           \u2013            <p>Either an updated payload dict or an Exception to short-circuit.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.store.base.UNSET","title":"<code>agentlightning.store.base.UNSET = _UnsetType()</code>  <code>module-attribute</code>","text":""},{"location":"reference/internal/#agentlightning.store.utils.propagate_status","title":"<code>agentlightning.store.utils.propagate_status(update_rollout_status, attempt, config)</code>  <code>async</code>","text":"<p>Propagate the status of an attempt to the rollout.</p> <p>The rollout should be made sure in a state to be outdated. Requeue the rollout if it should be retried.</p> <p>This operation is completely unlocked. The caller is responsible for locking the store.</p>"},{"location":"reference/internal/#agentlightning.tracer.agentops.LightningSpanProcessor","title":"<code>agentlightning.tracer.agentops.LightningSpanProcessor</code>","text":"<p>               Bases: <code>SpanProcessor</code></p> <p>Span processor that subclasses OpenTelemetry's <code>SpanProcessor</code> and adds support to dump traces to a <code>LightningStore</code>.</p>"},{"location":"reference/internal/#agentlightning.tracer.agentops.LightningSpanProcessor.on_end","title":"<code>on_end(span)</code>","text":"<p>Process a span when it ends.</p> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>ReadableSpan</code>)           \u2013            <p>The span that has ended.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.tracer.agentops.LightningSpanProcessor.spans","title":"<code>spans()</code>","text":"<p>Get the list of spans collected by this processor. This is useful for debugging and testing purposes.</p> <p>Returns:</p> <ul> <li> <code>List[ReadableSpan]</code>           \u2013            <p>List of ReadableSpan objects collected during tracing.</p> </li> </ul>"},{"location":"reference/internal/#deprecated-apis","title":"Deprecated APIs","text":""},{"location":"reference/internal/#agentlightning.server.AgentLightningServer","title":"<code>agentlightning.server.AgentLightningServer</code>","text":"<p>High-level controller for the legacy Agent Lightning FastAPI server.</p> <p>The controller orchestrates server start-up, task queueing, resource updates, and retrieval of client rollouts. It is primarily used by existing systems that still rely on the HTTP-based workflow.</p> <p>Deprecated</p> <p><code>AgentLightningServer</code> is part of the legacy client/server stack. Prefer the store-based runtime for new integrations.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.__init__","title":"<code>__init__(host='127.0.0.1', port=8000, task_timeout_seconds=300.0)</code>","text":"<p>Initialize the controller.</p> <p>Parameters:</p> <ul> <li> <code>host</code>               (<code>str</code>, default:                   <code>'127.0.0.1'</code> )           \u2013            <p>Hostname or IP address to bind the HTTP server to.</p> </li> <li> <code>port</code>               (<code>int</code>, default:                   <code>8000</code> )           \u2013            <p>TCP port exposed by the server.</p> </li> <li> <code>task_timeout_seconds</code>               (<code>float</code>, default:                   <code>300.0</code> )           \u2013            <p>Seconds before a claimed task is considered stale and re-queued.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.get_completed_rollout","title":"<code>get_completed_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Retrieve a specific completed rollout by identifier.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.poll_completed_rollout","title":"<code>poll_completed_rollout(rollout_id, timeout=None)</code>  <code>async</code>","text":"<p>Poll for a completed rollout until it becomes available or a timeout expires.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout to wait for.</p> </li> <li> <code>timeout</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of seconds to wait. <code>None</code> waits indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[RolloutLegacy]</code>           \u2013            <p>Retrieved rollout, or <code>None</code> when the timeout is reached without success.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.queue_task","title":"<code>queue_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Add a task to the queue for a client to process.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Return every completed rollout and clear the internal buffer.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Run the server indefinitely until <code>stop()</code> is invoked.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Start the FastAPI server in the background.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Stop the FastAPI server and wait for a graceful shutdown.</p>"},{"location":"reference/internal/#agentlightning.server.AgentLightningServer.update_resources","title":"<code>update_resources(resources)</code>  <code>async</code>","text":"<p>Publish a new resource bundle and return its generated identifier.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore","title":"<code>agentlightning.server.ServerDataStore</code>","text":"<p>Async-safe container for in-memory server state.</p> <p>The store tracks queued tasks, claimed tasks, uploaded rollouts, and the currently published resources. All interactions are guarded by asyncio locks so that the FastAPI handlers can safely run in parallel.</p> <p>Deprecated</p> <p><code>ServerDataStore</code> is part of the legacy client/server stack. Use <code>LightningStore</code> instead.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.add_task","title":"<code>add_task(sample, mode=None, resources_id=None, metadata=None)</code>  <code>async</code>","text":"<p>Enqueue a new task and return the generated rollout identifier.</p> <p>Parameters:</p> <ul> <li> <code>sample</code>               (<code>Any</code>)           \u2013            <p>Payload that describes the task input.</p> </li> <li> <code>mode</code>               (<code>Literal['train', 'val', 'test'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Phase in which the sample should be executed (<code>\"train\"</code>, <code>\"val\"</code>, or <code>\"test\"</code>).</p> </li> <li> <code>resources_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Identifier of a resource bundle that the executor should load before running the task.</p> </li> <li> <code>metadata</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional metadata forwarded to the executor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Unique rollout identifier assigned to the task.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Return the most recent resource bundle, if one exists.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.get_next_task","title":"<code>get_next_task()</code>  <code>async</code>","text":"<p>Retrieve the next task from the queue without blocking.</p> <p>Returns:</p> <ul> <li> <code>Optional[Task]</code>           \u2013            <p>Next <code>Task</code> ready to execute, or <code>None</code></p> </li> <li> <code>Optional[Task]</code>           \u2013            <p>when the queue is empty.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.get_processing_tasks","title":"<code>get_processing_tasks()</code>","text":"<p>Return a copy of currently processing tasks for timeout checking.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Retrieve a specific resource bundle by identifier.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>str</code>)           \u2013            <p>Identifier that was previously published to the store.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>Matching <code>ResourcesUpdate</code></p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>instance, or <code>None</code> when the identifier is unknown.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.requeue_task","title":"<code>requeue_task(task)</code>  <code>async</code>","text":"<p>Requeue a task that timed out while being processed.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.retrieve_completed_rollouts","title":"<code>retrieve_completed_rollouts()</code>  <code>async</code>","text":"<p>Return all completed rollouts and clear the internal buffer.</p>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.retrieve_rollout","title":"<code>retrieve_rollout(rollout_id)</code>  <code>async</code>","text":"<p>Retrieve and remove a stored rollout by identifier.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout to fetch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[RolloutLegacy]</code>           \u2013            <p>Stored <code>RolloutLegacy</code>, or <code>None</code></p> </li> <li> <code>Optional[RolloutLegacy]</code>           \u2013            <p>when the identifier is unknown.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.store_rollout","title":"<code>store_rollout(rollout)</code>  <code>async</code>","text":"<p>Persist a completed rollout for later inspection.</p> <p>Parameters:</p> <ul> <li> <code>rollout</code>               (<code>RolloutLegacy</code>)           \u2013            <p>Rollout returned by a client.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.server.ServerDataStore.update_resources","title":"<code>update_resources(update)</code>  <code>async</code>","text":"<p>Persist a new resource bundle and mark it as the latest version.</p> <p>Parameters:</p> <ul> <li> <code>update</code>               (<code>ResourcesUpdate</code>)           \u2013            <p>Resource payload received from a client.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient","title":"<code>agentlightning.client.AgentLightningClient</code>","text":"<p>Client wrapper for the legacy version-aware Agent Lightning server.</p> <p>The client exposes synchronous and asynchronous helpers for polling tasks, retrieving resource bundles, and submitting rollouts. It also maintains a simple in-memory cache keyed by the server-provided resource identifier to avoid redundant network requests.</p> <p>Deprecated</p> <p><code>AgentLightningClient</code> is part of the legacy client/server stack. New code should rely on the store-based APIs implemented in <code>agentlightning.store</code>.</p> <p>Attributes:</p> <ul> <li> <code>endpoint</code>           \u2013            <p>Base URL of the Agent Lightning server.</p> </li> <li> <code>poll_interval</code>           \u2013            <p>Delay in seconds between polling attempts when no task is available.</p> </li> <li> <code>timeout</code>           \u2013            <p>Timeout in seconds applied to HTTP requests.</p> </li> <li> <code>task_count</code>           \u2013            <p>Number of tasks claimed during the lifetime of this client.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.__init__","title":"<code>__init__(endpoint, poll_interval=5.0, timeout=10.0)</code>","text":"<p>Initialize the client.</p> <p>Parameters:</p> <ul> <li> <code>endpoint</code>               (<code>str</code>)           \u2013            <p>Root URL of the Agent Lightning server.</p> </li> <li> <code>poll_interval</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Seconds to wait between polling attempts.</p> </li> <li> <code>timeout</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>Seconds before a request to the server is considered timed out.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.get_latest_resources","title":"<code>get_latest_resources()</code>","text":"<p>Fetch the most recent resource bundle advertised by the server.</p> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code> for the</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>newest version, or <code>None</code> when unavailable.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.get_latest_resources_async","title":"<code>get_latest_resources_async()</code>  <code>async</code>","text":"<p>Fetch the most recent resource bundle advertised by the server.</p> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code> for the</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>newest version, or <code>None</code> when unavailable.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.get_resources_by_id","title":"<code>get_resources_by_id(resource_id)</code>","text":"<p>Fetch a specific resource bundle by identifier.</p> <p>Parameters:</p> <ul> <li> <code>resource_id</code>               (<code>str</code>)           \u2013            <p>Identifier sourced from the task metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>Cached or freshly downloaded</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code>, or</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>None</code> when the server returns an error.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.get_resources_by_id_async","title":"<code>get_resources_by_id_async(resource_id)</code>  <code>async</code>","text":"<p>Fetch a specific resource bundle by identifier.</p> <p>Parameters:</p> <ul> <li> <code>resource_id</code>               (<code>str</code>)           \u2013            <p>Identifier sourced from the task metadata.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>Cached or freshly downloaded</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>ResourcesUpdate</code>, or</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p><code>None</code> when the server returns an error.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.poll_next_task","title":"<code>poll_next_task()</code>","text":"<p>Poll the server synchronously until a task becomes available.</p> <p>Returns:</p> <ul> <li> <code>Optional[Task]</code>           \u2013            <p>The next <code>Task</code> available for execution, or</p> </li> <li> <code>Optional[Task]</code>           \u2013            <p><code>None</code> if polling fails.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.poll_next_task_async","title":"<code>poll_next_task_async()</code>  <code>async</code>","text":"<p>Poll the server asynchronously until a task becomes available.</p> <p>Returns:</p> <ul> <li> <code>Optional[Task]</code>           \u2013            <p>The next <code>Task</code> exposed by the server,</p> </li> <li> <code>Optional[Task]</code>           \u2013            <p>or <code>None</code> if polling fails.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.post_rollout","title":"<code>post_rollout(rollout)</code>","text":"<p>Submit a completed rollout back to the server.</p> <p>Parameters:</p> <ul> <li> <code>rollout</code>               (<code>RolloutLegacy</code>)           \u2013            <p>Legacy rollout payload produced by the executor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Parsed JSON response returned by the server, or <code>None</code> when the request fails.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.AgentLightningClient.post_rollout_async","title":"<code>post_rollout_async(rollout)</code>  <code>async</code>","text":"<p>Submit a completed rollout back to the server.</p> <p>Parameters:</p> <ul> <li> <code>rollout</code>               (<code>RolloutLegacy</code>)           \u2013            <p>Legacy rollout payload produced by the executor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Dict[str, Any]]</code>           \u2013            <p>Parsed JSON response returned by the server, or <code>None</code> when the request fails.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.DevTaskLoader","title":"<code>agentlightning.client.DevTaskLoader</code>","text":"<p>               Bases: <code>AgentLightningClient</code></p> <p>In-memory task loader used for development and integration tests.</p> <p>The loader mimics the behavior of the legacy HTTP server by storing tasks and resources locally. Polling methods simply iterate over the provided collection, allowing rapid iteration without provisioning any external infrastructure.</p> <p>Deprecated</p> <p><code>DevTaskLoader</code> is a compatibility shim. Prefer <code>Trainer.dev</code> for new code.</p>"},{"location":"reference/internal/#agentlightning.client.DevTaskLoader.rollouts","title":"<code>rollouts</code>  <code>property</code>","text":"<p>Return the rollouts posted back to the loader during development runs.</p>"},{"location":"reference/internal/#agentlightning.client.DevTaskLoader.__init__","title":"<code>__init__(tasks, resources, **kwargs)</code>","text":"<p>Initialize the loader with predefined tasks and resources.</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>               (<code>Union[List[TaskInput], List[Task]]</code>)           \u2013            <p>Sequence of task inputs or preconstructed tasks that will be served in order.</p> </li> <li> <code>resources</code>               (<code>Union[NamedResources, ResourcesUpdate]</code>)           \u2013            <p>Static resources returned for any <code>resources_id</code> query.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments forwarded to the parent client.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no tasks are provided or both <code>Task</code> and <code>TaskInput</code> instances are mixed.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.client.DevTaskLoader.poll_next_task","title":"<code>poll_next_task()</code>","text":"<p>Return the next task from the local queue.</p> <p>If <code>TaskInput</code> instances were provided, they are converted into <code>Task</code> objects on the fly. Otherwise, the preconstructed tasks are returned in sequence.</p> <p>Returns:</p> <ul> <li> <code>Optional[Task]</code>           \u2013            <p>Next task to execute.</p> </li> </ul>"},{"location":"reference/internal/#agentlightning.Task","title":"<code>agentlightning.Task</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rollout request served to client agents.</p> <p>Deprecated</p> <p>The legacy HTTP client/server stack still uses this model. Prefer <code>LightningStore</code> APIs for new workflows.</p>"},{"location":"reference/internal/#agentlightning.TaskInput","title":"<code>agentlightning.TaskInput = Any</code>  <code>module-attribute</code>","text":"<p>Task input type. Accepts arbitrary payloads.</p>"},{"location":"reference/internal/#agentlightning.TaskIfAny","title":"<code>agentlightning.TaskIfAny</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A task or indication that no task is available.</p> <p>Deprecated</p> <p>Use <code>LightningStore</code> APIs for new workflows.</p>"},{"location":"reference/internal/#agentlightning.TaskIfAny.is_available","title":"<code>is_available</code>  <code>instance-attribute</code>","text":"<p>Indication that a task is available.</p>"},{"location":"reference/internal/#agentlightning.RolloutRawResultLegacy","title":"<code>agentlightning.RolloutRawResultLegacy = Union[None, float, List[Triplet], List[Dict[str, Any]], List[ReadableSpan], RolloutLegacy]</code>  <code>module-attribute</code>","text":"<p>Legacy rollout result type.</p> <p>Deprecated</p> <p>Use <code>RolloutRawResult</code> instead.</p>"},{"location":"reference/internal/#agentlightning.RolloutLegacy","title":"<code>agentlightning.RolloutLegacy</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Legacy reporting payload exchanged with the deprecated HTTP server.</p> <p>Deprecated</p> <p>Use <code>Rollout</code> instead.</p>"},{"location":"reference/runner/","title":"Runner-side References","text":"<p>Note</p> <p>This reference covers APIs that are designed to be used at \"Runner Side\".</p>"},{"location":"reference/runner/#runners","title":"Runners","text":""},{"location":"reference/runner/#agentlightning.LitAgentRunner","title":"<code>agentlightning.LitAgentRunner</code>","text":"<p>               Bases: <code>Runner[T_task]</code></p> <p>Execute <code>LitAgent</code> tasks with tracing support.</p> <p>This runner manages the complete lifecycle of agent rollout execution, including task polling, resource management, tracing, and hooks. It supports both continuous iteration over tasks from the store and single-step execution.</p> <p>Attributes:</p> <ul> <li> <code>worker_id</code>               (<code>Optional[int]</code>)           \u2013            <p>Identifier for the active worker process, if any.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.tracer","title":"<code>tracer</code>  <code>property</code>","text":"<p>Get the tracer instance.</p> <p>Returns:</p> <ul> <li> <code>Tracer</code>           \u2013            <p>The Tracer instance used by this runner.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.__init__","title":"<code>__init__(tracer, max_rollouts=None, poll_interval=5.0)</code>","text":"<p>Initialize the agent runner.</p> <p>Parameters:</p> <ul> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p><code>Tracer</code> used for rollout spans.</p> </li> <li> <code>max_rollouts</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional cap on iterations processed by <code>iter</code>.</p> </li> <li> <code>poll_interval</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>Seconds to wait between store polls when no work is available.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.get_agent","title":"<code>get_agent()</code>","text":"<p>Get the agent instance.</p> <p>Returns:</p> <ul> <li> <code>LitAgent[T_task]</code>           \u2013            <p>The LitAgent instance managed by this runner.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the agent has not been initialized via <code>init</code>.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.get_store","title":"<code>get_store()</code>","text":"<p>Get the store instance.</p> <p>Returns:</p> <ul> <li> <code>LightningStore</code>           \u2013            <p>The LightningStore instance for this worker.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the store has not been initialized via <code>init_worker</code>.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.get_worker_id","title":"<code>get_worker_id()</code>","text":"<p>Get the formatted worker ID string.</p> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>A formatted string like \"Worker-0\" if initialized, or \"Worker-Unknown\"</p> </li> <li> <code>str</code>           \u2013            <p>if the worker ID has not been set.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.init","title":"<code>init(agent, *, hooks=None, **kwargs)</code>","text":"<p>Initialize the runner with the agent.</p> <p>This sets up the agent-runner relationship, registers hooks, and initializes the tracer.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_task]</code>)           \u2013            <p><code>LitAgent</code> instance executed by the runner.</p> </li> <li> <code>hooks</code>               (<code>Optional[Sequence[Hook]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence of <code>Hook</code> callbacks invoked around tracing and rollout boundaries.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional initialization arguments (currently unused).</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.init_worker","title":"<code>init_worker(worker_id, store, **kwargs)</code>","text":"<p>Initialize the runner for each worker with worker_id and store.</p> <p>This method is called once per worker in a distributed setup to provide the worker with its ID and store connection.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Unique identifier for this worker process.</p> </li> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p><code>LightningStore</code> used for task coordination and persistence.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional worker-specific initialization arguments (currently unused).</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.iter","title":"<code>iter(*, event=None)</code>  <code>async</code>","text":"<p>Run the runner, continuously iterating over tasks in the store.</p> <p>This method polls the store for new rollouts and executes them until:</p> <ul> <li>The event is set (if provided)</li> <li>The max_rollouts limit is reached (if configured)</li> <li>No more tasks are available</li> </ul> <p>All exceptions during rollout execution are caught and logged but not propagated, allowing the runner to continue processing subsequent tasks.</p> <p>Parameters:</p> <ul> <li> <code>event</code>               (<code>Optional[ExecutionEvent]</code>, default:                   <code>None</code> )           \u2013            <p>Optional ExecutionEvent object to signal the runner to stop. The runner will check this event periodically and stop gracefully when set.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.step","title":"<code>step(input, *, resources=None, mode=None, event=None)</code>  <code>async</code>","text":"<p>Execute a single task directly, bypassing the task queue.</p> <p>This method creates a new rollout for the given input and executes it immediately. Unlike <code>iter()</code>, exceptions are propagated to the caller.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>T_task</code>)           \u2013            <p>The task input to be processed by the agent.</p> </li> <li> <code>resources</code>               (<code>Optional[NamedResources]</code>, default:                   <code>None</code> )           \u2013            <p>Optional named resources to be used for this specific task. If provided, a new resources entry will be created in the store. If not provided, the latest resources from the store will be used.</p> </li> <li> <code>mode</code>               (<code>Optional[RolloutMode]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout mode (\"train\" or \"validation\"). If not provided, the agent's default mode will be used.</p> </li> <li> <code>event</code>               (<code>Optional[ExecutionEvent]</code>, default:                   <code>None</code> )           \u2013            <p>Optional ExecutionEvent object to signal interruption (currently unused but included for interface consistency).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Rollout</code>           \u2013            <p>The completed rollout.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exception</code>             \u2013            <p>Any exception that occurs during rollout execution will be re-raised to the caller.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.teardown","title":"<code>teardown(*args, **kwargs)</code>","text":"<p>Teardown the runner and clean up all resources.</p> <p>This method resets all internal state including the agent, store, hooks, and worker ID, and calls the tracer's teardown method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional teardown arguments (currently unused).</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional teardown keyword arguments (currently unused).</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.LitAgentRunner.teardown_worker","title":"<code>teardown_worker(worker_id, *args, **kwargs)</code>","text":"<p>Teardown the runner for a specific worker.</p> <p>This method cleans up worker-specific resources and resets the worker ID.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Unique identifier of the worker being torn down.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Additional teardown arguments (currently unused).</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional teardown keyword arguments (currently unused).</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner","title":"<code>agentlightning.Runner</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code>, <code>Generic[T_task]</code></p> <p>Abstract base class for long-running agent executors.</p> <p>Runner implementations coordinate <code>LitAgent</code> instances, acquire work from a <code>LightningStore</code>, and emit <code>Rollout</code> objects. Subclasses decide how to schedule work (polling, streaming, etc.) while this base class provides a minimal lifecycle contract.</p>"},{"location":"reference/runner/#agentlightning.Runner.init","title":"<code>init(agent, **kwargs)</code>","text":"<p>Prepare the runner to execute tasks for <code>agent</code>.</p> <p>This method is called only once during the setup for all workers, not for each worker.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_task]</code>)           \u2013            <p>Agent instance providing task-specific logic.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Optional runner-specific configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must supply the initialization routine.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.init_worker","title":"<code>init_worker(worker_id, store, **kwargs)</code>","text":"<p>Configure worker-local state before processing tasks.</p> <p>This method is called for each worker during the setup.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Unique identifier for this worker process or thread.</p> </li> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>Shared <code>LightningStore</code> backing task coordination.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Optional worker-specific configuration.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must prepare per-worker resources.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.iter","title":"<code>iter(*, event=None)</code>  <code>async</code>","text":"<p>Run the runner, continuously iterating over tasks in the store.</p> <p>This method runs in a loop, polling the store for new tasks and executing them until interrupted by the event or when no more tasks are available.</p> <p>Parameters:</p> <ul> <li> <code>event</code>               (<code>Optional[ExecutionEvent]</code>, default:                   <code>None</code> )           \u2013            <p>Cooperative stop signal. When set, the runner should complete the current unit of work and exit the loop.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses provide the iteration behavior.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.run","title":"<code>run(*args, **kwargs)</code>","text":"<p>Deprecated synchronous entry point.</p> <p>Use <code>iter()</code> or <code>step()</code> instead.</p> <p>Raises:</p> <ul> <li> <code>RuntimeError</code>             \u2013            <p>Always raised to direct callers to iter() or step().</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.run_context","title":"<code>run_context(*, agent, store, hooks=None, worker_id=None)</code>","text":"<p>Initialize and tear down a runner within a simple context manager.</p> <p>The helper is primarily intended for debugging runner implementations outside of a full <code>Trainer</code> stack.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_task]</code>)           \u2013            <p>Agent executed by this runner.</p> </li> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>Backing <code>LightningStore</code>. If you don't have one, you can easily create one with <code>InMemoryLightningStore</code>.</p> </li> <li> <code>hooks</code>               (<code>Optional[Sequence[Hook]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence of hooks recognised by the runner. Not all runners support hooks.</p> </li> <li> <code>worker_id</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Override the worker identifier used during setup. Defaults to <code>0</code>.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.step","title":"<code>step(input, *, resources=None, mode=None, event=None)</code>  <code>async</code>","text":"<p>Execute a single task with the given input.</p> <p>This method provides fine-grained control for executing individual tasks directly, bypassing the store's task queue.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>T_task</code>)           \u2013            <p>Task payload consumed by the agent.</p> </li> <li> <code>resources</code>               (<code>Optional[NamedResources]</code>, default:                   <code>None</code> )           \u2013            <p>Optional named resources scoped to this invocation.</p> </li> <li> <code>mode</code>               (<code>Optional[RolloutMode]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout mode such as <code>\"train\"</code> or <code>\"eval\"</code>.</p> </li> <li> <code>event</code>               (<code>Optional[ExecutionEvent]</code>, default:                   <code>None</code> )           \u2013            <p>Cooperative stop signal for long-running tasks.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Rollout</code>           \u2013            <p>Completed rollout produced by the agent.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses provide the execution behavior.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.teardown","title":"<code>teardown(*args, **kwargs)</code>","text":"<p>Release resources acquired during <code>init()</code>.</p> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the shutdown routine.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Runner.teardown_worker","title":"<code>teardown_worker(worker_id, *args, **kwargs)</code>","text":"<p>Release per-worker resources allocated by <code>init_worker()</code>.</p> <p>Parameters:</p> <ul> <li> <code>worker_id</code>               (<code>int</code>)           \u2013            <p>Identifier of the worker being torn down.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the shutdown routine.</p> </li> </ul>"},{"location":"reference/runner/#tracer","title":"Tracer","text":""},{"location":"reference/runner/#agentlightning.AgentOpsTracer","title":"<code>agentlightning.AgentOpsTracer</code>","text":"<p>               Bases: <code>Tracer</code></p> <p>Traces agent execution using AgentOps.</p> <p>This tracer provides functionality to capture execution details using the AgentOps library. It manages the AgentOps client initialization, server setup, and integration with the OpenTelemetry tracing ecosystem.</p> <p>Attributes:</p> <ul> <li> <code>agentops_managed</code>           \u2013            <p>Whether to automatically manage <code>agentops</code>.               When set to true, tracer calls <code>agentops.init()</code>               automatically and launches an agentops endpoint locally.               If not, you are responsible for calling and using it               before using the tracer.</p> </li> <li> <code>instrument_managed</code>           \u2013            <p>Whether to automatically manage instrumentation.                 When set to false, you will manage the instrumentation                 yourself and the tracer might not work as expected.</p> </li> <li> <code>daemon</code>           \u2013            <p>Whether the AgentOps server runs as a daemon process.     Only applicable if <code>agentops_managed</code> is True.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.AgentOpsTracer.get_langchain_handler","title":"<code>get_langchain_handler(tags=None)</code>","text":"<p>Get the Langchain callback handler for integrating with Langchain.</p> <p>Parameters:</p> <ul> <li> <code>tags</code>               (<code>List[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of tags to apply to the Langchain callback handler.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>LangchainCallbackHandler</code>           \u2013            <p>An instance of the Langchain callback handler.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.AgentOpsTracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> <ul> <li> <code>List[ReadableSpan]</code>           \u2013            <p>A list of OpenTelemetry <code>ReadableSpan</code> objects.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.AgentOpsTracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>  <code>async</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the tracing context.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>Optional store to add the spans to.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout ID to add the spans to.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt ID to add the spans to.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>AsyncGenerator[LightningSpanProcessor, None]</code>           \u2013            <p>The <code>LightningSpanProcessor</code> instance to collect spans.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.OtelTracer","title":"<code>agentlightning.OtelTracer</code>","text":"<p>               Bases: <code>Tracer</code></p> <p>Tracer that provides a basic OpenTelemetry tracer provider.</p> <p>You should be able to collect agent-lightning signals like rewards with this tracer, but no other function instrumentations like <code>openai.chat.completion</code>.</p>"},{"location":"reference/runner/#agentlightning.OtelTracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> <ul> <li> <code>List[ReadableSpan]</code>           \u2013            <p>A list of OpenTelemetry <code>ReadableSpan</code> objects.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.OtelTracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>  <code>async</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional name for the tracing context.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>Optional store to add the spans to.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout ID to add the spans to.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt ID to add the spans to.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>AsyncGenerator[LightningSpanProcessor, None]</code>           \u2013            <p>The LightningSpanProcessor instance to collect spans.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer","title":"<code>agentlightning.Tracer</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>An abstract base class for tracers.</p> <p>This class defines a standard interface for tracing code execution, capturing the resulting spans, and providing them for analysis. It is designed to be backend-agnostic, allowing for different implementations (e.g., for AgentOps, OpenTelemetry, Docker, etc.).</p> <p>The primary interaction pattern is through the <code>trace_context</code> context manager, which ensures that traces are properly started and captured, even in the case of exceptions.</p> <p>A typical workflow:</p> <pre><code>tracer = YourTracerImplementation()\n\ntry:\n    async with tracer.trace_context(name=\"my_traced_task\"):\n        # ... code to be traced ...\n        await run_my_agent_logic()\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n\n# Retrieve the trace data after the context block\nspans: list[ReadableSpan] = tracer.get_last_trace()\n\n# Process the trace data\nif trace_tree:\n    rl_triplets = TracerTraceToTriplet().adapt(spans)\n    # ... do something with the triplets\n</code></pre>"},{"location":"reference/runner/#agentlightning.Tracer.get_langchain_handler","title":"<code>get_langchain_handler()</code>","text":"<p>Get a handler to install in langchain agent callback.</p> <p>Agents are expected to use this handler in their agents to enable tracing.</p>"},{"location":"reference/runner/#agentlightning.Tracer.get_last_trace","title":"<code>get_last_trace()</code>","text":"<p>Retrieves the raw list of captured spans from the most recent trace.</p> <p>Returns:</p> <ul> <li> <code>List[ReadableSpan]</code>           \u2013            <p>A list of OpenTelemetry <code>ReadableSpan</code> objects.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.trace_context","title":"<code>trace_context(name=None, *, store=None, rollout_id=None, attempt_id=None)</code>","text":"<p>Starts a new tracing context. This should be used as a context manager.</p> <p>The implementation should handle the setup and teardown of the tracing for the enclosed code block. It must ensure that any spans generated within the <code>with</code> block are collected and made available via <code>get_last_trace</code>.</p> <p>If a store is provided, the spans will be added to the store when tracing.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The name for the root span of this trace context.</p> </li> <li> <code>store</code>               (<code>Optional[LightningStore]</code>, default:                   <code>None</code> )           \u2013            <p>The store to add the spans to.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The rollout ID to add the spans to.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>The attempt ID to add the spans to.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.trace_run","title":"<code>trace_run(func, *args, **kwargs)</code>","text":"<p>A convenience wrapper to trace the execution of a single synchronous function.</p> <p>Deprecated in favor of customizing Runners.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable[..., Any]</code>)           \u2013            <p>The synchronous function to execute and trace.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Positional arguments to pass to the function.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to the function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The return value of the function.</p> </li> </ul>"},{"location":"reference/runner/#agentlightning.Tracer.trace_run_async","title":"<code>trace_run_async(func, *args, **kwargs)</code>  <code>async</code>","text":"<p>A convenience wrapper to trace the execution of a single asynchronous function.</p> <p>Deprecated in favor of customizing Runners.</p> <p>Parameters:</p> <ul> <li> <code>func</code>               (<code>Callable[..., Awaitable[Any]]</code>)           \u2013            <p>The asynchronous function to execute and trace.</p> </li> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Positional arguments to pass to the function.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to the function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>The return value of the function.</p> </li> </ul>"},{"location":"reference/store/","title":"Store References","text":""},{"location":"reference/store/#agentlightning.LightningStore","title":"<code>agentlightning.LightningStore</code>","text":"<p>Contract for the persistent control-plane that coordinates training rollouts.</p> <p>A <code>LightningStore</code> mediates every interaction between algorithms and runners:</p> <ul> <li>Rollout lifecycle: accept new rollouts, queue them for execution, create attempts,   and drive the rollout status machine (<code>\"queuing\"</code> \u2192 <code>\"preparing\"</code> \u2192 <code>\"running\"</code> \u2192   <code>{\"succeeded\",\"failed\",\"cancelled\"}</code> or <code>\"requeuing\"</code> when a retry is justified).</li> <li>Attempt tracking: record each execution attempt, including progress heartbeats,   retry sequencing, and terminal states such as <code>\"timeout\"</code> or <code>\"unresponsive\"</code>.</li> <li>Span ingest: capture structured telemetry emitted by runners (either as native   <code>Span</code> objects or as <code>opentelemetry.sdk.trace.ReadableSpan</code>   instances) so that algorithms can reconstruct trajectories and rewards.</li> <li>Resource versioning: manage immutable snapshots of named resources   (prompt templates, model checkpoints, proxy endpoints, \u2026) and expose a single   \"latest\" snapshot that runners can fetch just after claiming work.</li> </ul> <p>Implementations must provide thread-safe/async-safe semantics: each coroutine should appear atomic to callers even when multiple algorithms or runners call the API concurrently. Unless stated otherwise, missing identifiers should result in a <code>ValueError</code>.</p>"},{"location":"reference/store/#agentlightning.LightningStore.add_otel_span","title":"<code>add_otel_span(rollout_id, attempt_id, readable_span, sequence_id=None)</code>  <code>async</code>","text":"<p>Convert and persist an OpenTelemetry span for a particular attempt.</p> <p>Implementations must transform the <code>readable_span</code> into a <code>Span</code> (typically via <code>Span.from_opentelemetry()</code>), assign a strictly increasing <code>sequence_id</code> when one is not provided, and persist it using the same semantics as <code>add_span()</code>.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout that produced the span.</p> </li> <li> <code>attempt_id</code>               (<code>str</code>)           \u2013            <p>Attempt identifier the span belongs to.</p> </li> <li> <code>readable_span</code>               (<code>ReadableSpan</code>)           \u2013            <p>OpenTelemetry span in SDK form.</p> </li> <li> <code>sequence_id</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional explicit ordering hint. When omitted, call <code>get_next_span_sequence_id()</code> automatically.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Span</code>           \u2013            <p>The stored span record.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement span persistence.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout or attempt is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.add_resources","title":"<code>add_resources(resources)</code>  <code>async</code>","text":"<p>Persist a new immutable snapshot of named resources and mark it as latest.</p> <p>Implementations must assign a fresh <code>resources_id</code> and ensure subsequent calls to <code>get_latest_resources()</code> return the snapshot produced here.</p> <p>Parameters:</p> <ul> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Mapping of resource names to their serialized payloads.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ResourcesUpdate</code>           \u2013            <p>The stored <code>ResourcesUpdate</code> including its generated id.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement resource persistence.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.add_span","title":"<code>add_span(span)</code>  <code>async</code>","text":"<p>Persist a pre-constructed span emitted during rollout execution.</p> <p>The provided <code>Span</code> must already contain the <code>rollout_id</code>, <code>attempt_id</code>, and <code>sequence_id</code>. Implementations must:</p> <ul> <li>Verify that both rollout and attempt exist.</li> <li>Ensure span ordering remains strictly increasing per attempt (rejecting or keeping duplicates).</li> <li>Treat the span arrival as a heartbeat: update the attempt's <code>last_heartbeat_time</code>   and transition both attempt and rollout to <code>\"running\"</code> if they were still   <code>\"preparing\"</code> or <code>\"requeuing\"</code>.</li> </ul> <p>Parameters:</p> <ul> <li> <code>span</code>               (<code>Span</code>)           \u2013            <p>Fully populated span to persist.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Span</code>           \u2013            <p>The stored span record (implementations may return a copy).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement span persistence.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the referenced rollout or attempt is missing.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.dequeue_rollout","title":"<code>dequeue_rollout()</code>  <code>async</code>","text":"<p>Claim the oldest queued rollout and transition it to <code>preparing</code>.</p> <p>This function do not block.</p> <p>Retrieval must be FIFO across rollouts that remain in <code>queuing</code> or <code>requeuing</code> state. When a rollout is claimed, implementations must:</p> <ul> <li>Transition its status to <code>\"preparing\"</code>.</li> <li>Create a new attempt with <code>status=\"preparing\"</code> and <code>sequence_id</code> equal to   the number of attempts already registered for the rollout plus one.</li> <li>Return an <code>AttemptedRollout</code> snapshot so the   runner knows both rollout metadata and the attempt identifier.</li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[AttemptedRollout]</code>           \u2013            <p>The next attempt to execute, or <code>None</code> when no eligible rollouts are queued.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement queue retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.enqueue_rollout","title":"<code>enqueue_rollout(input, mode=None, resources_id=None, config=None, metadata=None)</code>  <code>async</code>","text":"<p>Persist a rollout in <code>queuing</code> state so runners can claim it later.</p> <p>Note</p> <p>Different from <code>start_rollout()</code>, this method is called when the caller only wants to submit work for later scheduling.</p> <p>Implementations must generate a unique <code>rollout_id</code>, stamp <code>start_time</code> with the current time, default <code>config</code> to a fresh <code>RolloutConfig</code>, and insert the rollout at the tail of the scheduling queue. No attempt is created yet.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>TaskInput</code>)           \u2013            <p>Arbitrary task payload supplied by an algorithm.</p> </li> <li> <code>mode</code>               (<code>Literal['train', 'val', 'test'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional semantic mode indicator (<code>\"train\"</code>, <code>\"val\"</code>, <code>\"test\"</code>).</p> </li> <li> <code>resources_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Resource snapshot used when a runner eventually executes the rollout.</p> </li> <li> <code>config</code>               (<code>RolloutConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Fine-grained retry/timeout parameters to persist with the rollout.</p> </li> <li> <code>metadata</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Free-form metadata stored verbatim with the rollout record.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Rollout</code>           \u2013            <p>The stored <code>Rollout</code> in <code>queuing</code> status.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must persist the rollout.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations should raise when <code>resources_id</code> does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_latest_attempt","title":"<code>get_latest_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Fetch the attempt with the highest <code>sequence_id</code> for <code>rollout_id</code>.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier to inspect.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Attempt]</code>           \u2013            <p>The most recent attempt or <code>None</code> when no attempts exist yet.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Fetch the latest resource snapshot marked as the global default.</p> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>The current latest <code>ResourcesUpdate</code>, or <code>None</code> when</p> </li> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>no resources have been registered yet.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_next_span_sequence_id","title":"<code>get_next_span_sequence_id(rollout_id, attempt_id)</code>  <code>async</code>","text":"<p>Allocate the next strictly increasing sequence number used to order spans.</p> <p>Implementations must retain counters so repeated calls return <code>1, 2, ...</code> without gaps unless spans were explicitly inserted with a custom <code>sequence_id</code>. The counter may be scoped per rollout or per attempt, but the sequence must be strictly increasing for spans emitted by the specified attempt so traces remain totally ordered.</p> <p>See Distributed Tracing for detailed motivations.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout emitting spans.</p> </li> <li> <code>attempt_id</code>               (<code>str</code>)           \u2013            <p>Attempt identifier for the upcoming span.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>The next integer sequence identifier, unique within the attempt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must provide the allocator.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout or attempt does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Return a specific named resource snapshot by identifier.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the snapshot.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>The stored <code>ResourcesUpdate</code>, or <code>None</code> when missing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.get_rollout_by_id","title":"<code>get_rollout_by_id(rollout_id)</code>  <code>async</code>","text":"<p>Fetch a rollout by identifier without mutating its state.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Rollout]</code>           \u2013            <p>The rollout when found, otherwise <code>None</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement retrieval.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.query_attempts","title":"<code>query_attempts(rollout_id)</code>  <code>async</code>","text":"<p>Return every attempt ever created for <code>rollout_id</code> in ascending sequence order.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout being inspected.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Attempt]</code>           \u2013            <p>Attempts sorted by <code>sequence_id</code> (oldest first). Returns an empty list when none exist.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the query.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.query_rollouts","title":"<code>query_rollouts(*, status=None, rollout_ids=None)</code>  <code>async</code>","text":"<p>Retrieve rollouts filtered by status and/or explicit identifiers.</p> <p>Parameters:</p> <ul> <li> <code>status</code>               (<code>Optional[Sequence[RolloutStatus]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional whitelist of <code>RolloutStatus</code> values.</p> </li> <li> <code>rollout_ids</code>               (<code>Optional[Sequence[str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional whitelist of rollout identifiers to include.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Rollout]</code>           \u2013            <p>A list of matching rollouts. Ordering is backend-defined but must be deterministic.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the query.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.query_spans","title":"<code>query_spans(rollout_id, attempt_id=None)</code>  <code>async</code>","text":"<p>Return the stored spans for a rollout, optionally scoped to one attempt.</p> <p>Spans must be returned in ascending <code>sequence_id</code> order. Implementations may raise a <code>RuntimeError</code> when spans were evicted or expired.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout being inspected.</p> </li> <li> <code>attempt_id</code>               (<code>str | Literal['latest'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Attempt identifier to filter by. Pass <code>\"latest\"</code> to retrieve only the most recent attempt, or <code>None</code> to return all spans across attempts.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Span]</code>           \u2013            <p>An ordered list of spans (possibly empty).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement the query.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout or attempt is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.start_attempt","title":"<code>start_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Create a manual retry attempt for an existing rollout.</p> <p>This is typically invoked by runners that wish to retry outside of the normal queue flow (for example in an online RL setup). Implementations must validate that the rollout exists, allocate a fresh <code>attempt_id</code>, increment the <code>sequence_id</code> monotonically, stamp the new attempt with <code>status=\"preparing\"</code>, and return an up-to-date <code>AttemptedRollout</code>.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Unique identifier of the rollout receiving a new attempt.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AttemptedRollout</code>           \u2013            <p>The rollout paired with its newly-created attempt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement attempt creation.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when <code>rollout_id</code> is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.start_rollout","title":"<code>start_rollout(input, mode=None, resources_id=None, config=None, metadata=None)</code>  <code>async</code>","text":"<p>Register a rollout and immediately create its first attempt.</p> <p>Note</p> <p>Use <code>enqueue_rollout()</code> when the caller only wants to submit work for later scheduling.</p> <p>The rollout must be persisted with <code>status=\"preparing\"</code> and an initial attempt with <code>sequence_id == 1</code> so the caller can begin execution without visiting the public queue. Implementations are expected to:</p> <ol> <li>Generate a unique <code>rollout_id</code> and <code>attempt_id</code>.</li> <li>Record <code>start_time</code> for both rollout and attempt based on the current clock.</li> <li>Copy <code>config</code> and <code>metadata</code> so later mutations do not leak shared references.</li> <li>Resolve <code>resources_id</code> to the latest resource snapshot when <code>None</code> is supplied.</li> </ol> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>TaskInput</code>)           \u2013            <p>Arbitrary task payload supplied by an algorithm.</p> </li> <li> <code>mode</code>               (<code>Literal['train', 'val', 'test'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional semantic mode for downstream analytics (<code>\"train\"</code>, <code>\"val\"</code>, <code>\"test\"</code>).</p> </li> <li> <code>resources_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Concrete resource snapshot to execute against; defaults to the latest stored snapshot.</p> </li> <li> <code>config</code>               (<code>RolloutConfig | None</code>, default:                   <code>None</code> )           \u2013            <p>Rollout retry/timeout policy. Should default to a fresh <code>RolloutConfig</code>.</p> </li> <li> <code>metadata</code>               (<code>Dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Free-form metadata persisted verbatim with the rollout.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AttemptedRollout</code>           \u2013            <p>The fully-populated <code>AttemptedRollout</code> including</p> </li> <li> <code>AttemptedRollout</code>           \u2013            <p>the just-created attempt.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must provide durable storage for the rollout.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations should raise when <code>resources_id</code> does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.update_attempt","title":"<code>update_attempt(rollout_id, attempt_id, status=UNSET, worker_id=UNSET, last_heartbeat_time=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update attempt bookkeeping such as status, worker ownership, and heartbeats.</p> <p>When <code>attempt_id</code> is <code>\"latest\"</code> the update must target the attempt with the highest <code>sequence_id</code>; otherwise it must target the specific attempt. Implementations should propagate status changes to the rollout (for example via <code>propagate_status()</code>) once the latest attempt transitions to a terminal state.</p> <p>Similar to <code>update_rollout()</code>, parameters also default to the sentinel <code>UNSET</code>.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout whose attempt will be updated.</p> </li> <li> <code>attempt_id</code>               (<code>str | Literal['latest']</code>)           \u2013            <p>Attempt identifier or <code>\"latest\"</code> as a convenience.</p> </li> <li> <code>status</code>               (<code>AttemptStatus | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement attempt status. Terminal statuses must set <code>end_time</code>.</p> </li> <li> <code>worker_id</code>               (<code>str | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Identifier for the worker currently processing the attempt.</p> </li> <li> <code>last_heartbeat_time</code>               (<code>float | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Wall-clock timestamp (seconds) of the latest heartbeat/span.</p> </li> <li> <code>metadata</code>               (<code>Optional[Dict[str, Any]] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement metadata dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Attempt</code>           \u2013            <p>The updated attempt record.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement mutation logic.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout or attempt is unknown.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.update_resources","title":"<code>update_resources(resources_id, resources)</code>  <code>async</code>","text":"<p>Overwrite or extend an existing resource snapshot and mark it as latest.</p> <p>This API is typically used by algorithms that maintain mutable resources (e.g., model checkpoints) under a stable identifier.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the snapshot to replace.</p> </li> <li> <code>resources</code>               (<code>NamedResources</code>)           \u2013            <p>Updated mapping of resource names to payloads.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ResourcesUpdate</code>           \u2013            <p>The persisted <code>ResourcesUpdate</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement resource persistence.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when <code>resources_id</code> does not exist.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.update_rollout","title":"<code>update_rollout(rollout_id, input=UNSET, mode=UNSET, resources_id=UNSET, status=UNSET, config=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update rollout metadata and, when provided, drive status transitions.</p> <p>Parameters default to the sentinel <code>UNSET</code> to distinguish omitted fields from explicit <code>None</code> assignments. Implementations must:</p> <ul> <li>Validate the rollout exists before mutating it.</li> <li>Replace each property when a concrete value (including <code>None</code>) is supplied.</li> <li>When the status switches into a terminal state, set <code>end_time</code> and signal any waiters.</li> <li>When the status re-enters a queueing state, ensure the rollout is enqueued exactly once.</li> </ul> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the rollout to update.</p> </li> <li> <code>input</code>               (<code>TaskInput | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement task payload; pass <code>None</code> to explicitly clear the input.</p> </li> <li> <code>mode</code>               (<code>Optional[Literal['train', 'val', 'test']] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement rollout mode.</p> </li> <li> <code>resources_id</code>               (<code>Optional[str] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement resources snapshot reference.</p> </li> <li> <code>status</code>               (<code>RolloutStatus | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Target rollout status.</p> </li> <li> <code>config</code>               (<code>RolloutConfig | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement retry/timeout configuration.</p> </li> <li> <code>metadata</code>               (<code>Optional[Dict[str, Any]] | Unset</code>, default:                   <code>UNSET</code> )           \u2013            <p>Replacement metadata dictionary.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Rollout</code>           \u2013            <p>The updated rollout record.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement mutation logic.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when the rollout is unknown or the update is invalid.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStore.wait_for_rollouts","title":"<code>wait_for_rollouts(*, rollout_ids, timeout=None)</code>  <code>async</code>","text":"<p>Block until the targeted rollouts reach a terminal status or the timeout expires.</p> <p>Terminal statuses are <code>\"succeeded\"</code>, <code>\"failed\"</code>, and <code>\"cancelled\"</code>. When the timeout elapses, implementations should return the subset of rollouts that are already terminal and omit the rest.</p> <p>Warning</p> <p>It's dangerous and might be event-loop blocking to call this function with a long timeout. It's a good idea to poll for the method to check if new completed rollouts can coming. Be careful in implementing the sleep logic to avoid busy-waiting.</p> <p>Parameters:</p> <ul> <li> <code>rollout_ids</code>               (<code>List[str]</code>)           \u2013            <p>Identifiers of rollouts to watch.</p> </li> <li> <code>timeout</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time in seconds to wait. <code>None</code> waits indefinitely.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Rollout]</code>           \u2013            <p>Rollouts that finished before the deadline, in arbitrary order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must implement waiting semantics.</p> </li> <li> <code>ValueError</code>             \u2013            <p>Implementations must raise when a rollout identifier is unknown.</p> </li> </ul>"},{"location":"reference/store/#store-implementations","title":"Store Implementations","text":""},{"location":"reference/store/#agentlightning.InMemoryLightningStore","title":"<code>agentlightning.InMemoryLightningStore</code>","text":"<p>               Bases: <code>LightningStore</code></p> <p>In-memory implementation of LightningStore using Python data structures. Thread-safe and async-compatible but data is not persistent.</p> <p>The methods in this class should generally not call each other, especially those that are locked.</p> <p>Parameters:</p> <ul> <li> <code>eviction_memory_threshold</code>               (<code>float | int | None</code>, default:                   <code>None</code> )           \u2013            <p>The threshold for evicting spans in bytes. By default, it's 70% of the total VRAM available.</p> </li> <li> <code>safe_memory_threshold</code>               (<code>float | int | None</code>, default:                   <code>None</code> )           \u2013            <p>The threshold for safe memory usage in bytes. By default, it's 80% of the eviction threshold.</p> </li> <li> <code>span_size_estimator</code>               (<code>Callable[[Span], int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A function to estimate the size of a span in bytes. By default, it's a simple size estimator that uses sys.getsizeof.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.add_otel_span","title":"<code>add_otel_span(rollout_id, attempt_id, readable_span, sequence_id=None)</code>  <code>async</code>","text":"<p>Add an opentelemetry span to the store.</p> <p>See <code>LightningStore.add_otel_span()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.add_resources","title":"<code>add_resources(resources)</code>  <code>async</code>","text":"<p>Stores a new version of named resources and sets it as the latest.</p> <p>See <code>LightningStore.add_resources()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.add_span","title":"<code>add_span(span)</code>  <code>async</code>","text":"<p>Persist a pre-converted span.</p> <p>See <code>LightningStore.add_span()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.dequeue_rollout","title":"<code>dequeue_rollout()</code>  <code>async</code>","text":"<p>Retrieves the next task from the queue without blocking. Returns <code>None</code> if the queue is empty.</p> <p>Will set the rollout status to preparing and create a new attempt.</p> <p>See <code>LightningStore.dequeue_rollout()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.enqueue_rollout","title":"<code>enqueue_rollout(input, mode=None, resources_id=None, config=None, metadata=None)</code>  <code>async</code>","text":"<p>Adds a new task to the queue with specific metadata and returns the rollout.</p> <p>See <code>LightningStore.enqueue_rollout()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.get_latest_attempt","title":"<code>get_latest_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves the latest attempt for a given rollout ID.</p> <p>See <code>LightningStore.get_latest_attempt()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Retrieves the latest version of named resources.</p> <p>See <code>LightningStore.get_latest_resources()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.get_next_span_sequence_id","title":"<code>get_next_span_sequence_id(rollout_id, attempt_id)</code>  <code>async</code>","text":"<p>Get the next span sequence ID for a given rollout and attempt. The number is strictly increasing for each rollout. The store will not issue the same sequence ID twice.</p> <p>See <code>LightningStore.get_next_span_sequence_id()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Retrieves a specific version of named resources by its ID.</p> <p>See <code>LightningStore.get_resources_by_id()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.get_rollout_by_id","title":"<code>get_rollout_by_id(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves a specific rollout by its ID.</p> <p>See <code>LightningStore.get_rollout_by_id()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.query_attempts","title":"<code>query_attempts(rollout_id)</code>  <code>async</code>","text":"<p>Retrieves all attempts associated with a specific rollout ID. Returns an empty list if no attempts are found.</p> <p>See <code>LightningStore.query_attempts()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.query_rollouts","title":"<code>query_rollouts(*, status=None, rollout_ids=None)</code>  <code>async</code>","text":"<p>Retrieves rollouts filtered by their status and rollout ids. If no status is provided, returns all rollouts.</p> <p>See <code>LightningStore.query_rollouts()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.query_spans","title":"<code>query_spans(rollout_id, attempt_id=None)</code>  <code>async</code>","text":"<p>Query and retrieve all spans associated with a specific rollout ID. Returns an empty list if no spans are found.</p> <p>See <code>LightningStore.query_spans()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.start_attempt","title":"<code>start_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Creates a new attempt for a given rollout ID and return the attempt details.</p> <p>See <code>LightningStore.start_attempt()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.start_rollout","title":"<code>start_rollout(input, mode=None, resources_id=None, config=None, metadata=None)</code>  <code>async</code>","text":"<p>Notify the store that I'm about to run a rollout.</p> <p>See <code>LightningStore.start_rollout()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.update_attempt","title":"<code>update_attempt(rollout_id, attempt_id, status=UNSET, worker_id=UNSET, last_heartbeat_time=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update a specific or latest attempt for a given rollout.</p> <p>See <code>LightningStore.update_attempt()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.update_resources","title":"<code>update_resources(resources_id, resources)</code>  <code>async</code>","text":"<p>Safely stores a new version of named resources and sets it as the latest.</p> <p>See <code>LightningStore.update_resources()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.update_rollout","title":"<code>update_rollout(rollout_id, input=UNSET, mode=UNSET, resources_id=UNSET, status=UNSET, config=UNSET, metadata=UNSET)</code>  <code>async</code>","text":"<p>Update the rollout status and related metadata.</p> <p>See <code>LightningStore.update_rollout()</code> for semantics.</p>"},{"location":"reference/store/#agentlightning.InMemoryLightningStore.wait_for_rollouts","title":"<code>wait_for_rollouts(*, rollout_ids, timeout=None)</code>  <code>async</code>","text":"<p>Wait for specified rollouts to complete with a timeout. Returns the completed rollouts, potentially incomplete if timeout is reached.</p> <p>This method does not change the state of the store.</p> <p>See <code>LightningStore.wait_for_rollouts()</code> for semantics.</p>"},{"location":"reference/store/#client-server-and-thread-safe-wrappers","title":"Client-Server and Thread-safe Wrappers","text":""},{"location":"reference/store/#agentlightning.LightningStoreServer","title":"<code>agentlightning.LightningStoreServer</code>","text":"<p>               Bases: <code>LightningStore</code></p> <p>Server wrapper that exposes a LightningStore via HTTP API. Delegates all operations to an underlying store implementation.</p> <p>Healthcheck and watchdog relies on the underlying store.</p> <p><code>agl store</code> is a convenient CLI to start a store server.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Control pickling to prevent server state from being sent to subprocesses.</p> <p>When LightningStoreServer is pickled (e.g., passed to a subprocess), we only serialize the underlying store and connection details. The FastAPI app and uvicorn server are excluded as they should not be transferred between processes.</p> <p>The subprocess should create its own server instance if needed.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore from pickle by reconstructing only the essential attributes.</p> <p>Note: This creates a new server instance without FastAPI/uvicorn initialized. Call init() pattern or create a new LightningStoreServer if you need a fully functional server in the subprocess.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.run_forever","title":"<code>run_forever()</code>  <code>async</code>","text":"<p>Runs the FastAPI server indefinitely.</p> <p>You need to call this method in the same process as the server was created in.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.start","title":"<code>start()</code>  <code>async</code>","text":"<p>Starts the FastAPI server in the background.</p> <p>You need to call this method in the same process as the server was created in.</p>"},{"location":"reference/store/#agentlightning.LightningStoreServer.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Gracefully stops the running FastAPI server.</p> <p>You need to call this method in the same process as the server was created in.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient","title":"<code>agentlightning.LightningStoreClient</code>","text":"<p>               Bases: <code>LightningStore</code></p> <p>HTTP client that talks to a remote LightningStoreServer.</p> <p>Parameters:</p> <ul> <li> <code>server_address</code>               (<code>str</code>)           \u2013            <p>The address of the LightningStoreServer to connect to.</p> </li> <li> <code>retry_delays</code>               (<code>Sequence[float]</code>, default:                   <code>(1.0, 2.0, 5.0)</code> )           \u2013            <p>Backoff schedule (seconds) used when the initial request fails for a non-application reason. Each entry is a retry attempt.</p> </li> <li> <code>health_retry_delays</code>               (<code>Sequence[float]</code>, default:                   <code>(0.1, 0.2, 0.5)</code> )           \u2013            <p>Delays between /health probes while waiting for the server to come back.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStoreClient.__getstate__","title":"<code>__getstate__()</code>","text":"<p>When LightningStoreClient is pickled (e.g., passed to a subprocess), we only serialize the server address and retry configurations. The ClientSessions are excluded as they should not be transferred between processes.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Restore from pickle by reconstructing only the essential attributes.</p> <p>Replicating <code>__init__</code> logic to create another client instance in the subprocess.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.close","title":"<code>close()</code>  <code>async</code>","text":"<p>Close the HTTP session.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.dequeue_rollout","title":"<code>dequeue_rollout()</code>  <code>async</code>","text":"<p>Dequeue a rollout from the server queue.</p> <p>Returns:</p> <ul> <li> <code>Optional[AttemptedRollout]</code>           \u2013            <p>AttemptedRollout if a rollout is available, None if queue is empty.</p> </li> </ul> Note <p>This method does NOT retry on failures. If any exception occurs (network error, server error, etc.), it logs the error and returns None immediately.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.get_latest_attempt","title":"<code>get_latest_attempt(rollout_id)</code>  <code>async</code>","text":"<p>Get the latest attempt for a rollout.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>ID of the rollout to query.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Attempt]</code>           \u2013            <p>Attempt if found, None if not found or if all retries are exhausted.</p> </li> </ul> Note <p>This method retries on transient failures (network errors, 5xx status codes). If all retries fail, it logs the error and returns None instead of raising an exception.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.get_latest_resources","title":"<code>get_latest_resources()</code>  <code>async</code>","text":"<p>Get the latest resources.</p> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>ResourcesUpdate if found, None if not found or if all retries are exhausted.</p> </li> </ul> Note <p>This method retries on transient failures (network errors, 5xx status codes). If all retries fail, it logs the error and returns None instead of raising an exception.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.get_resources_by_id","title":"<code>get_resources_by_id(resources_id)</code>  <code>async</code>","text":"<p>Get resources by their ID.</p> <p>Parameters:</p> <ul> <li> <code>resources_id</code>               (<code>str</code>)           \u2013            <p>ID of the resources to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[ResourcesUpdate]</code>           \u2013            <p>ResourcesUpdate if found, None if not found or if all retries are exhausted.</p> </li> </ul> Note <p>This method retries on transient failures (network errors, 5xx status codes). If all retries fail, it logs the error and returns None instead of raising an exception.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.get_rollout_by_id","title":"<code>get_rollout_by_id(rollout_id)</code>  <code>async</code>","text":"<p>Get a rollout by its ID.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>ID of the rollout to retrieve.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Rollout]</code>           \u2013            <p>Rollout if found, None if not found or if all retries are exhausted.</p> </li> </ul> Note <p>This method retries on transient failures (network errors, 5xx status codes). If all retries fail, it logs the error and returns None instead of raising an exception.</p>"},{"location":"reference/store/#agentlightning.LightningStoreClient.wait_for_rollouts","title":"<code>wait_for_rollouts(*, rollout_ids, timeout=None)</code>  <code>async</code>","text":"<p>Wait for rollouts to complete.</p> <p>Parameters:</p> <ul> <li> <code>rollout_ids</code>               (<code>List[str]</code>)           \u2013            <p>List of rollout IDs to wait for.</p> </li> <li> <code>timeout</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Timeout in seconds. If not None, the method will raise a ValueError if the timeout is greater than 0.1 seconds.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Rollout]</code>           \u2013            <p>List of rollouts that are completed.</p> </li> </ul>"},{"location":"reference/store/#agentlightning.LightningStoreThreaded","title":"<code>agentlightning.LightningStoreThreaded</code>","text":"<p>               Bases: <code>LightningStore</code></p> <p>Facade that delegates all store operations to a underlying store instance.</p> <p>The operations are guaranteed to be thread-safe. Make sure the threaded stores are instantiated before initializing the threads.</p>"},{"location":"reference/trainer/","title":"Agent-lightning Trainer","text":""},{"location":"reference/trainer/#agentlightning.Trainer","title":"<code>agentlightning.Trainer</code>","text":"<p>               Bases: <code>TrainerLegacy</code></p> <p>High-level orchestration layer that wires Algorithm &lt;-&gt; Runner &lt;-&gt; Store.</p> <p>A <code>Trainer</code> packages the moving parts of Agent-Lightning's training loop into a single entry point:</p> <ul> <li>Algorithm lifecycle: Instantiates or accepts an <code>Algorithm</code>,   attaches the current <code>LightningStore</code>, adapter, and   initial resources, then executes the algorithm role inside the configured execution strategy.</li> <li>Runner fleet: Spawns one or more <code>Runner</code> instances (defaulting   to <code>LitAgentRunner</code>) that hydrate a <code>LitAgent</code>,   claim rollouts, stream spans, and respect graceful termination signals from the execution strategy.</li> <li>Execution strategy: Delegates process management to an   <code>ExecutionStrategy</code> (shared memory, client/server, etc.),   so advanced users can swap orchestration backends without changing trainer code.</li> <li>Telemetry plumbing: Ensures tracers, adapters, and optional <code>LLMProxy</code>   are wired into both algorithm and runners so telemetry flows back into the store.</li> </ul> <p>The trainer exposes two convenience entry points: <code>fit()</code> for full training and <code>dev()</code> for fast, reproducible dry-runs. See the Train the First Agent and Write the First Algorithm tutorials for the broader context.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.adapter","title":"<code>adapter = self._make_adapter(adapter_spec)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>TraceAdapter</code> to export data consumble by algorithms from traces.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.algorithm","title":"<code>algorithm = self._make_algorithm(algorithm)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>Algorithm</code> to use for training.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.daemon","title":"<code>daemon = daemon</code>  <code>instance-attribute</code>","text":"<p>Whether worker processes should be daemons. Daemon processes are terminated automatically when the main process exits. Deprecated. Only have effect with <code>fit_v0</code>.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.hooks","title":"<code>hooks = self._normalize_hooks(hooks)</code>  <code>instance-attribute</code>","text":"<p>A sequence of <code>Hook</code> instances to be called at various lifecycle stages (e.g., <code>on_trace_start</code>, <code>on_trace_end</code>, <code>on_rollout_start</code>, <code>on_rollout_end</code>).</p>"},{"location":"reference/trainer/#agentlightning.Trainer.initial_resources","title":"<code>initial_resources = initial_resources</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>NamedResources</code> to use for bootstrapping the fit/dev process.</p> <p>The resources will be handed over to the algorithm. Note that not all algorithms support seeding resources.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.llm_proxy","title":"<code>llm_proxy = self._make_llm_proxy(llm_proxy, store=(self.store))</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>LLMProxy</code> to use for intercepting the LLM calls. If not provided, algorithm may create one on its own.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.max_rollouts","title":"<code>max_rollouts = max_rollouts</code>  <code>instance-attribute</code>","text":"<p>Maximum number of rollouts to process per runner. If None, workers run until no more rollouts are available.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.max_tasks","title":"<code>max_tasks = max_tasks if max_tasks is not None else max_rollouts</code>  <code>instance-attribute</code>","text":"<p>Maximum number of tasks to process per runner. Deprecated in favor of <code>max_rollouts</code>.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.n_runners","title":"<code>n_runners = n_runners</code>  <code>instance-attribute</code>","text":"<p>Number of agent runners to run in parallel.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.n_workers","title":"<code>n_workers = n_runners</code>  <code>instance-attribute</code>","text":"<p>Number of agent workers to run in parallel. Deprecated in favor of <code>n_runners</code>.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.port","title":"<code>port = port</code>  <code>instance-attribute</code>","text":"<p>Port forwarded to <code>ClientServerExecutionStrategy</code>.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.runner","title":"<code>runner = self._make_runner(runner)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>Runner</code> to use for running the agent.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.store","title":"<code>store = self._make_store(store)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>LightningStore</code> to use for storing tasks and traces.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.strategy","title":"<code>strategy = self._make_strategy(strategy, n_runners=(self.n_runners), port=port)</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>ExecutionStrategy</code> to use for spawning the algorithm and runners.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.tracer","title":"<code>tracer = self._make_tracer(tracer)</code>  <code>instance-attribute</code>","text":"<p>A tracer instance, or a string pointing to the class full name or a dictionary with a 'type' key that specifies the class full name and other initialization parameters. If None, a default <code>AgentOpsTracer</code> will be created with the current settings.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.triplet_exporter","title":"<code>triplet_exporter = self.adapter</code>  <code>instance-attribute</code>","text":"<p>An instance of <code>TracerTraceToTriplet</code> to export triplets from traces, or a dictionary with the initialization parameters for the exporter. Deprecated. Use <code>adapter</code> instead.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.__init__","title":"<code>__init__(*, dev=False, n_runners=None, max_rollouts=None, initial_resources=None, tracer=None, adapter=None, store=None, runner=None, strategy=None, port=None, algorithm=None, llm_proxy=None, n_workers=None, max_tasks=None, daemon=True, triplet_exporter=None, hooks=None)</code>","text":"<p>Configure the trainer and resolve user-provided component specifications.</p> <p>Each keyword accepts either a concrete instance, a class, a callable factory, a registry string, or a lightweight configuration dictionary (see <code>build_component()</code>).</p> <p>When <code>port</code> is provided it is forwarded to <code>ClientServerExecutionStrategy</code> instances constructed (or supplied) for the trainer.</p>"},{"location":"reference/trainer/#agentlightning.Trainer.dev","title":"<code>dev(agent, train_dataset=None, *, val_dataset=None)</code>","text":"<p>Exercise the infrastructure using a fast, synchronous algorithm.</p> <p><code>Trainer.dev</code> mirrors <code>fit()</code> but insists on an <code>Algorithm</code> subtype that also derives from <code>FastAlgorithm</code>. This keeps the loop responsive for debugging while still touching the same store, runners, hooks, and tracer plumbing.</p> <p>If no algorithm is provided, a default <code>Baseline</code> algorithm will be used.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_co]</code>)           \u2013            <p><code>LitAgent</code> implementation to execute.</p> </li> <li> <code>train_dataset</code>               (<code>Optional[Dataset[T_co]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional iterable passed to the algorithm.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[T_co]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional iterable passed to the algorithm.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the configured algorithm does not inherit from <code>FastAlgorithm</code>.</p> </li> </ul>"},{"location":"reference/trainer/#agentlightning.Trainer.fit","title":"<code>fit(agent, train_dataset=None, *, val_dataset=None)</code>","text":"<p>Execute the full algorithm/runner training loop.</p> <p><code>Trainer.fit</code> packages the algorithm and runner bundles, then hands them to the active <code>ExecutionStrategy</code>. The strategy rarely returns until:</p> <ul> <li>The algorithm exhausts the dataset(s) and stops enqueuing rollouts.</li> <li><code>max_rollouts</code> causes individual runners to exit.</li> <li>An exception or interrupt cancels the shared <code>ExecutionEvent</code>.</li> </ul> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[T_co]</code>)           \u2013            <p><code>LitAgent</code> implementation executed by runners.</p> </li> <li> <code>train_dataset</code>               (<code>Optional[Dataset[T_co]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional iterable of rollout inputs consumed by the algorithm.</p> </li> <li> <code>val_dataset</code>               (<code>Optional[Dataset[T_co]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional iterable consumed by validation passes.</p> </li> </ul>"},{"location":"reference/trainer/#agentlightning.build_component","title":"<code>agentlightning.build_component(spec, *, expected_type, spec_name, default_factory=None, allow_none=False, optional_defaults=None, dict_requires_type=True, dict_default_cls=None, type_error_fmt=None, invalid_spec_error_fmt=None, registry=None)</code>","text":"<pre><code>build_component(\n    spec: Union[\n        T,\n        str,\n        Dict[str, Any],\n        type[T],\n        Callable[[], T],\n        None,\n    ],\n    *,\n    expected_type: type[T],\n    spec_name: str,\n    default_factory: Callable[[], T],\n    allow_none: bool = ...,\n    optional_defaults: Optional[OptionalDefaults] = ...,\n    dict_requires_type: bool = ...,\n    dict_default_cls: type[T] | None = ...,\n    type_error_fmt: str | None = ...,\n    invalid_spec_error_fmt: str | None = ...,\n    registry: Optional[Dict[str, str]] = ...\n) -&gt; T\n</code></pre><pre><code>build_component(\n    spec: Union[\n        T,\n        str,\n        Dict[str, Any],\n        type[T],\n        Callable[[], T],\n        None,\n    ],\n    *,\n    expected_type: type[T],\n    spec_name: str,\n    default_factory: None = ...,\n    allow_none: bool,\n    optional_defaults: Optional[OptionalDefaults] = ...,\n    dict_requires_type: bool = ...,\n    dict_default_cls: type[T] | None = ...,\n    type_error_fmt: str | None = ...,\n    invalid_spec_error_fmt: str | None = ...,\n    registry: Optional[Dict[str, str]] = ...\n) -&gt; T | None\n</code></pre><pre><code>build_component(\n    spec: Union[\n        T,\n        str,\n        Dict[str, Any],\n        type[T],\n        Callable[[], T],\n        None,\n    ],\n    *,\n    expected_type: type[T],\n    spec_name: str,\n    default_factory: None = ...,\n    allow_none: bool = ...,\n    optional_defaults: Optional[OptionalDefaults] = ...,\n    dict_requires_type: bool = ...,\n    dict_default_cls: type[T] | None = ...,\n    type_error_fmt: str | None = ...,\n    invalid_spec_error_fmt: str | None = ...,\n    registry: Optional[Dict[str, str]] = ...\n) -&gt; T | None\n</code></pre> <p>Build and return a component instance from a flexible specification.</p> <p>This function provides a flexible way to create component instances from various input formats including direct instances, class types, factory functions, import paths, or configuration dictionaries.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>Union[T, str, Dict[str, Any], type[T], Callable[[], T], None]</code>)           \u2013            <p>The component specification. Can be: - An instance of expected_type (returned as-is) - A string import path (e.g., 'module.Class') or registry key - A dict with 'type' key (import path or registry key) and constructor kwargs - A class type (will be instantiated) - A factory function (will be called) - None (uses default_factory or returns None if allow_none=True)</p> </li> <li> <code>expected_type</code>               (<code>type[T]</code>)           \u2013            <p>The type that the resulting instance must be or inherit from.</p> </li> <li> <code>spec_name</code>               (<code>str</code>)           \u2013            <p>Descriptive name for the spec, used in error messages.</p> </li> <li> <code>default_factory</code>               (<code>Callable[[], T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional factory function called when spec is None.</p> </li> <li> <code>allow_none</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, allows None to be returned when spec is None and no default_factory is provided.</p> </li> <li> <code>optional_defaults</code>               (<code>Optional[OptionalDefaults]</code>, default:                   <code>None</code> )           \u2013            <p>Dict mapping parameter names to default values or factory functions that will be injected if the constructor accepts them.</p> </li> <li> <code>dict_requires_type</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, dict specs must include a 'type' key.</p> </li> <li> <code>dict_default_cls</code>               (<code>type[T] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default class to use for dict specs without a 'type' key (only used when dict_requires_type=False).</p> </li> <li> <code>type_error_fmt</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Custom format string for type validation errors. Should include {type_name} and {expected_type} placeholders.</p> </li> <li> <code>invalid_spec_error_fmt</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Custom format string for invalid spec type errors. Should include {actual_type} and {expected_type} placeholders.</p> </li> <li> <code>registry</code>               (<code>Optional[Dict[str, str]]</code>, default:                   <code>None</code> )           \u2013            <p>Optional mapping of short names to fully qualified import paths. When provided, string specs or dict 'type'/'name' entries are first resolved through this registry before attempting to import.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T | None</code>           \u2013            <p>An instance of expected_type, or None if allow_none=True and spec is None</p> </li> <li> <code>T | None</code>           \u2013            <p>without a default_factory.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If the instantiated object is not an instance of expected_type.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If spec is None and neither default_factory nor allow_none is set, or if spec type is invalid, or if dict spec is invalid.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Direct instance\n&gt;&gt;&gt; optimizer = build_component(AdamW(), expected_type=Optimizer, spec_name='optimizer')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # String import path\n&gt;&gt;&gt; optimizer = build_component('torch.optim.AdamW', expected_type=Optimizer, spec_name='optimizer')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Dict with type and kwargs\n&gt;&gt;&gt; spec = {'type': 'torch.optim.AdamW', 'lr': 0.001}\n&gt;&gt;&gt; optimizer = build_component(spec, expected_type=Optimizer, spec_name='optimizer')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Class type\n&gt;&gt;&gt; optimizer = build_component(AdamW, expected_type=Optimizer, spec_name='optimizer')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Factory function\n&gt;&gt;&gt; optimizer = build_component(lambda: AdamW(lr=0.001), expected_type=Optimizer,\n...                            spec_name='optimizer')\n</code></pre>"},{"location":"reference/trainer/#execution-strategy","title":"Execution Strategy","text":""},{"location":"reference/trainer/#agentlightning.ExecutionStrategy","title":"<code>agentlightning.ExecutionStrategy</code>","text":"<p>Coordinate algorithm and runner bundles within a single process abstraction.</p> <p>Strategies decide how many worker bundles to launch, whether to communicate through shared memory or an HTTP boundary, and how to react to shutdown signals. They intentionally avoid inspecting the bundle internals; instead, each bundle remains responsible for its own scheduling semantics.</p> <p>Note</p> <p>Implementations must honor the execute() contract by propagating <code>KeyboardInterrupt</code> and ensuring resources are released when an error occurs on either side of the algorithm/runner pair.</p>"},{"location":"reference/trainer/#agentlightning.ExecutionStrategy.execute","title":"<code>execute(algorithm, runner, store)</code>","text":"<p>Run the provided bundles using the configured orchestration model.</p> <p>Parameters:</p> <ul> <li> <code>algorithm</code>               (<code>AlgorithmBundle</code>)           \u2013            <p>Callable bundle responsible for algorithm execution.</p> </li> <li> <code>runner</code>               (<code>RunnerBundle</code>)           \u2013            <p>Callable bundle for runner workers.</p> </li> <li> <code>store</code>               (<code>LightningStore</code>)           \u2013            <p>Concrete <code>LightningStore</code> shared across bundles.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>NotImplementedError</code>             \u2013            <p>Subclasses must provide the orchestration implementation.</p> </li> </ul>"},{"location":"reference/trainer/#agentlightning.ClientServerExecutionStrategy","title":"<code>agentlightning.ClientServerExecutionStrategy</code>","text":"<p>               Bases: <code>ExecutionStrategy</code></p> <p>Run algorithm and runner bundles as separate processes over HTTP.</p> <p>Execution Roles:</p> <ul> <li><code>\"algorithm\"</code>: Start <code>LightningStoreServer</code>   in-process and execute the algorithm bundle against it.</li> <li><code>\"runner\"</code>: Connect to an existing server with   <code>LightningStoreClient</code> and run the   runner bundle locally (spawning multiple processes when requested).</li> <li><code>\"both\"</code>: Spawn runner processes first, then execute the algorithm and   server on the same machine. This mode orchestrates the full loop locally.</li> </ul> <p>When <code>role == \"both\"</code> you may choose which side runs on the main process via <code>main_process</code>. The runner-on-main option is limited to <code>n_runners == 1</code> because each additional runner requires its own event loop and process.</p> <p>Warning</p> <p>When <code>main_process == \"runner\"</code> the algorithm and HTTP server execute in a child process. Store mutations remain isolated inside that process, so the original store instance passed to execute() is not updated.</p> <p>Abort Model (four-step escalation):</p> <ol> <li>Cooperative stop. Every bundle receives a shared    <code>MultiprocessingEvent</code> (<code>stop_evt</code>).    Any failure flips the event so peers can exit cleanly. Ctrl+C on the main    process also sets the flag.</li> <li>KeyboardInterrupt synthesis. Remaining subprocesses receive <code>SIGINT</code> to    trigger <code>KeyboardInterrupt</code> handlers.</li> <li>Termination. Stubborn processes are asked to <code>terminate()</code>    (<code>SIGTERM</code> on POSIX).</li> <li>Kill. As a last resort <code>kill()</code> is invoked (<code>SIGKILL</code> on POSIX).</li> </ol> <p>This mirrors the semantics implemented in <code>SharedMemoryExecutionStrategy</code> but adapts them to multiple processes and the HTTP client/server boundary.</p>"},{"location":"reference/trainer/#agentlightning.ClientServerExecutionStrategy.__init__","title":"<code>__init__(role=None, server_host=None, server_port=None, n_runners=1, graceful_timeout=5.0, terminate_timeout=5.0, main_process='algorithm', managed_store=None)</code>","text":"<p>Configure the strategy.</p> <p>Parameters:</p> <ul> <li> <code>role</code>               (<code>Literal['algorithm', 'runner', 'both'] | None</code>, default:                   <code>None</code> )           \u2013            <p>Which side(s) to run in this process. When omitted, the <code>AGL_CURRENT_ROLE</code> environment variable is used.</p> </li> <li> <code>server_host</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Interface the HTTP server binds to when running the algorithm bundle locally. Defaults to <code>AGL_SERVER_HOST</code> or <code>\"localhost\"</code> if unset.</p> </li> <li> <code>server_port</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Port for the HTTP server in \"algorithm\"/\"both\" modes. Defaults to <code>AGL_SERVER_PORT</code> or <code>4747</code> if unset.</p> </li> <li> <code>n_runners</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of runner processes to spawn in \"runner\"/\"both\".</p> </li> <li> <code>graceful_timeout</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>How long to wait (seconds) after setting the stop event before escalating to signals.</p> </li> <li> <code>terminate_timeout</code>               (<code>float</code>, default:                   <code>5.0</code> )           \u2013            <p>How long to wait between escalation steps beyond the cooperative phase (re-used for SIGINT, terminate, and kill).</p> </li> <li> <code>main_process</code>               (<code>Literal['algorithm', 'runner']</code>, default:                   <code>'algorithm'</code> )           \u2013            <p>Which bundle runs on the main process when <code>role == \"both\"</code>. <code>\"runner\"</code> requires <code>n_runners == 1</code> and is primarily intended for debugging.</p> </li> <li> <code>managed_store</code>               (<code>bool | None</code>, default:                   <code>None</code> )           \u2013            <p>When <code>True</code> (default) the strategy constructs LightningStore client/server wrappers automatically. When <code>False</code> the provided <code>store</code> is passed directly to the bundles, allowing callers to manage store wrappers manually.</p> </li> </ul>"},{"location":"reference/trainer/#agentlightning.SharedMemoryExecutionStrategy","title":"<code>agentlightning.SharedMemoryExecutionStrategy</code>","text":"<p>               Bases: <code>ExecutionStrategy</code></p> <p>Execute bundles in a single process with cooperative worker threads.</p> <p>Stop Model:</p> <ul> <li>All bundles share one <code>ThreadingEvent</code>   named <code>stop_evt</code>.</li> <li>Only the main thread receives <code>KeyboardInterrupt</code>. When Ctrl+C occurs we   set <code>stop_evt</code>.</li> <li>Any exception raised inside a bundle sets <code>stop_evt</code> so other threads can   unwind cooperatively.</li> <li>Once the bundle running on the main thread exits successfully the   treatment depends on <code>main_thread</code>:<ul> <li><code>\"algorithm\"</code>: the runners are asked to stop by setting <code>stop_evt</code>.</li> <li><code>\"runner\"</code>: the algorithm keeps running until it exits naturally.</li> </ul> </li> <li>Background threads are marked as daemons. We join them briefly and log any   stragglers before shutting down.</li> </ul> <p>Note</p> <p>Signals other than <code>SIGINT</code> (such as <code>SIGTERM</code>) are not intercepted; Python's default behavior for those signals is preserved.</p>"},{"location":"reference/trainer/#events","title":"Events","text":""},{"location":"reference/trainer/#agentlightning.ExecutionEvent","title":"<code>agentlightning.ExecutionEvent</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol capturing the cooperative stop contract shared by strategies.</p> <p>Implementations mirror the API of <code>threading.Event</code> and <code>multiprocessing.Event</code> so the rest of the execution layer can remain agnostic to the underlying concurrency primitive.</p> <p>Methods:</p> <pre><code>set: Signal cancellation. The call must be idempotent.\nclear: Reset the event to the unsignaled state.\nis_set: Return ``True`` when cancellation has been requested.\nwait: Block until the event is signaled or an optional timeout elapses.\n</code></pre>"},{"location":"reference/trainer/#agentlightning.ThreadingEvent","title":"<code>agentlightning.ThreadingEvent</code>","text":"<p>Thread-safe implementation of <code>ExecutionEvent</code>.</p>"},{"location":"reference/trainer/#agentlightning.MultiprocessingEvent","title":"<code>agentlightning.MultiprocessingEvent</code>","text":"<p>Process-safe implementation of <code>ExecutionEvent</code>.</p>"},{"location":"reference/trainer/#cli-builder","title":"CLI Builder","text":""},{"location":"reference/trainer/#agentlightning.lightning_cli","title":"<code>agentlightning.lightning_cli(*classes)</code>","text":"<pre><code>lightning_cli(cls1: Type[_C1]) -&gt; _C1\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1], cls2: Type[_C2]\n) -&gt; Tuple[_C1, _C2]\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1], cls2: Type[_C2], cls3: Type[_C3]\n) -&gt; Tuple[_C1, _C2, _C3]\n</code></pre><pre><code>lightning_cli(\n    cls1: Type[_C1],\n    cls2: Type[_C2],\n    cls3: Type[_C3],\n    cls4: Type[_C4],\n) -&gt; Tuple[_C1, _C2, _C3, _C4]\n</code></pre><pre><code>lightning_cli(\n    *classes: Type[CliConfigurable],\n) -&gt; Tuple[CliConfigurable, ...]\n</code></pre> <p>Parses command-line arguments to configure and instantiate provided CliConfigurable classes.</p> <p>Parameters:</p> <ul> <li> <code>*classes</code>               (<code>Type[CliConfigurable]</code>, default:                   <code>()</code> )           \u2013            <p>One or more classes that inherit from CliConfigurable. Each class's       init parameters will be exposed as command-line arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>CliConfigurable | Tuple[CliConfigurable, ...]</code>           \u2013            <p>A tuple of instantiated objects, corresponding to the input classes in order.</p> </li> </ul>"},{"location":"reference/types/","title":"Type References","text":""},{"location":"reference/types/#core-types","title":"Core Types","text":""},{"location":"reference/types/#agentlightning.Triplet","title":"<code>agentlightning.Triplet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single interaction turn captured during reinforcement learning.</p>"},{"location":"reference/types/#agentlightning.RolloutRawResult","title":"<code>agentlightning.RolloutRawResult = Union[None, float, List[ReadableSpan], List[Span]]</code>  <code>module-attribute</code>","text":"<p>Rollout result type.</p> <p>Possible return values of <code>rollout</code>.</p>"},{"location":"reference/types/#agentlightning.RolloutMode","title":"<code>agentlightning.RolloutMode = Literal['train', 'val', 'test']</code>  <code>module-attribute</code>","text":"<p>Possible rollout modes.</p>"},{"location":"reference/types/#agentlightning.GenericResponse","title":"<code>agentlightning.GenericResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generic server response used by compatibility endpoints.</p> <p>Deprecated</p> <p>This response is no longer used by the new <code>LightningStore</code> APIs.</p> <p>Attributes:</p> <ul> <li> <code>status</code>               (<code>str</code>)           \u2013            <p>Status string describing the result of the request.</p> </li> <li> <code>message</code>               (<code>Optional[str]</code>)           \u2013            <p>Optional human readable explanation.</p> </li> <li> <code>data</code>               (<code>Optional[Dict[str, Any]]</code>)           \u2013            <p>Arbitrary payload serialized as JSON.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase","title":"<code>agentlightning.ParallelWorkerBase</code>","text":"<p>Base class for workloads executed across multiple worker processes.</p> <p>The lifecycle is orchestrated by the main process:</p> <ul> <li><code>init()</code> prepares shared state.</li> <li>Each worker calls <code>init_worker()</code> during start-up.</li> <li><code>run()</code> performs the parallel workload.</li> <li>Workers call <code>teardown_worker()</code> before exiting.</li> <li>The main process finalizes through <code>teardown()</code>.</li> </ul> <p>Subclasses must implement <code>run()</code> and can override other lifecycle hooks.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the base class. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.init","title":"<code>init(*args, **kwargs)</code>","text":"<p>Initialize before spawning the workers. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.init_worker","title":"<code>init_worker(worker_id, *args, **kwargs)</code>","text":"<p>Initialize the worker. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.run","title":"<code>run(*args, **kwargs)</code>","text":"<p>Run the workload. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.teardown","title":"<code>teardown(*args, **kwargs)</code>","text":"<p>Teardown after the workers have exited. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.ParallelWorkerBase.teardown_worker","title":"<code>teardown_worker(worker_id, *args, **kwargs)</code>","text":"<p>Teardown the worker. This method can be overridden by subclasses.</p>"},{"location":"reference/types/#agentlightning.Dataset","title":"<code>agentlightning.Dataset</code>","text":"<p>               Bases: <code>Protocol</code>, <code>Generic[T_co]</code></p> <p>The general interface for a dataset.</p> <p>It's currently implemented as a protocol, having a similar interface to <code>torch.utils.data.Dataset</code>. You don't have to inherit from this class; you can use a simple list if you want to.</p>"},{"location":"reference/types/#agentlightning.AttemptStatus","title":"<code>agentlightning.AttemptStatus = Literal['preparing', 'running', 'failed', 'succeeded', 'unresponsive', 'timeout']</code>  <code>module-attribute</code>","text":"<p>The status of an attempt.</p>"},{"location":"reference/types/#agentlightning.RolloutStatus","title":"<code>agentlightning.RolloutStatus = Literal['queuing', 'preparing', 'running', 'failed', 'succeeded', 'cancelled', 'requeuing']</code>  <code>module-attribute</code>","text":"<p>The status of a rollout.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig","title":"<code>agentlightning.RolloutConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration controlling rollout retries and timeouts.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig.max_attempts","title":"<code>max_attempts = Field(default=1, ge=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The maximum number of attempts for the rollout, including the first attempt.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig.retry_condition","title":"<code>retry_condition = Field(default_factory=(cast(Callable[[], List[AttemptStatus]], list)))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The list of statuses that should trigger a retry.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig.timeout_seconds","title":"<code>timeout_seconds = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timeout for the rollout, in seconds. None indicates no timeout.</p>"},{"location":"reference/types/#agentlightning.RolloutConfig.unresponsive_seconds","title":"<code>unresponsive_seconds = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The unresponsive timeout for the rollout, in seconds. None indicates no unresponsive timeout.</p>"},{"location":"reference/types/#agentlightning.Rollout","title":"<code>agentlightning.Rollout</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/types/#agentlightning.Rollout.config","title":"<code>config = Field(default_factory=RolloutConfig)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Retry and timeout configuration associated with the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.end_time","title":"<code>end_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Timestamp when the rollout ended.</p>"},{"location":"reference/types/#agentlightning.Rollout.input","title":"<code>input</code>  <code>instance-attribute</code>","text":"<p>Task input used to generate the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.metadata","title":"<code>metadata = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional metadata attached to the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.mode","title":"<code>mode = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Execution mode such as <code>\"train\"</code>, <code>\"val\"</code> or <code>\"test\"</code>. See <code>RolloutMode</code>.</p>"},{"location":"reference/types/#agentlightning.Rollout.resources_id","title":"<code>resources_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Identifier of the resources required to execute the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.rollout_id","title":"<code>rollout_id</code>  <code>instance-attribute</code>","text":"<p>Unique identifier for the rollout.</p>"},{"location":"reference/types/#agentlightning.Rollout.start_time","title":"<code>start_time</code>  <code>instance-attribute</code>","text":"<p>Timestamp when the rollout started.</p>"},{"location":"reference/types/#agentlightning.Rollout.status","title":"<code>status = 'queuing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Latest status emitted by the controller.</p>"},{"location":"reference/types/#agentlightning.Attempt","title":"<code>agentlightning.Attempt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Execution attempt for a rollout, including metadata for retries.</p>"},{"location":"reference/types/#agentlightning.Attempt.attempt_id","title":"<code>attempt_id</code>  <code>instance-attribute</code>","text":"<p>The universal id for current attempt.</p>"},{"location":"reference/types/#agentlightning.Attempt.end_time","title":"<code>end_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The time when the attempt has ended.</p>"},{"location":"reference/types/#agentlightning.Attempt.last_heartbeat_time","title":"<code>last_heartbeat_time = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The last time when the worker has reported progress (i.e., a span).</p>"},{"location":"reference/types/#agentlightning.Attempt.metadata","title":"<code>metadata = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A bucket for any other relevant information.</p>"},{"location":"reference/types/#agentlightning.Attempt.rollout_id","title":"<code>rollout_id</code>  <code>instance-attribute</code>","text":"<p>The rollout which this attempt belongs to.</p>"},{"location":"reference/types/#agentlightning.Attempt.sequence_id","title":"<code>sequence_id</code>  <code>instance-attribute</code>","text":"<p>The sequence number of the attempt, starting from 1.</p>"},{"location":"reference/types/#agentlightning.Attempt.start_time","title":"<code>start_time</code>  <code>instance-attribute</code>","text":"<p>The time when the attempt has started.</p>"},{"location":"reference/types/#agentlightning.Attempt.status","title":"<code>status = 'preparing'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The status of the attempt.</p>"},{"location":"reference/types/#agentlightning.Attempt.worker_id","title":"<code>worker_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The rollout worker which is executing this attempt.</p>"},{"location":"reference/types/#agentlightning.AttemptedRollout","title":"<code>agentlightning.AttemptedRollout</code>","text":"<p>               Bases: <code>Rollout</code></p> <p>Rollout paired with the currently active attempt.</p>"},{"location":"reference/types/#agentlightning.AttemptedRollout.attempt","title":"<code>attempt</code>  <code>instance-attribute</code>","text":"<p>The attempt that is currently processing the rollout.</p>"},{"location":"reference/types/#agentlightning.Hook","title":"<code>agentlightning.Hook</code>","text":"<p>               Bases: <code>ParallelWorkerBase</code></p> <p>Base class for defining hooks in the agent runner's lifecycle.</p>"},{"location":"reference/types/#agentlightning.Hook.on_rollout_end","title":"<code>on_rollout_end(*, agent, runner, rollout, spans)</code>  <code>async</code>","text":"<p>Hook called after a rollout attempt completes.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[Any]</code>)           \u2013            <p>The <code>LitAgent</code> instance associated with the runner.</p> </li> <li> <code>runner</code>               (<code>Runner[Any]</code>)           \u2013            <p>The <code>Runner</code> managing the rollout.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>The <code>Rollout</code> object that has been processed.</p> </li> <li> <code>spans</code>               (<code>Union[List[ReadableSpan], List[Span]]</code>)           \u2013            <p>The spans that have been added to the store.</p> </li> </ul> <p>Subclasses can override this method for cleanup or additional logging. By default, this is a no-op.</p>"},{"location":"reference/types/#agentlightning.Hook.on_rollout_start","title":"<code>on_rollout_start(*, agent, runner, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately before a rollout attempt begins.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[Any]</code>)           \u2013            <p>The <code>LitAgent</code> instance associated with the runner.</p> </li> <li> <code>runner</code>               (<code>Runner[Any]</code>)           \u2013            <p>The <code>Runner</code> managing the rollout.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>The <code>Rollout</code> object that will be processed.</p> </li> </ul> <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. By default, this is a no-op.</p>"},{"location":"reference/types/#agentlightning.Hook.on_trace_end","title":"<code>on_trace_end(*, agent, runner, tracer, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately after the rollout completes but before the tracer exits the trace context.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[Any]</code>)           \u2013            <p>The <code>LitAgent</code> instance associated with the runner.</p> </li> <li> <code>runner</code>               (<code>Runner[Any]</code>)           \u2013            <p>The <code>Runner</code> managing the rollout.</p> </li> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p>The <code>Tracer</code> instance associated with the runner.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>The <code>Rollout</code> object that has been processed.</p> </li> </ul> <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource cleanup. By default, this is a no-op.</p>"},{"location":"reference/types/#agentlightning.Hook.on_trace_start","title":"<code>on_trace_start(*, agent, runner, tracer, rollout)</code>  <code>async</code>","text":"<p>Hook called immediately after the tracer enters the trace context but before the rollout begins.</p> <p>Parameters:</p> <ul> <li> <code>agent</code>               (<code>LitAgent[Any]</code>)           \u2013            <p>The <code>LitAgent</code> instance associated with the runner.</p> </li> <li> <code>runner</code>               (<code>Runner[Any]</code>)           \u2013            <p>The <code>Runner</code> managing the rollout.</p> </li> <li> <code>tracer</code>               (<code>Tracer</code>)           \u2013            <p>The <code>Tracer</code> instance associated with the runner.</p> </li> <li> <code>rollout</code>               (<code>Rollout</code>)           \u2013            <p>The <code>Rollout</code> object that will be processed.</p> </li> </ul> <p>Subclasses can override this method to implement custom logic such as logging, metric collection, or resource setup. By default, this is a no-op.</p>"},{"location":"reference/types/#resources","title":"Resources","text":""},{"location":"reference/types/#agentlightning.Resource","title":"<code>agentlightning.Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for tunable resources distributed to executors.</p>"},{"location":"reference/types/#agentlightning.Resource.resource_type","title":"<code>resource_type</code>  <code>instance-attribute</code>","text":"<p>Alias of the resource type.</p>"},{"location":"reference/types/#agentlightning.LLM","title":"<code>agentlightning.LLM</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Resource that identifies an LLM endpoint and its configuration.</p>"},{"location":"reference/types/#agentlightning.LLM.api_key","title":"<code>api_key = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional secret used to authenticate requests.</p>"},{"location":"reference/types/#agentlightning.LLM.endpoint","title":"<code>endpoint</code>  <code>instance-attribute</code>","text":"<p>The URL of the LLM API endpoint.</p>"},{"location":"reference/types/#agentlightning.LLM.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>The identifier for the model to be used (e.g., 'gpt-4o').</p>"},{"location":"reference/types/#agentlightning.LLM.sampling_parameters","title":"<code>sampling_parameters = Field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of hyperparameters for model inference, such as temperature, top_p, etc.</p>"},{"location":"reference/types/#agentlightning.LLM.get_base_url","title":"<code>get_base_url(*args, **kwargs)</code>","text":"<p>Return the base URL consumed by OpenAI-compatible clients.</p> <p>Users are encouraged to use <code>get_base_url(rollout_id, attempt_id)</code> to get the LLM endpoint instead of accessing <code>.endpoint</code> directly.</p>"},{"location":"reference/types/#agentlightning.ProxyLLM","title":"<code>agentlightning.ProxyLLM</code>","text":"<p>               Bases: <code>LLM</code></p> <p>LLM resource that rewrites endpoints through <code>LLMProxy</code>.</p> <p>The proxy injects rollout- and attempt-specific routing information into the endpoint so that downstream services can attribute requests correctly.</p>"},{"location":"reference/types/#agentlightning.ProxyLLM.__getattribute__","title":"<code>__getattribute__(name)</code>","text":"<p>Emit a warning when <code>endpoint</code> is accessed directly after initialization.</p>"},{"location":"reference/types/#agentlightning.ProxyLLM.get_base_url","title":"<code>get_base_url(rollout_id, attempt_id)</code>","text":"<p>Return the routed endpoint for a specific rollout/attempt pair.</p> <p>Parameters:</p> <ul> <li> <code>rollout_id</code>               (<code>Optional[str]</code>)           \u2013            <p>Identifier of the rollout making the request.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>)           \u2013            <p>Identifier of the attempt within that rollout.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Fully qualified endpoint including rollout metadata.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If exactly one of <code>rollout_id</code> or <code>attempt_id</code> is provided.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.ProxyLLM.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Mark initialization as complete after Pydantic finishes setup.</p>"},{"location":"reference/types/#agentlightning.ProxyLLM.with_attempted_rollout","title":"<code>with_attempted_rollout(rollout)</code>","text":"<p>Bake rollout metadata into a concrete <code>LLM</code> instance.</p>"},{"location":"reference/types/#agentlightning.PromptTemplate","title":"<code>agentlightning.PromptTemplate</code>","text":"<p>               Bases: <code>Resource</code></p> <p>Resource describing a reusable prompt template.</p>"},{"location":"reference/types/#agentlightning.PromptTemplate.engine","title":"<code>engine</code>  <code>instance-attribute</code>","text":"<p>The templating engine to use for rendering the prompt.</p>"},{"location":"reference/types/#agentlightning.PromptTemplate.template","title":"<code>template</code>  <code>instance-attribute</code>","text":"<p>The template string. The format depends on the engine.</p>"},{"location":"reference/types/#agentlightning.PromptTemplate.format","title":"<code>format(**kwargs)</code>","text":"<p>Format the prompt using keyword arguments.</p> <p>Warning</p> <p>Only the <code>f-string</code> engine is supported for now.</p>"},{"location":"reference/types/#agentlightning.ResourceUnion","title":"<code>agentlightning.ResourceUnion = Annotated[Union[LLM, ProxyLLM, PromptTemplate], Field(discriminator='resource_type')]</code>  <code>module-attribute</code>","text":""},{"location":"reference/types/#agentlightning.NamedResources","title":"<code>agentlightning.NamedResources = Dict[str, ResourceUnion]</code>  <code>module-attribute</code>","text":"<p>Mapping from resource names to their configured instances.</p> <p>Examples:</p> <pre><code>resources: NamedResources = {\n    \"main_llm\": LLM(\n        endpoint=\"http://localhost:8080\",\n        model=\"llama3\",\n        sampling_parameters={\"temperature\": 0.7, \"max_tokens\": 100},\n    ),\n    \"system_prompt\": PromptTemplate(\n        template=\"You are a helpful assistant.\",\n        engine=\"f-string\",\n    ),\n}\n</code></pre>"},{"location":"reference/types/#agentlightning.ResourcesUpdate","title":"<code>agentlightning.ResourcesUpdate</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Update payload broadcast to clients when resources change.</p>"},{"location":"reference/types/#agentlightning.ResourcesUpdate.resources","title":"<code>resources</code>  <code>instance-attribute</code>","text":"<p>Mapping of resource names to their definitions.</p>"},{"location":"reference/types/#agentlightning.ResourcesUpdate.resources_id","title":"<code>resources_id</code>  <code>instance-attribute</code>","text":"<p>Identifier used to version the resources.</p>"},{"location":"reference/types/#traces","title":"Traces","text":""},{"location":"reference/types/#agentlightning.AttributeValue","title":"<code>agentlightning.AttributeValue = Union[str, bool, int, float, Sequence[str], Sequence[bool], Sequence[int], Sequence[float]]</code>  <code>module-attribute</code>","text":"<p>Possible values for OpenTelemetry attributes.</p>"},{"location":"reference/types/#agentlightning.Attributes","title":"<code>agentlightning.Attributes = Dict[str, AttributeValue]</code>  <code>module-attribute</code>","text":"<p>Mapping from attribute names to their values. Same as OpenTelemetry <code>Attributes</code> type.</p>"},{"location":"reference/types/#agentlightning.TraceState","title":"<code>agentlightning.TraceState = Dict[str, str]</code>  <code>module-attribute</code>","text":"<p>Mapping from trace state key to its value. Same as OpenTelemetry <code>TraceState</code> type.</p>"},{"location":"reference/types/#agentlightning.SpanContext","title":"<code>agentlightning.SpanContext</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic representation of <code>opentelemetry.trace.SpanContext</code> values.</p>"},{"location":"reference/types/#agentlightning.SpanContext.is_remote","title":"<code>is_remote</code>  <code>instance-attribute</code>","text":"<p>Whether the span is remote.</p>"},{"location":"reference/types/#agentlightning.SpanContext.span_id","title":"<code>span_id</code>  <code>instance-attribute</code>","text":"<p>The span ID of the span.</p>"},{"location":"reference/types/#agentlightning.SpanContext.trace_id","title":"<code>trace_id</code>  <code>instance-attribute</code>","text":"<p>The trace ID of the span.</p>"},{"location":"reference/types/#agentlightning.SpanContext.trace_state","title":"<code>trace_state</code>  <code>instance-attribute</code>","text":"<p>Mapping from trace state key to its value.</p>"},{"location":"reference/types/#agentlightning.SpanContext.from_opentelemetry","title":"<code>from_opentelemetry(src)</code>  <code>classmethod</code>","text":"<p>Construct a <code>SpanContext</code> from OpenTelemetry data.</p>"},{"location":"reference/types/#agentlightning.TraceStatus","title":"<code>agentlightning.TraceStatus</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable variant of <code>opentelemetry.trace.Status</code>.</p>"},{"location":"reference/types/#agentlightning.TraceStatus.description","title":"<code>description = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The description of the span. Same as OpenTelemetry <code>Status.description</code> type.</p>"},{"location":"reference/types/#agentlightning.TraceStatus.status_code","title":"<code>status_code</code>  <code>instance-attribute</code>","text":"<p>The status code of the span. Same as OpenTelemetry <code>Status.status_code</code> type.</p>"},{"location":"reference/types/#agentlightning.TraceStatus.from_opentelemetry","title":"<code>from_opentelemetry(src)</code>  <code>classmethod</code>","text":"<p>Create a <code>TraceStatus</code> from OpenTelemetry metadata.</p>"},{"location":"reference/types/#agentlightning.Event","title":"<code>agentlightning.Event</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable representation of OpenTelemetry <code>Event</code> values.</p>"},{"location":"reference/types/#agentlightning.Event.attributes","title":"<code>attributes</code>  <code>instance-attribute</code>","text":"<p>Mapping from attribute names to their values. Same as OpenTelemetry <code>Attributes</code> type.</p>"},{"location":"reference/types/#agentlightning.Event.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the event.</p>"},{"location":"reference/types/#agentlightning.Event.timestamp","title":"<code>timestamp = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The timestamp of the event. Same as OpenTelemetry <code>Event.timestamp</code> type.</p>"},{"location":"reference/types/#agentlightning.Event.from_opentelemetry","title":"<code>from_opentelemetry(src)</code>  <code>classmethod</code>","text":"<p>Create an <code>Event</code> from an OpenTelemetry event.</p>"},{"location":"reference/types/#agentlightning.Link","title":"<code>agentlightning.Link</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable representation of OpenTelemetry <code>Link</code> values.</p>"},{"location":"reference/types/#agentlightning.Link.attributes","title":"<code>attributes = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Optional attributes.</p>"},{"location":"reference/types/#agentlightning.Link.context","title":"<code>context</code>  <code>instance-attribute</code>","text":"<p>The context of the link.</p>"},{"location":"reference/types/#agentlightning.Link.from_opentelemetry","title":"<code>from_opentelemetry(src)</code>  <code>classmethod</code>","text":"<p>Create a <code>Link</code> from an OpenTelemetry link.</p>"},{"location":"reference/types/#agentlightning.Resource","title":"<code>agentlightning.Resource</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for tunable resources distributed to executors.</p>"},{"location":"reference/types/#agentlightning.Resource.resource_type","title":"<code>resource_type</code>  <code>instance-attribute</code>","text":"<p>Alias of the resource type.</p>"},{"location":"reference/types/#agentlightning.Span","title":"<code>agentlightning.Span</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Agent Lightning's canonical span model used for persistence and analytics.</p> <p>The model captures the most relevant fields from <code>opentelemetry.sdk.trace.ReadableSpan</code> instances while preserving unmodeled attributes in Pydantic <code>BaseModel</code>'s extra storage. This keeps the serialized format stable even as upstream OpenTelemetry types evolve.</p>"},{"location":"reference/types/#agentlightning.Span.attempt_id","title":"<code>attempt_id</code>  <code>instance-attribute</code>","text":"<p>The attempt which this span belongs to.</p>"},{"location":"reference/types/#agentlightning.Span.attributes","title":"<code>attributes</code>  <code>instance-attribute</code>","text":"<p>The attributes of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.context","title":"<code>context</code>  <code>instance-attribute</code>","text":"<p>The context of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.end_time","title":"<code>end_time</code>  <code>instance-attribute</code>","text":"<p>The end time of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.events","title":"<code>events</code>  <code>instance-attribute</code>","text":"<p>The events of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.links","title":"<code>links</code>  <code>instance-attribute</code>","text":"<p>The links of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.parent","title":"<code>parent</code>  <code>instance-attribute</code>","text":"<p>The parent context of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.parent_id","title":"<code>parent_id</code>  <code>instance-attribute</code>","text":"<p>The parent span ID of the span.</p>"},{"location":"reference/types/#agentlightning.Span.resource","title":"<code>resource</code>  <code>instance-attribute</code>","text":"<p>The resource of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.rollout_id","title":"<code>rollout_id</code>  <code>instance-attribute</code>","text":"<p>The rollout which this span belongs to.</p>"},{"location":"reference/types/#agentlightning.Span.sequence_id","title":"<code>sequence_id</code>  <code>instance-attribute</code>","text":"<p>The ID to make spans ordered within a single attempt.</p>"},{"location":"reference/types/#agentlightning.Span.span_id","title":"<code>span_id</code>  <code>instance-attribute</code>","text":"<p>The span ID of the span. This ID comes from the OpenTelemetry span ID generator.</p>"},{"location":"reference/types/#agentlightning.Span.start_time","title":"<code>start_time</code>  <code>instance-attribute</code>","text":"<p>The start time of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.status","title":"<code>status</code>  <code>instance-attribute</code>","text":"<p>The status of the span. See OpenTelemetry docs.</p>"},{"location":"reference/types/#agentlightning.Span.trace_id","title":"<code>trace_id</code>  <code>instance-attribute</code>","text":"<p>The trace ID of the span. One rollout/attempt can have multiple traces. This ID comes from the OpenTelemetry trace ID generator.</p>"},{"location":"reference/types/#agentlightning.Span.from_attributes","title":"<code>from_attributes(*, attributes, rollout_id=None, attempt_id=None, sequence_id=None, name=None, trace_id=None, span_id=None, parent_id=None, start_time=None, end_time=None, resource=None)</code>  <code>classmethod</code>","text":"<p>Build a synthetic span from raw attributes. Different from the <code>from_opentelemetry</code> method, all parameters other than <code>attributes</code> are optional and will be generated if not provided.</p> <p>Parameters:</p> <ul> <li> <code>attributes</code>               (<code>Attributes</code>)           \u2013            <p>Span attributes to persist.</p> </li> <li> <code>rollout_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional rollout identifier associated with the span.</p> </li> <li> <code>attempt_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional attempt identifier associated with the span.</p> </li> <li> <code>sequence_id</code>               (<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>Optional sequence number to preserve ordering.</p> </li> <li> <code>name</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional human-readable span name.</p> </li> <li> <code>trace_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom trace identifier. When omitted, a random identifier is generated.</p> </li> <li> <code>span_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Custom span identifier. When omitted, a random identifier is generated.</p> </li> <li> <code>parent_id</code>               (<code>Optional[str]</code>, default:                   <code>None</code> )           \u2013            <p>Optional parent span identifier.</p> </li> <li> <code>start_time</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Span start timestamp in seconds.</p> </li> <li> <code>end_time</code>               (<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>Span end timestamp in seconds.</p> </li> <li> <code>resource</code>               (<code>Optional[OtelResource]</code>, default:                   <code>None</code> )           \u2013            <p>Explicit resource information to attach to the span.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'Span'</code>           \u2013            <p><code>Span</code> populated with the provided attributes.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.Span.from_opentelemetry","title":"<code>from_opentelemetry(src, rollout_id, attempt_id, sequence_id)</code>  <code>classmethod</code>","text":"<p>Convert an OpenTelemetry span into the Agent Lightning data model.</p> <p>Parameters:</p> <ul> <li> <code>src</code>               (<code>ReadableSpan</code>)           \u2013            <p>Span captured by OpenTelemetry.</p> </li> <li> <code>rollout_id</code>               (<code>str</code>)           \u2013            <p>Identifier for the rollout that produced the span.</p> </li> <li> <code>attempt_id</code>               (<code>str</code>)           \u2013            <p>Identifier of the attempt within the rollout.</p> </li> <li> <code>sequence_id</code>               (<code>int</code>)           \u2013            <p>Monotonically increasing identifier assigned to the span.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>'Span'</code>           \u2013            <p>Parsed <code>Span</code> instance suitable for persistence.</p> </li> </ul>"},{"location":"reference/types/#agentlightning.SpanNames","title":"<code>agentlightning.SpanNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumerated span names recognised by Agent-lightning.</p>"},{"location":"reference/types/#agentlightning.SpanNames.EXCEPTION","title":"<code>EXCEPTION = 'agentlightning.exception'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the exception span.</p>"},{"location":"reference/types/#agentlightning.SpanNames.MESSAGE","title":"<code>MESSAGE = 'agentlightning.message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the message span.</p>"},{"location":"reference/types/#agentlightning.SpanNames.OBJECT","title":"<code>OBJECT = 'agentlightning.object'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the object span.</p>"},{"location":"reference/types/#agentlightning.SpanNames.REWARD","title":"<code>REWARD = 'agentlightning.reward'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the reward span.</p>"},{"location":"reference/types/#agentlightning.SpanNames.VIRTUAL","title":"<code>VIRTUAL = 'agentlightning.virtual'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the virtual span. It represents derived spans without concrete operations.</p>"},{"location":"reference/types/#agentlightning.SpanAttributeNames","title":"<code>agentlightning.SpanAttributeNames</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Canonical attribute names written by Agent Lightning emitters.</p>"},{"location":"reference/types/#agentlightning.SpanAttributeNames.MESSAGE","title":"<code>MESSAGE = 'message'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the message attribute.</p>"},{"location":"reference/types/#agentlightning.SpanAttributeNames.OBJECT","title":"<code>OBJECT = 'object'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The name of the object attribute.</p>"},{"location":"reference/types/#agentlightning.SpanLike","title":"<code>agentlightning.SpanLike = Union[ReadableSpan, Span]</code>  <code>module-attribute</code>","text":"<p>Union type of OpenTelemetry <code>ReadableSpan</code> and Agent-lightning <code>Span</code>.</p>"},{"location":"tutorials/debug/","title":"Debugging and Troubleshooting","text":"<p>When you train your own agent with Agent-lightning, most failures surface because the agent logic is brittle or simply incorrect. Debugging becomes easier when you peel back the stack: start by driving the rollout logic on its own, dry-run the trainer loop, and only then bring the full algorithm and runner topology online. The <code>examples/apo/apo_debug.py</code> script demonstrates these techniques; this guide expands on each approach and helps you decide when to reach for them.</p>"},{"location":"tutorials/debug/#using-runner-in-isolation","title":"Using <code>Runner</code> in Isolation","text":"<p><code>Runner</code> is a long-lived worker that wraps your <code>LitAgent</code>, coordinates tracing, and talks to the <code>LightningStore</code>. In typical training flows the trainer manages runners for you, but being able to spin one up manually is invaluable while debugging.</p> <p>If you define rollout logic with <code>@rollout</code> or implement a <code>LitAgent</code> directly, you will get a <code>LitAgent</code> instance and you should be able to execute it with <code>LitAgentRunner</code>, which is a subclass of <code>Runner</code>. The runner needs but does not instantiate a <code>Tracer</code>, so supply one yourself. See Working with Traces for a walkthrough of tracer options.</p> <p><code>Runner.run_context</code> prepares the runner to execute a particular agent. Besides the agent and tracer you must provide a store that will collect spans and rollouts. <code>InMemoryLightningStore</code> keeps everything in-process, which is perfect for debugging sessions.</p> <pre><code>import agentlightning as agl\n\ntracer = agl.OtelTracer()\nrunner = agl.LitAgentRunner(tracer)\nstore = agl.InMemoryLightningStore()\n\nwith runner.run_context(agent=apo_rollout, store=store):\n    ...\n</code></pre> <p>Inside the <code>run_context</code> block you can call <code>runner.step(...)</code> to execute a single rollout. The payload includes the task input and any <code>NamedResources</code> the agent expects. Read introduction to Resources and NamedResources for more details. For example, if your agent references a <code>PromptTemplate</code>, pass it through the <code>resources</code> argument:</p> <pre><code>with runner.run_context(agent=apo_rollout, store=store):\n    resource = agl.PromptTemplate(template=\"You are a helpful assistant. {any_question}\", engine=\"f-string\")\n    rollout = await runner.step(\n        \"Explain why the sky appears blue using principles of light scattering in 100 words.\",\n        resources={\"main_prompt\": resource},\n    )\n</code></pre> <p>You can do as many things as you want within the <code>Runner.run_context</code> block. After the rollout finishes you can query the store to inspect what happened:</p> <pre><code>print(await store.query_rollouts())\nprint(await store.query_spans(rollout.rollout_id))\n</code></pre> <p>Example output (with a reward span captured):</p> <pre><code>[Rollout(rollout_id='ro-519769241af8', input='Explain why the sky appears blue using principles of light scattering in 100 words.', start_time=1760706315.6996238, ..., status='succeeded')]\n[Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=1, ..., name='agentlightning.reward', attributes={'reward': 0.95}, ...)]\n</code></pre> <p>Swap in an <code>AgentOpsTracer</code> instead of <code>OtelTracer</code> to see the underlying LLM spans alongside reward information:</p> <pre><code>[\n    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=1, ..., name='openai.chat.completion', attributes={..., 'gen_ai.prompt.0.role': 'user', 'gen_ai.prompt.0.content': 'You are a helpful assistant. Explain why the sky appears blue using principles of light scattering in 100 words.', ...}),\n    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=2, ..., name='openai.chat.completion', attributes={..., 'gen_ai.prompt.0.role': 'user', 'gen_ai.prompt.0.content': 'Evaluate how well the output fulfills the task...', ...}),\n    Span(rollout_id='ro-519769241af8', attempt_id='at-a6b62caf', sequence_id=3, ..., name='agentlightning.reward', attributes={'reward': 0.95}, ...)\n]\n</code></pre> <p>Tip</p> <p>Spans too difficult to read? Try using <code>Adapter</code> to convert them into a more readable format.</p> <p><code>Runner.step</code> executes a full rollout even though it is named \"step\". The companion method <code>Runner.iter</code> executes multiple \"steps\" by continuously pulling new rollout inputs from the store until a stop event is set. Use <code>iter</code> once you are confident the single-step path works and you have another worker <code>enqueue_rollout</code> to the store.</p> <p>Tip</p> <p>You can also call <code>Runner.step</code> to inject ad-hoc rollouts into a running store being used by another algorithm, so that the rollouts can be consumed by the algorithms. This is very recently known as the paradigm of \"online RL\". At the moment, no algorithm in the algorithm zoo consumes externally generated rollouts, but the data flow is available there if you need it.</p>"},{"location":"tutorials/debug/#hook-into-runners-lifecycle","title":"Hook into Runner's Lifecycle","text":"<p><code>Runner.run_context</code> accepts a <code>hooks</code> argument so you can observe or augment lifecycle events without editing your agent. Hooks subclass <code>Hook</code> and can respond to four asynchronous callbacks: <code>on_trace_start</code>, <code>on_rollout_start</code>, <code>on_rollout_end</code>, and <code>on_trace_end</code>. This is useful for:</p> <ul> <li>Capturing raw OpenTelemetry spans before they hit the store and before the <code>LitAgentRunner</code> do postprocessing on the rollout</li> <li>Inspecting the tracer instance after they are activated</li> <li>Logging rollout inputs before they are processed by the agent</li> </ul> <p>The <code>hook</code> mode in <code>examples/apo/apo_debug.py</code> prints every span collected during a rollout:</p> <pre><code>import agentlightning as agl\n\n# ... Same as previous example\n\nclass DebugHook(agl.Hook):\n    async def on_trace_end(self, *, agent, runner, tracer, rollout):\n        trace = tracer.get_last_trace()\n        print(\"Trace spans collected during the rollout:\")\n        for span in trace:\n            print(f\"- {span.name} (status: {span.status}):\\n  {span.attributes}\")\n\nwith runner.run_context(\n    agent=apo_rollout,\n    store=store,\n    hooks=[DebugHook()],\n):\n    await runner.step(\n        \"Explain why the sky appears blue using principles of light scattering in 100 words.\",\n        resources={\"main_prompt\": resource},\n    )\n</code></pre> <p>Because hooks run inside the runner process you can also attach debuggers or breakpoints directly in the callback implementations.</p> <p>Note</p> <p>For a better understanding of where hooks are called, we show a pseudo code of Runner's working flow below:</p> <pre><code>resources = await store.get_latest_resources()\nrollout = ...\ntry:\n    # &lt;-- on_rollout_start\n    with tracer.trace_context(...):\n        # &lt;--- on_trace_start\n        result = await agent.rollout(...)\n        # &lt;--- on_trace_end\n    post_process_result(result)\nexcept Exception:\n    # &lt;-- on_rollout_end\n    await store.update_attempt(status=...)\n</code></pre>"},{"location":"tutorials/debug/#dry-run-the-trainer-loop","title":"Dry-Run the Trainer Loop","text":"<p>Once single rollouts behave, switch to the trainer\u2019s dry-run mode. <code>Trainer.dev</code> spins up a lightweight fast algorithm \u2014 <code>agentlightning.Baseline</code> by default \u2014 so you can exercise the same infrastructure as <code>Trainer.fit</code> without standing up complex stacks like RL or SFT.</p> <p>Warning</p> <p>When you enable multiple runners via <code>n_runners</code>, the trainer may execute them in separate worker processes. Attaching a debugger such as <code>pdb</code> is only practical when <code>n_runners=1</code>, and even then the runner might not live in the main process.</p> <pre><code>import agentlightning as agl\n\ndataset: agl.Dataset[str] = [\n    \"Explain why the sky appears blue using principles of light scattering in 100 words.\",\n    \"What's the capital of France?\",\n]\nresource = agl.PromptTemplate(template=\"You are a helpful assistant. {any_question}\", engine=\"f-string\")\n\ntrainer = agl.Trainer(\n    n_runners=1,\n    initial_resources={\"main_prompt\": resource},\n)\ntrainer.dev(apo_rollout, dataset)\n</code></pre> <p>Just like <code>Runner.run_context</code>, <code>Trainer.dev</code> requires the <code>NamedResources</code> your agent expects. The key difference is that resources are attached to the trainer rather than the runner.</p> <p><code>Trainer.dev</code> uses an almost switchable interface from <code>Trainer.fit</code>. It also needs a dataset to iterate over, similar to <code>fit</code>. Under the hood <code>dev</code> uses the same implementation as <code>fit</code>, which means you can spin up multiple runners, observe scheduler behavior, and validate how algorithms adapt rollouts. The default <code>Baseline</code> logs detailed traces so you can see each rollout as the algorithm perceives it:</p> <pre><code>21:20:30 Initial resources set: {'main_prompt': PromptTemplate(resource_type='prompt_template', template='You are a helpful assistant. {any_question}', engine='f-string')}\n21:20:30 Proceeding epoch 1/1.\n21:20:30 Enqueued rollout ro-302fb202bd85 in train mode with sample: Explain why the sky appears blue using principles of light scattering in 100 words.\n21:20:30 Enqueued rollout ro-e65a3ffaa540 in train mode with sample: What's the capital of France?\n21:20:30 Waiting for 2 harvest tasks to complete...\n21:20:30 [Rollout ro-302fb202bd85] Status is initialized to queuing.\n21:20:30 [Rollout ro-e65a3ffaa540] Status is initialized to queuing.\n21:20:35 [Rollout ro-302fb202bd85] Finished with status succeeded in 3.80 seconds.\n21:20:35 [Rollout ro-302fb202bd85 | Attempt 1] ID: at-f84ad21c. Status: succeeded. Worker: Worker-0\n21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span 3a286a856af6bea8] #1 (openai.chat.completion) ... 1.95 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]\n21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span e2f44b775e058dd6] #2 (openai.chat.completion) ... 1.24 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]\n21:20:35 [Rollout ro-302fb202bd85 | Attempt at-f84ad21c | Span 45ee3c94fa1070ec] #3 (agentlightning.reward) ... 0.00 seconds. Attribute keys: ['reward']\n21:20:35 [Rollout ro-302fb202bd85] Adapted data: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': '...', 'agent_name': ''}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=0.95, metadata={'response_id': '...', 'agent_name': ''})]\n21:20:35 Finished 1 rollouts.\n21:20:35 [Rollout ro-e65a3ffaa540] Status changed to preparing.\n21:20:40 [Rollout ro-e65a3ffaa540] Finished with status succeeded in 6.39 seconds.\n21:20:40 [Rollout ro-e65a3ffaa540 | Attempt 1] ID: at-eaefa5d4. Status: succeeded. Worker: Worker-0\n21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 901dd6acc0f50147] #1 (openai.chat.completion) ... 1.30 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]\n21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 52e0aa63e02be611] #2 (openai.chat.completion) ... 1.26 seconds. Attribute keys: ['gen_ai.request.type', 'gen_ai.system', ...]\n21:20:40 [Rollout ro-e65a3ffaa540 | Attempt at-eaefa5d4 | Span 6c452de193fbffd3] #3 (agentlightning.reward) ... 0.00 seconds. Attribute keys: ['reward']\n21:20:40 [Rollout ro-e65a3ffaa540] Adapted data: [Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=None, metadata={'response_id': '...', 'agent_name': ''}), Triplet(prompt={'token_ids': []}, response={'token_ids': []}, reward=1.0, metadata={'response_id': '...', 'agent_name': ''})]\n21:20:40 Finished 2 rollouts.\n</code></pre> <p>The only limitation is that resources remain static and components like <code>LLMProxy</code> are not wired in. For richer dry runs you can subclass <code>FastAlgorithm</code> and override the pieces you care about.</p>"},{"location":"tutorials/debug/#debug-the-algorithm-runner-boundary","title":"Debug the Algorithm-Runner Boundary","text":"<p>Debugging algorithms in Agent-Lightning is often more challenging than debugging agents. Algorithms are typically stateful and depend on several moving parts \u2014 runners, stores, and trainers \u2014 which makes it difficult to isolate and inspect their behavior. Even mocking an agent to cooperate with an algorithm can be costly and error-prone. To simplify this, Agent-Lightning provides a way to run algorithms in isolation so you can attach a debugger and inspect internal state without interference from other components.</p> <p>By default, <code>Trainer.fit</code> runs the algorithm in the main process and thread, but its logs are interleaved with those from the store and runners, making it hard to follow what\u2019s happening inside the algorithm itself. In Write Your First Algorithm, we covered how to stand up a store, algorithm, and runner in isolation for your own implementations. This section extends that approach to cover two common questions:</p> <ol> <li>How can I run built-in or class-based algorithms (inheriting from <code>Algorithm</code>) in isolation?</li> <li>How can I still use <code>Trainer</code> features like <code>n_runners</code>, <code>adapter</code>, or <code>llm_proxy</code> while debugging?</li> </ol> <p>The solution is to keep using a <code>Trainer</code> instance but manage the store yourself, running the algorithm and runner roles separately. This approach mirrors the internal process orchestration of <code>Trainer.fit</code>, but with more visibility and control. Below, we show a step-by-step guide to achieve this with the <code>calc_agent</code> example.</p> <p>1. Launch the store manually. In a separate terminal, start the store:</p> <pre><code>agl store --port 4747\n</code></pre> <p>Then, in your training script, create a <code>LightningStoreClient</code> and pass it to the trainer:</p> <pre><code>client = agl.LightningStoreClient(\"http://localhost:4747\")\ntrainer = agl.Trainer(store=client, ...)\n</code></pre> <p>Set the environment variable <code>AGL_MANAGED_STORE=0</code> so the trainer doesn't attempt to manage the store automatically.</p> <p>2. Start the runner and algorithm processes separately. Each process should run the same training script, but with different environment variables specifying the current role. This setup faithfully mirrors how <code>Trainer.fit</code> orchestrates these components behind the scenes.</p> <pre><code># Terminal 2 \u2013 Runner process\nAGL_MANAGED_STORE=0 AGL_CURRENT_ROLE=runner \\\n    python train_calc_agent.py --external-store-address http://localhost:4747 --val-file data/test_mini.parquet\n\n# Terminal 3 \u2013 Algorithm process\nAGL_MANAGED_STORE=0 AGL_CURRENT_ROLE=algorithm \\\n    python train_calc_agent.py --external-store-address http://localhost:4747 --val-file data/test_mini.parquet\n</code></pre> <p>3. Reuse your existing trainer configuration. You can continue using the same datasets, adapters, and proxies as usual. Because the store is now external, you can:</p> <ul> <li>Attach debuggers to either the algorithm or runner process</li> <li>Add fine-grained logging or tracing</li> <li>Simulate partial failures or latency in individual components</li> </ul> <p>This setup provides a faithful reproduction of the algorithm\u2013runner interaction while keeping the store visible for inspection. Once you\u2019ve resolved the issue, simply set <code>AGL_MANAGED_STORE=1</code> (or omit it) to return to the standard managed training workflow.</p>"},{"location":"tutorials/installation/","title":"Installation Guide","text":"<p>This guide explains how to install Agent-Lightning. You can install it from PyPI (the Python Package Index) for general use or directly from the source code if you plan to contribute or need fine-grained control over dependencies.</p> <p>Platform and Hardware Requirements</p> <p>Agent-Lightning is officially supported on Linux distributions (Ubuntu 22.04 or later is recommended). At the moment macOS and Windows (outside of WSL2) are not supported.</p> <p>The Python runtime must be Python 3.10 or newer. We recommend using the latest patch release of Python 3.10, 3.11, or 3.12 to pick up performance and security updates.</p> <p>A GPU is optional\u2014you only need CUDA-capable hardware if you plan to fine-tune model weights or run GPU-accelerated workloads. CPU-only environments are fully supported for evaluation and inference.</p>"},{"location":"tutorials/installation/#installing-from-pypi","title":"Installing from PyPI","text":"<p>The easiest way to get started is by installing Agent-Lightning directly from PyPI. This ensures you get the latest stable release of the package, tested for compatibility and reliability.</p>"},{"location":"tutorials/installation/#install-the-stable-release","title":"Install the Stable Release","text":"<p>Run the following command in your terminal:</p> <pre><code>pip install --upgrade agentlightning\n</code></pre> <p>This installs or upgrades Agent-Lightning to the newest stable version.</p> <p>Tip</p> <p>If you intend to use Agent-Lightning with VERL or run any of its example scripts, you\u2019ll need to install some additional dependencies. See the sections on Algorithm-specific installation and Example-specific installation for details.</p>"},{"location":"tutorials/installation/#install-the-nightly-build-latest-features","title":"Install the Nightly Build (Latest Features)","text":"<p>Agent-Lightning also publishes nightly builds, which contain the latest experimental features and improvements from the main branch. These are available via Test PyPI.</p> <pre><code>pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ agentlightning\n</code></pre> <p>Warning</p> <p>The nightly builds are cutting-edge but may include unstable or untested changes. Use them at your own risk, especially in production environments.</p>"},{"location":"tutorials/installation/#algorithm-specific-installation","title":"Algorithm-specific Installation","text":"<p>Agent-Lightning supports multiple learning algorithms. Some of them like APO or VERL require extra dependencies. You can install them automatically using optional extras or manually if you prefer finer control.</p>"},{"location":"tutorials/installation/#installing-apo","title":"Installing APO","text":"<p>APO is an algorithm module that depends on libraries such as POML. You can install Agent-Lightning with APO support by running:</p> <pre><code>pip install agentlightning[apo]\n</code></pre> <p>Warning</p> <p>APO also depends on the OpenAI Python SDK, version 2.0 or newer. Ensure your SDK version is up to date to avoid compatibility issues.</p>"},{"location":"tutorials/installation/#installing-verl","title":"Installing VERL","text":"<p>VERL integrates with libraries like PyTorch, vLLM, and VERL framework. Although you can install all dependencies automatically, we recommend doing it manually to avoid version conflicts.</p> <pre><code>pip install agentlightning[verl]\n</code></pre> <p>Recommended Manual Setup (More Stable)</p> <p>Automated installation may cause issues if you don\u2019t have a compatible PyTorch or CUDA version preinstalled. For a more stable setup, install dependencies step-by-step:</p> <pre><code>pip install torch==2.8.0 torchvision==0.23.0 --index-url https://download.pytorch.org/whl/cu128\npip install flash-attn --no-build-isolation\npip install vllm==0.10.2\npip install verl==0.5.0\n</code></pre> <p>This approach ensures compatibility with CUDA 12.8 and minimizes dependency conflicts.</p>"},{"location":"tutorials/installation/#example-specific-installation","title":"Example-specific Installation","text":"<p>Each example in the <code>examples/</code> directory may have its own additional dependencies. Please refer to the README file of each example for detailed setup instructions:</p> <p>See Example READMEs.</p>"},{"location":"tutorials/installation/#installing-from-source-for-developers-and-contributors","title":"Installing from Source (for Developers and Contributors)","text":"<p>If you plan to contribute to Agent-Lightning or prefer to work with the latest development code, install it directly from the source repository.</p>"},{"location":"tutorials/installation/#why-install-from-source","title":"Why Install from Source?","text":"<ul> <li>You want to modify or contribute to the project.</li> <li>You prefer an isolated development environment.</li> <li>You want to test unreleased features or fix bugs locally.</li> </ul>"},{"location":"tutorials/installation/#using-uv-for-dependency-management","title":"Using <code>uv</code> for Dependency Management","text":"<p>Starting with version 0.2, Agent-Lightning uses <code>uv</code> as its default dependency manager.</p> <p><code>uv</code> is a fast and safe alternative to <code>pip</code> that:</p> <ul> <li>Installs packages in seconds (instead of minutes),</li> <li>Prevents dependency conflicts,</li> <li>Supports grouped dependencies for optional features.</li> </ul> <p>Before proceeding, make sure <code>uv</code> is installed.</p>"},{"location":"tutorials/installation/#minimal-developer-installation","title":"Minimal Developer Installation","text":"<pre><code>git clone https://github.com/microsoft/agent-lightning\ncd agent-lightning\nuv sync --group dev\n</code></pre> <p>This command sets up a clean development environment with only the essential dependencies.</p>"},{"location":"tutorials/installation/#installing-all-extras-cpu-or-gpu","title":"Installing All Extras (CPU or GPU)","text":"<p><code>uv sync</code> can also handle algorithm-specific and example-specific dependencies in one step.</p> <p>For a CPU-only machine:</p> <pre><code>uv sync --frozen \\\n    --extra apo \\\n    --extra verl \\\n    --group dev \\\n    --group torch-cpu \\\n    --group torch-stable \\\n    --group trl \\\n    --group agents \\\n    --no-default-groups\n</code></pre> <p>For a GPU-equipped machine that is CUDA 12.8 compatible:</p> <pre><code>uv sync --frozen \\\n    --extra apo \\\n    --extra verl \\\n    --group dev \\\n    --group torch-gpu-stable \\\n    --group trl \\\n    --group agents \\\n    --no-default-groups\n</code></pre> <p>Read more about Agent-lightning managed dependency groups here.</p>"},{"location":"tutorials/installation/#activating-your-environment","title":"Activating Your Environment","text":"<p>After syncing dependencies, <code>uv</code> automatically creates a virtual environment inside the <code>.venv/</code> directory.</p> <p>You can use it in two ways:</p> <pre><code># Option 1: Prefix commands with uv run\nuv run python your_script.py\n\n# Option 2: Activate the virtual environment\nsource .venv/bin/activate\npython your_script.py\n</code></pre> <p>Before Contributing</p> <p>Agent-Lightning enforces code style and linting rules via pre-commit hooks. Installing them early prevents many avoidable formatting issues.</p> <pre><code>uv run pre-commit install\nuv run pre-commit run --all-files --show-diff-on-failure --color=always\n</code></pre>"},{"location":"tutorials/parallelize/","title":"Scaling out Algorithms and Rollouts","text":"<p>Agent-lightning splits training into an algorithm bundle and a runner bundle that exchange work through the <code>LightningStore</code>. This tutorial shows how to increase rollout throughput, place bundles across processes or machines, and keep the algorithm side scalable with external frameworks.</p>"},{"location":"tutorials/parallelize/#parallelizing-rollouts-with-trainer","title":"Parallelizing Rollouts with <code>Trainer</code>","text":"<p>Before we dive into the details of the bundles and execution strategies, let's first revisit how to parallelize rollouts with <code>Trainer</code>.</p> <p><code>Trainer</code> is the quickest way to dial up parallelism. Even when <code>n_runners = 1</code>, calling <code>Trainer.fit</code> runs the algorithm and runners in parallel. The algorithm enqueues rollouts; runners dequeue them and execute your <code>LitAgent</code>, and the algorithm collects spans via its <code>Adapter</code> before scheduling the next batch.</p> <p>Note</p> <p>One of the most important features of <code>Trainer</code> is the ability to abort things gracefully. For example, if you press <code>Ctrl+C</code> in the terminal, the algorithm will abort and the runners will stop executing. If the algorithm crashes, the runners will also stop executing.</p> <p>Increase throughput by setting <code>n_runners</code> when constructing the trainer. The following example comes from train_calc_agent.py. Since backend LLMs usually use techniques like continuous batching to increase throughput, you do not have to worry about overwhelming the backend with too many requests.</p> <pre><code>import agentlightning as agl\nfrom datasets import Dataset as HFDataset\nfrom calc_agent import calc_agent\n\ntrain_dataset = HFDataset.from_parquet(\"data/train.parquet\").to_list()\nval_dataset = HFDataset.from_parquet(\"data/test.parquet\").to_list()\n\nalgorithm = agl.VERL(verl_config)\n\ntrainer = agl.Trainer(\n    algorithm=algorithm,\n    n_runners=8,  # launch eight rollout workers\n    tracer=agl.OtelTracer(),\n    adapter=agl.LlmProxyTraceToTriplet(),\n)\n\ntrainer.fit(calc_agent, train_dataset=train_dataset, val_dataset=val_dataset)\n</code></pre> <p>In <code>Trainer</code>, there are multiple other initialization parameters that you can use to customize the training process. For example, you can use <code>max_rollouts</code> to keep smoke tests short. Pass a concrete <code>LightningStore</code> instance when you need persistence or want to share the queue across multiple scripts.</p> <p>Tip</p> <p>Before scaling out, run <code>Trainer.dev()</code> with <code>n_runners=1</code> to verify the rollout logic and spans without burning GPU hours.</p>"},{"location":"tutorials/parallelize/#bundles-and-execution-strategies","title":"Bundles and Execution Strategies","text":"<p>When <code>Trainer</code> starts, it packages its configuration into two callable bundles:</p> <p></p> <p>The algorithm bundle wraps your <code>Algorithm</code>, adapter, and any LLM proxy into a single callable that can be aborted via a signal event.</p> <pre><code>async def algorithm_bundle(store: LightningStore, event: ExecutionEvent) -&gt; None:\n    ...\n</code></pre> <p>The runner bundle wraps the <code>Runner</code>, tracer, hooks, and agent into a single callable that can be aborted via a signal event. Unlike the algorithm bundle, the runner bundle is expected to be replicated.</p> <pre><code>async def runner_bundle(store: LightningStore, worker_id: int, event: ExecutionEvent) -&gt; None:\n    ...\n</code></pre> <p>An execution strategy then decides where those bundles are placed (threads vs processes vs multiple machines), how many runner replicas to launch, and how lifecycle events such as shutdown are coordinated.</p> <p>By default, the trainer builds an <code>InMemoryLightningStore</code> if you do not provide one. Because that store has no locking or cross-process transport, the execution strategy is the component that wraps it in thread-safe or HTTP-safe facades (<code>LightningStoreThreaded</code>, <code>LightningStoreServer</code>) before handing it to bundles. For a deeper look at these facades, see Understanding the Store and Birds' Eye View.</p> <p>Agent-lightning provides two built-in execution strategies: <code>SharedMemoryExecutionStrategy</code> and <code>ClientServerExecutionStrategy</code>. You can pass a string alias, a configuration dictionary, or a pre-built strategy instance:</p> <pre><code>import agentlightning as agl\n\nalgorithm = agl.Baseline()\n\n# Short alias for the shared-memory strategy.\n# Because the runner lives on the main thread in this mode,\n# n_runners must be 1 unless you move the algorithm to the main thread.\ntrainer = agl.Trainer(algorithm=algorithm, n_runners=1, strategy=\"shm\")\n\n# Dict with overrides; keep the algorithm on the main thread so multiple runner threads can spawn.\n# Specifying `n_runners` inside strategy is equivalent to passing `n_runners` to the trainer.\ntrainer = agl.Trainer(\n    algorithm=algorithm,\n    strategy={\n        \"type\": \"shm\",\n        \"n_runners\": 8,\n        \"main_thread\": \"algorithm\",\n    },\n)\n\n# Pass an existing strategy instance \u2013 Trainer respects the strategy's own `n_runners`.\nstrategy = agl.SharedMemoryExecutionStrategy(main_thread=\"algorithm\", n_runners=4)\ntrainer = agl.Trainer(algorithm=algorithm, strategy=strategy)\n</code></pre> <p>If you omit the strategy, the trainer defaults to <code>ClientServerExecutionStrategy(n_runners=trainer.n_runners)</code>. You can still re-specify the client-server strategy through aliases or configuration to tweak ports and other settings:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    n_runners=8,\n    strategy={\"type\": \"cs\", \"server_port\": 9999},\n)\n</code></pre> <p>Environment variables give you another layer of control. For example:</p> <pre><code>import os\n\nos.environ[\"AGL_SERVER_PORT\"] = \"10000\"\nos.environ[\"AGL_CURRENT_ROLE\"] = \"algorithm\"\nos.environ[\"AGL_MANAGED_STORE\"] = \"0\"\n\ntrainer = agl.Trainer(algorithm=algorithm, n_runners=8, strategy=\"cs\")\n</code></pre> <p>The resulting <code>ClientServerExecutionStrategy</code> picks up the port, role, and managed-store flag from the environment.</p> <p>Tip</p> <p>The same configuration patterns apply to other trainer components. For example, <pre><code>trainer = agl.Trainer(algorithm=algorithm, tracer=agl.OtelTracer())\n</code></pre> wires in a custom tracer, while <pre><code>trainer = agl.Trainer(algorithm=algorithm, adapter=\"agentlightning.adapter.TraceToMessages\")\n</code></pre> swaps in a different adapter. Passing a dict lets you tweak the init parameters of defaults without naming the class explicitly:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    adapter={\"agent_match\": \"plan_agent\", \"repair_hierarchy\": False},\n)\n</code></pre> <p>The next sections walk through the two built-in strategies and how they affect placement and store access.</p>"},{"location":"tutorials/parallelize/#client-server-architecture","title":"Client-server Architecture","text":"<p>The default <code>ClientServerExecutionStrategy</code> starts a <code>LightningStoreServer</code> alongside the algorithm and spawns runner processes that talk to it through <code>LightningStoreClient</code>. All runners share the HTTP endpoint, so the queue and spans stay consistent across processes or machines.</p> <p>If you simply instantiate <code>Trainer</code> (as above), it will send the algorithm bundle and runner bundle to <code>ClientServerExecutionStrategy</code>, which will then:</p> <ol> <li>Launch \\(N+1\\) processes: \\(N\\) runner processes and 1 algorithm process (one of them could live in the main process).</li> <li>The algorithm process will take the store received from <code>Trainer</code>, wrap it in a <code>LightningStoreServer</code>, and start serving it over HTTP.</li> <li>The runner processes discard the store and create a new store, which is a client that connects to the algorithm process through <code>LightningStoreClient</code>, and start executing the runner bundle.</li> <li>The strategy automatically escalates shutdown (cooperative stop \u2192 <code>SIGINT</code> \u2192 <code>terminate()</code> \u2192 <code>kill()</code>) so long-running runners do not linger.</li> </ol> <p>You can override server placement or ports, and whether to automatically wrap the store, through constructor arguments or environment variables:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    n_runners=1,\n    strategy={\n        \"type\": \"cs\",\n        \"server_host\": \"0.0.0.0\",\n        \"server_port\": 9999,\n        \"main_process\": \"runner\",\n    },\n)\n</code></pre> <p>Set <code>AGL_SERVER_HOST</code> and <code>AGL_SERVER_PORT</code> if you prefer environment-based configuration. You can also use <code>AGL_MANAGED_STORE</code> if you do not want the execution strategy to wrap the store for you. An example is shown in Debugging with External Store.</p> <p>Algorithms sometimes require heterogeneous computation resources, such as GPU accelerators, while runners sometimes require a specific environment to run because many agent frameworks are fragile in their dependencies. A role-based launch pattern helps you place the algorithm on a dedicated machine with more GPU memory, while runners can live on another machine with more flexible dependencies. This is possible via <code>AGL_CURRENT_ROLE=\"algorithm\"</code> or <code>AGL_CURRENT_ROLE=\"runner\"</code> environment variables. When running on different machines, you also need to set <code>AGL_SERVER_HOST</code> and <code>AGL_SERVER_PORT</code> to the IP address and port of the algorithm machine. You might recognize that this convention is very similar to <code>MASTER_ADDR</code> and <code>MASTER_PORT</code> in PyTorch distributed training.</p>"},{"location":"tutorials/parallelize/#launching-algorithm-and-runner-roles-on-separate-machines","title":"Launching Algorithm and Runner Roles on Separate Machines","text":"<p>When you want to stretch the algorithm onto a GPU-rich machine and keep rollout workers close to the data source (or on machines with a more permissive dependency stack), launch the same training script in different terminals with role-specific environment variables. The client\u2013server strategy will route each process to the right side of the queue as long as they share the same <code>AGL_SERVER_HOST</code>/<code>AGL_SERVER_PORT</code> pair.</p> <p>1. Pick an address and port for the store. Decide which machine will host the algorithm. Choose a TCP port that can be reached by the runner machines (for example, open it in your firewall configuration). In this example we will use <code>10.0.0.4:4747</code>.</p> <p>2. Start the algorithm process. On the machine that should run the algorithm, expose the store by binding to all network interfaces and mark the role as <code>algorithm</code>.</p> <pre><code>export AGL_SERVER_HOST=0.0.0.0\nexport AGL_SERVER_PORT=4747\nexport AGL_CURRENT_ROLE=algorithm\n\npython train_calc_agent.py\n</code></pre> <p>Leaving <code>AGL_MANAGED_STORE</code> unset (or setting it to <code>1</code>) lets the strategy create the <code>LightningStoreServer</code> for you. Otherwise, you can use the method in the previous section to create a store on your own.</p> <p>3. Start rollout workers on remote machines. Every runner machine should point to the algorithm host and declare itself as the <code>runner</code> role. You can start multiple processes per machine or repeat the command on additional hosts.</p> <pre><code>export AGL_SERVER_HOST=10.0.0.4\nexport AGL_SERVER_PORT=4747\nexport AGL_CURRENT_ROLE=runner\npython train_calc_agent.py --n-runners 4\n</code></pre> <p>The runner process automatically connects via <code>LightningStoreClient</code>. Adjust <code>--n-runners</code> to spawn the desired number of worker processes on that machine.</p> <p>4. Scale out as needed. Repeat step 3 on as many machines as you need. When you are done, stop the algorithm process. However, since the runners are on different machines, the strategy WILL NOT send a cooperative stop signal to the connected runners. So you need to kill the runners on your own.</p> <p>This role-based launch mirrors what <code>Trainer.fit</code> does inside a single machine while letting you spread work across a fleet. Because every process shares the same training script, you keep a single source of truth for dataset loading, adapters, and tracers, but you can tune compute resources independently for the algorithm and rollout workers.</p>"},{"location":"tutorials/parallelize/#shared-memory-strategy","title":"Shared-memory Strategy","text":"<p><code>SharedMemoryExecutionStrategy</code> keeps everything inside one process. The runner runs on the main thread (by default) while the algorithm lives on a Python thread guarded by <code>LightningStoreThreaded</code>.</p> <p>Use it when you want easier debugging with shared breakpoints and no serialization overhead, or minimal startup time for unit tests. It's not a good choice for many algorithms that require heavy model training because <code>LightningStoreThreaded</code> does not work for multiprocessing. Using it with multiprocessing algorithms will lead to undefined behavior.</p> <p>Sample configuration:</p> <pre><code>trainer = agl.Trainer(\n    algorithm=algorithm,\n    strategy=\"shm\",\n)\n</code></pre> <p>You can further customize the init parameters of <code>SharedMemoryExecutionStrategy</code>. With <code>main_thread=\"runner\"</code>, the runner occupies the main thread and <code>n_runners</code> must be <code>1</code>. The strategy respects <code>AGL_MANAGED_STORE</code>; set it to <code>0</code> to opt out of the <code>LightningStoreThreaded</code> wrapper.</p>"},{"location":"tutorials/parallelize/#parallelizing-algorithms","title":"Parallelizing Algorithms","text":"<p>Runner parallelism scales rollout throughput, but the algorithm loop remains a single-process loop inside the execution strategy. We understand that many algorithms have parallelization built in, but that's outside the parallelization scope of Agent-lightning.</p> <p>Agent-lightning strives to make algorithms\u2019 own parallelization work well under our execution strategies. The biggest challenge turns out to come from the store. For example, <code>VERL</code> uses Ray and launches FSDP and vLLM components internally. <code>ClientServerExecutionStrategy</code> has to make sure that the server is not simultaneously serving in multiple processes or Ray workers, and that there is only one single authoritative source of truth for all subprocesses to connect to. Subprocesses connect to the store via a small <code>LightningStoreClient</code> bundled within <code>LightningStoreServer</code>.</p> <p>Note</p> <p>The birds' eye view illustrates how adapters, proxies, and stores interact when the algorithm spawns additional workers. Use that diagram as a checklist when introducing new distributed components.</p>"},{"location":"tutorials/traces/","title":"Working with Traces","text":"<p>Tracing is the secret capability that lets Agent-lightning train almost any agent without rewriting its core logic. The idea was born in observability tooling inside LLMOps workflows and, in Agent-lightning, evolved into a first-class primitive inside the learning loop. Beyond helping you understand what happened inside a rollout, traces provide reward spans and other learning signals that power reinforcement learning and fine-tuning algorithms.</p> <p></p> <p>Agent-lightning stores every recorded operation as a <code>Span</code> inside a <code>LightningStore</code>. The naming comes from OpenTelemetry spans, shown in the screenshot above. A span can represent an LLM call, a tool invocation, a graph edge, an explicit reward emission, or an arbitrary Python code block. Spans form a tree where parent spans describe higher-level steps and children record the detailed work. The sections below walk through how spans are produced and how to interpret them once they reach the store.</p>"},{"location":"tutorials/traces/#writing-spans","title":"Writing Spans","text":"<p>Most <code>Runner</code> implementations wire a <code>Tracer</code> into the agent\u2019s lifecycle. The tracer is responsible for installing instrumentation, buffering OpenTelemetry spans, and committing them to the <code>LightningStore</code>. When a runner executes a rollout, it allocates a store-backed tracing context:</p> <pre><code>async with tracer.trace_context(\n    name=\"my-rollout\",\n    store=store,\n    rollout_id=rollout.rollout_id,\n    attempt_id=attempt.attempt_id,\n):\n    await run_agent_logic()\n</code></pre> <p>The context manager then requests sequence numbers from the store, converts OpenTelemetry spans into <code>Span</code> objects, and persists them in the middle or at the end of the attempt, depending on the tracer implementation. Agent-lightning ships two tracers out of the box; both rely on OpenTelemetry Traces and ignore metrics or logs.</p> <p>What's instrumentation?</p> <p>In simple terms, instrumentation means adding \"patches\" or hooks inside your code so you can observe what it\u2019s doing while it runs. Think of it like putting flight recorders in an airplane \u2014 instrumentation records key actions, inputs, outputs, and timings without changing how the code behaves. In Agent-lightning tracers, this instrumentation automatically creates spans (small, structured records of work) that show what each part of an agent did, how long it took, and how different steps connect together.</p>"},{"location":"tutorials/traces/#agentops-tracer","title":"AgentOps Tracer","text":"<p><code>AgentOpsTracer</code> will be the default tracer when <code>Trainer</code> is used but no tracer is explicitly specified. It bootstraps the AgentOps SDK locally, installs the supplied instrumentation hooks (LangChain, LangGraph, LiteLLM, FastAPI, and others) provided by the AgentOps Python SDK, and forwards everything through a local OpenTelemetry <code>TracerProvider</code>. <code>AgentOpsTracer</code> never calls the hosted AgentOps service; instead, it attaches a <code>LightningSpanProcessor</code> implemented by the Agent-lightning team so that spans are captured and shipped straight into the store.</p> <p>Because it shares the AgentOps instrumentation surface, any framework supported by AgentOps automatically gains tracing in Agent-lightning. We layer additional hooks on top of AgentOps to capture features that the SDK misses today:</p> <ol> <li>Certain providers emit extra metadata \u2014 for example, token IDs returned by vLLM \u2014 that are not recorded by the stock SDK. We augment those spans with the missing payloads.</li> <li>AgentOps constructs parent-child relationships on a best-effort basis, but mixed instrumentation (for example, OpenAI Agent SDK alongside direct OpenAI Chat Completion calls) can leave segments disconnected. Our implementation (actually implemented in the <code>TracerTraceToTriplet</code> adapter) repairs those relationships when the hierarchy can be inferred from rollout context.</li> <li>Some versions of downstream frameworks simply do not emit spans for critical events (LangGraph node entrances are a common example). The tracer installs lightweight shims so those spans appear consistently.</li> </ol> <p>If a vendor integration behaves unexpectedly, users are encouraged to combine the tracer with Hooks to inspect the raw spans or diagnostics, and/or implement a specialized tracer for the framework in question.</p>"},{"location":"tutorials/traces/#opentelemetry-tracer","title":"OpenTelemetry Tracer","text":"<p><code>OtelTracer</code> is a minimal implementation that initializes a vanilla <code>TracerProvider</code> and gives you direct control over span creation through the standard <code>opentelemetry.trace</code> API. Use it when you already have explicit instrumentation in your agent, when the AgentOps SDK does not support your framework, or when you want to emit custom spans from business logic.</p> <p>Note</p> <p>Microsoft Agent Framework is a typical example with built-in OpenTelemetry support. Once you set <code>OBSERVABILITY_SETTINGS.enable_otel = True</code>, the framework will automatically emit OpenTelemetry spans, and <code>OtelTracer</code> will be able to capture them. No extra instrumentation is needed.</p> <p>Inside your agent you can call <code>opentelemetry.trace.get_trace_provider().get_tracer(\"my-agent\")</code> and use that tracer to create spans exactly as you would in any OpenTelemetry application. The Lightning span processor attached by <code>OtelTracer</code> guarantees that every span is sequenced, converted, and written to the store. The same applies for emitted rewards (<code>emit_reward</code>) and other emitter signals, which are just a special case of manually-created spans.</p>"},{"location":"tutorials/traces/#llm-proxy","title":"LLM Proxy","text":"<p>Sometimes the runner can\u2019t observe the agent directly \u2014 because it\u2019s in another language or running remotely. <code>LLMProxy</code> bridges that gap by instrumenting the server side of LLM calls. It wraps LiteLLM and adds middleware that accepts prefixed routes like <code>/rollout/{rid}/attempt/{aid}/v1/chat/completions</code>. Before forwarding, the middleware rewrites the path to <code>/v1/chat/completions</code>, fetches a monotonic <code>sequence_id</code> from the <code>LightningStore</code>, injects <code>x-rollout-id</code>, <code>x-attempt-id</code>, and <code>x-sequence-id</code> into the request headers, and then forwards the request to the backend LLM endpoint.</p> <p>LiteLLM produces OpenTelemetry spans for the request/response. A custom <code>LightningSpanExporter</code> reads the rollout/attempt/sequence identifiers from the recorded request headers and persists each span to the store. Because the <code>sequence_id</code> is allocated at the start of the request, traces stay in strict order even across machines with skewed clocks or asynchronous responses.</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant Proxy as LLM Proxy\n    participant Backend as LLM Backend\n    participant Store as LightningStore\n\n    Agent-&gt;&gt;Proxy: POST /rollout/{rid}/attempt/{aid}/v1/chat/completions\n    Proxy-&gt;&gt;Store: get_next_span_sequence_id(rid, aid)\n    Store--&gt;&gt;Proxy: sequence_id\n    Proxy-&gt;&gt;Backend: Forward /v1/chat/completions&lt;br&gt;(headers: rid, aid, sid)\n    Backend--&gt;&gt;Proxy: Response (tokens, usage, token_ids)\n    Proxy-&gt;&gt;Store: Export OTEL spans (rid, aid, sequence_id)\n    Proxy--&gt;&gt;Agent: OpenAI-compatible response</code></pre> <p><code>LLMProxy</code> actually provides more functionalities than just the middleware for tracing. Read Serving LLM for more details.</p> <p></p> <p>Distributed Tracing</p> <p>Agent-lightning enforces deterministic span ordering by assigning a monotonic <code>sequence_id</code> to every span within an attempt. Before calling <code>LightningStore.add_span</code> or <code>LightningStore.add_otel_span</code>, tracers are expected to call <code>LightningStore.get_next_span_sequence_id</code> to get the next sequence id. This removes clock skew and merges spans produced on different machines or threads. If you implement a custom tracer or exporter, make sure you do this (or respect the one provided in headers by components such as <code>LLMProxy</code>); otherwise, adapters will struggle to properly reconstruct the execution tree.</p>"},{"location":"tutorials/traces/#custom-tracer","title":"Custom Tracer","text":"<p>If none of the built-in tracers fit your environment, the first option to consider is to return the spans directly from your agent implementation. If that's not possible, or you want to support multiple agents in a unified effort, you can implement your own tracer by subclassing <code>Tracer</code>.</p> <p>Custom tracers must implement at least <code>trace_context</code>. The <code>trace_context</code> coroutine should install or activate whatever instrumentation you need, then yield a span processor that ultimately adds spans to the store. You can reuse the <code>LightningSpanProcessor</code> if you produce OpenTelemetry <code>ReadableSpan</code> objects, or call <code>LightningStore.add_span</code> directly if you generate <code>Span</code> instances yourself.</p> <p>Advanced tracers often run auxiliary services (for example, starting a telemetry daemon or attaching to a container runtime) inside <code>init_worker</code> and tear them down in <code>teardown_worker</code>. The <code>ParallelWorkerBase</code> lifecycle that <code>Tracer</code> inherits from ensures those hooks are executed in every runner subprocess.</p>"},{"location":"tutorials/traces/#reading-traces","title":"Reading Traces","text":"<p>Generally, there are two approaches to reading traces. When you only need a quick look, <code>Tracer.get_last_trace</code> returns the raw OpenTelemetry spans captured most recently. For historical analysis, use the <code>LightningStore.query_spans</code> API, which yields normalized <code>Span</code> objects keyed by rollout ID and attempt ID. Combine those queries with <code>LightningStore.query_rollouts</code> to align spans with rollout status, retries, and timing information.</p> <p>Spans arrive asynchronously, originate from different processes, and form hierarchies rather than simple lists. The attributes of each span are tedious and unfriendly to human readers. This combination makes raw traces time-consuming to inspect, especially when you only care about specific signals such as rewards, LLM prompts, responses, or tool outputs. Understanding how the store exposes traces and how adapters reshape them will save hours when debugging or training.</p> <p>Why traces can be difficult to read?</p> <p>The trace tree for a single rollout typically mixes multiple abstraction layers: a planner span may contain several LLM spans, each of which contains tool execution spans that can themselves trigger nested agent invocations. There are also instrumentations at different levels. For example, when a request delegates to another library (e.g., from LangChain to OpenAI), two libraries might emit spans for the same request. At the top level, there could be concurrently running agents that may flush spans slightly out of order. Sorting by <code>sequence_id</code> restores the chronological view, but interpreting the tree requires additional context about parent-child relationships and rollout metadata.</p>"},{"location":"tutorials/traces/#adapter","title":"Adapter","text":"<p>Adapters transform lists of spans into higher-level data structures that training algorithms can consume directly. Agent-lightning provides several adapters out of the box:</p> <ul> <li><code>TracerTraceToTriplet</code> converts spans into <code>(prompt, response, reward)</code> triplets, which power reinforcement-learning algorithms such as VERL and connect trace data to gradient updates.</li> <li><code>TraceToMessages</code> rewrites spans into OpenAI chat message JSON suitable for supervised fine-tuning or evaluation harnesses.</li> <li><code>LlmProxyTraceToTriplet</code> mirrors <code>TracerTraceToTriplet</code> but understands spans emitted by LLMProxy. It is experimental and might be merged with <code>TracerTraceToTriplet</code> in the future.</li> </ul> <p>Adapters are regular Python callable instances, so you can plug them into <code>Trainer</code> via the <code>adapter</code> argument, or call them manually during exploration. When used in <code>Trainer</code>, adapters are bundled into the <code>Algorithm</code> before the algorithm runs, through the <code>Algorithm.set_adapter</code> method.</p> <p>You can also customize an <code>Adapter</code> by extending the implementations above or subclassing the base class. If you need a bespoke format, subclass <code>TraceAdapter</code> (for store spans) or <code>OtelTraceAdapter</code> (for raw OpenTelemetry spans) and implement <code>adapt</code> (these two classes can usually share the same implementation).</p>"},{"location":"tutorials/traces/#reading-rewards","title":"Reading Rewards","text":"<p>Rewards are recorded as dedicated spans named <code>agentlightning.reward</code>. Emitting a reward through <code>emit_reward</code> or the <code>@reward</code> decorator ensures the value is stored in the span\u2019s <code>attributes[\"reward\"]</code>. To audit rewards, fetch spans from the store and use the helper utilities in <code>agentlightning.emitter</code>:</p> <pre><code>from agentlightning.emitter import find_final_reward\n\nspans = await store.query_spans(rollout_id)\nreward = find_final_reward(spans)\nprint(f\"Final reward: {reward}\")\n</code></pre> <p><code>find_reward_spans</code> returns every reward span so you can visualize intermediate shaping signals, while <code>find_final_reward</code> extracts the last non-null reward per attempt. While these helpers are convenient, they may not help you fully understand the chronological or hierarchical relationships between reward spans and other spans. Using an <code>Adapter</code> \u2014 especially the same one used in the algorithm you\u2019re working with \u2014 remains the recommended way to inspect your generated spans.</p>"},{"location":"tutorials/write-agents/","title":"Writing Agents","text":"<p>This tutorial will focus on the heart of the system: the agent itself, guiding you through the different ways to define an agent's logic in Agent-lightning.</p> <p>The basic requirements for any agent are:</p> <ol> <li>It must accept a single task as input.</li> <li>It must accept a set of tunable resources (like a PromptTemplate or LLM).</li> <li>It must emit trace span data so that algorithms can understand its behavior and learn from it. The simplest way to do this is by returning a final reward.</li> </ol> <p>In practice, please also bear in mind that tasks, resources, and spans have extra requirements, in order to make it trainable within Agent-lightning:</p> <ol> <li>You will need a training dataset containing a set of tasks, of the same type that your agent expects as input.</li> <li>The tunable resources are related to the algorithm. For example, the APO algorithm we've seen tunes a PromptTemplate. Other algorithms might tune model weights or other configurations.</li> <li>The type of spans an algorithm can use varies. Almost all algorithms support a single, final reward span at the end of a rollout. However, not all algorithms support rewards emitted mid-rollout, let alone other kinds of spans like exceptions or log messages.</li> </ol> <p>This tutorial will show you how to write an agent that can handle various tasks and resources and emit all kinds of spans. However, you should understand that agents and algorithms are often co-designed. Supporting new types of resources or spans in an algorithm is often much more complex than just adding them to an agent.</p>"},{"location":"tutorials/write-agents/#rollout-decorator","title":"<code>@rollout</code> Decorator","text":"<p>The simplest way to create an agent is by writing a standard Python function and marking it with the @rollout decorator. This approach is perfect for agents with straightforward logic that doesn't require complex state management.</p> <p>Agent-lightning automatically inspects your function's signature and injects the required resources. For example, if your function has a parameter named <code>prompt_template</code>, Agent-lightning will find the PromptTemplate resource for the current rollout and pass it in.</p> <p>Let's revisit the <code>room_selector</code> agent from the first tutorial:</p> <pre><code>from typing import TypedDict\nfrom agentlightning import PromptTemplate, rollout\n\n# Define a data structure for the task input\nclass RoomSelectionTask(TypedDict):\n    # ... fields for the task ...\n    pass\n\n@rollout\ndef room_selector(task: RoomSelectionTask, prompt_template: PromptTemplate) -&gt; float:\n    # 1. Use the injected prompt_template to format the input for the LLM\n    prompt = prompt_template.format(**task)\n\n    # 2. Execute the agent's logic (e.g., call an LLM, use tools)\n    # ...\n\n    # 3. Grade the final choice to get a reward\n    reward = room_selection_grader(final_message, task[\"expected_choice\"])\n\n    # 4. Return the final reward as a float\n    return reward\n</code></pre> <p>When you train this agent, the dataset is expected to be a list of <code>RoomSelectionTask</code> objects:</p> <pre><code>from agentlightning import Dataset, Trainer\n\ndataset: Dataset[RoomSelectionTask] = [\n    RoomSelectionTask(date=\"2025-10-15\", time=\"10:00\", duration_min=60, attendees=10),\n    RoomSelectionTask(date=\"2025-10-16\", time=\"10:00\", duration_min=60, attendees=10),\n]\n\nTrainer().fit(agent=room_selector, train_dataset=dataset)\n</code></pre> <p>Behind the scenes, the <code>@rollout</code> decorator wraps your function in a <code>FunctionalLitAgent</code> object, which is a subclass of LitAgent introduced below, making it compatible with the Trainer and Runner. It supports parameters like <code>task</code>, <code>prompt_template</code>, <code>llm</code>, and <code>rollout</code>, giving you flexible access to the execution context.</p> <p>Here is another example with more advanced usage with <code>llm</code> and <code>rollout</code> as parameters. The <code>llm</code> parameter gives you an OpenAI-compatible LLM endpoint to interact with, which can be tuned under the hood by algorithms. The <code>rollout</code> parameter gives you the full Rollout object, which contains the rollout ID, rollout mode (training or validation), etc.</p> <pre><code>from openai import OpenAI\nfrom agentlightning import LLM, Rollout\n\nclass FlightBookingTask(TypedDict):\n    request: str\n    expected_booking: dict\n\n@rollout\ndef flight_assistant(task: FlightBookingTask, llm: LLM, rollout: Rollout) -&gt; float:\n    print(f\"Rollout ID: {rollout.rollout_id}\")\n    print(f\"Rollout Mode: {rollout.mode}\")\n\n    # Use the tuned LLM resource to create an OpenAI client\n    client = OpenAI(\n        # This endpoint could be a proxy to a proxy to a proxy ...\n        # It could be different every time `flight_assistant` is called\n        # But it should be OpenAI-API compatible\n        base_url=llm.endpoint,\n\n        # Use a dummy key if not provided\n        # Usually this does not matter because the training LLM is often not guarded by an API key\n        # But you can use `or os.environ[\"OPENAI_API_KEY\"]` to make the function compatible with 3rd-party LLMs\n        api_key=llm.api_key or \"dummy-key\",\n    )\n\n    # Make an API call with the specified model\n    response = client.chat.completions.create(\n        model=llm.model,\n        messages=[{\"role\": \"user\", \"content\": task[\"request\"]}],\n    )\n    # Whether the API supports features like streaming, tool calls, etc. depends on\n    # the endpoint that algorithms are serving to you.\n    final_message = response.choices[0].message.content\n\n    # Grade the result and return a reward\n    reward = grade_flight_booking(final_message, task[\"expected_booking\"])\n    return reward\n</code></pre>"},{"location":"tutorials/write-agents/#return-values-from-agents","title":"Return Values from Agents","text":"<p>The value your agent function returns (i.e., the return value of the function decorated by <code>@rollout</code>) is crucial, as it's the primary way to report the outcome of a rollout. Agent-lightning supports several return types to accommodate different scenarios, from simple rewards to detailed, custom traces.</p> <ul> <li> <p><code>float</code>: This is the simplest and most common return type. The <code>float</code> is treated as the final reward for the entire rollout. Agent-lightning automatically creates a final reward span based on this value.</p> </li> <li> <p><code>None</code>: Returning <code>None</code> tells the runner that trace collection is being handled entirely by the Tracer through auto-instrumentation (e.g., via AgentOps). In this case, the runner will simply retrieve the spans that the tracer has already captured.</p> </li> </ul> <p>Emitting the Final Reward</p> <p>When returning <code>None</code>, you must still ensure a final reward is logged. You can do this by using the <code>emit_reward</code> function (covered in the Emitter section below) or by wrapping your reward calculation function with the <code>@reward</code> decorator.</p> <ul> <li><code>list[ReadableSpan]</code> or <code>list[Span]</code>: For advanced use cases, you can manually construct and return a complete list of all spans for the rollout. This gives you full control over the trace data. You can return either a list of OpenTelemetry <code>ReadableSpan</code> objects or Agent-lightning's native <code>Span</code> objects.</li> </ul> <p>For most users, returning a <code>float</code> for simple agents or returning <code>None</code> and using the emitter for more complex ones are the recommended approaches.</p>"},{"location":"tutorials/write-agents/#class-based-agents","title":"Class-based Agents","text":"<p>For more complex agents that require state, helper methods, or distinct logic for training versus validation, you can create a class that inherits from <code>LitAgent</code>. This object-oriented approach provides more structure and control over the agent's lifecycle.</p> <p>To create a class-based agent, you subclass agentlightning.LitAgent and implement its <code>rollout</code> method.</p> <p></p> <p>Here's how the <code>room_selector</code> could be implemented as a class. The rollout method has a slightly different signature than the function-based agent, mainly in how it handles the resources. Putting it simply, algorithms do not just send a PromptTemplate to the agents, they instead send NamedResources, which is a mapping from resource key to Resource. This design is to allow for more advanced features like multi-resource tuning.</p> <p>With <code>@rollout</code> decorator, the resource with correctly matched type will be automatically injected into the rollout method. However, when you use a class-based agent, you need to manually access the resource from the <code>resources</code> dictionary. Built-in algorithms listed their resource key naming conventions here.</p> <pre><code>import agentlightning as agl\n\nclass RoomSelectorAgent(agl.LitAgent[RoomSelectionTask]):\n    def rollout(self, task: RoomSelectionTask, resources: agl.NamedResources, rollout: agl.Rollout) -&gt; float:\n        # 1. Access the prompt_template from the resources dictionary\n        prompt_template = resources[\"prompt_template\"]\n\n        # 2. Execute the agent's logic\n        prompt = prompt_template.format(**task)\n        # ...\n\n        # 3. Grade the final choice\n        reward = room_selection_grader(final_message, task[\"expected_choice\"])\n\n        # 4. Return the final reward\n        return reward\n\n# To use it with the trainer:\n# agent = RoomSelectorAgent()\n# trainer.fit(agent=agent, ...)\n</code></pre> <p>The <code>LitAgent</code> class provides several methods you can override for more fine-grained control:</p> <ul> <li><code>rollout()</code>: The primary method for the agent's logic. It's called for both training and validation by default.</li> <li><code>training_rollout()</code> / <code>validation_rollout()</code>: Implement these if you need different behavior during training (e.g., with exploration) and validation (e.g., with deterministic choices).</li> <li><code>rollout_async()</code> / <code>training_rollout_async()</code> / <code>validation_rollout_async()</code>: Implement the asynchronous versions of these methods if your agent uses <code>asyncio</code>.</li> </ul> <p>Note</p> <p>Rollout is always executed in an asynchronous context no matter whether the agent is asynchronous or synchronous. If your synchronous agent contains some <code>asyncio.run()</code> calls, it might raise an error that there is already an event loop running. To avoid blocking the event loop, it's recommended to offload the inner async operations to a separate thread. Here is a sample code:</p> <pre><code>import asyncio\nimport queue\nimport threading\n\ndef run_sync_ephemeral(coro) -&gt; Any:\n    \"\"\"\n    Run an async coroutine from sync code.\n    - If no loop in this thread: use asyncio.run() directly.\n    - If already in an event loop: spawn a worker thread that calls asyncio.run()\n    (which creates and closes a brand-new event loop per call).\n    \"\"\"\n    try:\n        asyncio.get_running_loop()\n    except RuntimeError:\n        # No running loop in this thread; safe to use asyncio.run\n        return asyncio.run(coro)\n\n    # Already in a running loop -&gt; execute in a worker thread\n    q = queue.Queue[Any]()\n\n    def worker():\n        try:\n            result = asyncio.run(coro)  # creates &amp; closes its own loop\n            q.put((True, result))\n        except BaseException as e:\n            q.put((False, e))\n\n    t = threading.Thread(target=worker, daemon=True)\n    t.start()\n    ok, payload = q.get()\n    t.join()\n    if ok:\n        return payload\n    raise payload\n</code></pre>"},{"location":"tutorials/write-agents/#using-the-emitter","title":"Using the Emitter","text":"<p>While returning a single float for the final reward is sufficient for many algorithms, some advanced scenarios require richer feedback. For instance, an algorithm might learn more effectively if it receives intermediate rewards throughout a multi-step task.</p> <p>Agent-lightning provides an emitter module that allows you to record custom spans from within your agent's logic. Like many common operations (like LLM calls) that are automatically instrumented by Tracer, the emitter will also send a Span that records an Agent-lightning-specific operation. Then algorithms can query and read those spans later. See Working with Traces for more details.</p> <p>You can find the emitter functions from agentlightning.emitter.</p>"},{"location":"tutorials/write-agents/#emitting-rewards-messages-and-more","title":"Emitting Rewards, Messages, and More","text":"<p>Here are the primary emitter functions:</p> <ul> <li><code>emit_reward(value: float)</code>: Records an intermediate reward.</li> <li><code>emit_message(message: str)</code>: Records a simple log message as a span.</li> <li><code>emit_exception(exception: BaseException)</code>: Records a Python exception, including its type, message, and stack trace.</li> <li><code>emit_object(obj: Any)</code>: Records any JSON-serializable object, perfect for structured data.</li> </ul> <p>Let's see an example of an agent using these emitters to provide detailed feedback.</p> <pre><code>import agentlightning as agl\n\n@agl.rollout\ndef multi_step_agent(task: dict, prompt_template: PromptTemplate) -&gt; float:\n    try:\n        # Step 1: Initial planning\n        agl.emit_message(\"Starting planning phase.\")\n        plan = generate_plan(task, prompt_template)\n        agl.emit_object({\"plan_steps\": len(plan), \"first_step\": plan[0]})\n\n        # Award a small reward for a valid plan\n        plan_reward = grade_plan(plan)\n        agl.emit_reward(plan_reward)\n\n        # Step 2: Execute the plan\n        agl.emit_message(f\"Executing {len(plan)}-step plan.\")\n        execution_result = execute_plan(plan)\n\n        # Step 3: Final evaluation\n        final_reward = custom_grade_final_result(execution_result, task[\"expected_output\"])\n\n        # The return value is treated as the final reward for the rollout\n        return final_reward\n\n    except ValueError as e:\n        # Record the specific error and return a failure reward\n        agl.emit_exception(e)\n        return 0.0\n</code></pre> <p>By using the emitter, you create a rich, detailed trace of your agent's execution. This data can be invaluable for debugging and is essential for advanced algorithms that can learn from more than just a single final score.</p>"}]}